<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn.jsdelivr.net/gh/stuxiaozhang/blogimage/img/20220330150132.png"><link rel="icon" href="https://cdn.jsdelivr.net/gh/stuxiaozhang/blogimage/img/20220330150132.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="NNLM 的 PyTorch 实现
A Neural Probabilistic Language Model
本文算是训练语言模型的经典之作，Bengio 将神经网络引入语言模型的训练中，并得到了词向量这个副产物。词向量对后面深度学习在自然语言处理方面有很大的贡献，也是获取词的语义特征的有效方法
其主要架构为三层神经网络，如下图所示

现在的任务是输入 \(w_{t−n+1},...,"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>NNLM 的 PyTorch 实现 - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://cdn.jsdelivr.net/gh/stuxiaozhang/blogimage/img/20220330150632.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="NNLM 的 PyTorch 实现"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-03-10 11:58" pubdate>2022年3月10日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.4k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 13 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">NNLM 的 PyTorch 实现</h1><p class="note note-info">本文最后更新于：2022年3月30日</p><div class="markdown-body"><h1 id="nnlm-的-pytorch-实现">NNLM 的 PyTorch 实现</h1><h2 id="a-neural-probabilistic-language-model">A Neural Probabilistic Language Model</h2><p>本文算是训练语言模型的经典之作，Bengio 将神经网络引入语言模型的训练中，并得到了词向量这个副产物。词向量对后面深度学习在自然语言处理方面有很大的贡献，也是获取词的语义特征的有效方法</p><p>其主要架构为三层神经网络，如下图所示</p><p><img src="https://cdn.jsdelivr.net/gh/stuxiaozhang/blogimage/img/20220330140630.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><p>现在的任务是输入 <span class="math inline">\(w_{t−n+1},...,w_{t−1}\)</span> 这前 n-1 个单词，然后预测出下一个单词 <span class="math inline">\(w_t\)</span></p><p>数学符号说明：</p><ul><li><span class="math inline">\(C(i)\)</span>：单词 <span class="math inline">\(w\)</span> 对应的词向量，其中 <span class="math inline">\(i\)</span> 为词 <span class="math inline">\(w\)</span> 在整个词汇表中的索引</li><li><span class="math inline">\(C\)</span>：词向量，大小为 <span class="math inline">\(|V|×m\)</span> 的矩阵</li><li><span class="math inline">\(|V|\)</span>：词汇表的大小，即预料库中去重后的单词个数</li><li><span class="math inline">\(m\)</span>：词向量的维度，一般大于 50</li><li><span class="math inline">\(H\)</span>：隐藏层的 weight</li><li><span class="math inline">\(d\)</span>：隐藏层的 bias</li><li><span class="math inline">\(U\)</span>：输出层的 weight</li><li><span class="math inline">\(b\)</span>：输出层的 bias</li><li><span class="math inline">\(W\)</span>：输入层到输出层的 weight</li><li><span class="math inline">\(h\)</span>：隐藏层神经元个数</li></ul><p>计算流程：</p><ol type="1"><li>首先将输入的 <span class="math inline">\(n−1\)</span> 个单词索引转为词向量，然后将这 <span class="math inline">\(n−1\)</span> 个向量进行 concat，形成一个 <span class="math inline">\((n−1)×w\)</span> 的矩阵，用 <span class="math inline">\(X\)</span> 表示</li><li>将 <span class="math inline">\(X\)</span> 送入隐藏层进行计算，<span class="math inline">\(hidden_{out}=tanh⁡(d+X∗H)\)</span></li><li>输出层共有 <span class="math inline">\(|V|\)</span> 个节点，每个节点 <span class="math inline">\(y_i\)</span> 表示预测下一个单词 <span class="math inline">\(i\)</span> 的概率，<span class="math inline">\(y\)</span> 的计算公式为 <span class="math inline">\(y=b+X∗W+hidden_{out}∗U\)</span></li></ol><h2 id="代码实现pytorch">代码实现（PyTorch）</h2><p>导入包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> Data<br><br>dtype = torch.FloatTensor<br></code></pre></td></tr></table></figure><p>数据预处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">sentences = [ <span class="hljs-string">&quot;i like dog&quot;</span>, <span class="hljs-string">&quot;i love coffee&quot;</span>, <span class="hljs-string">&quot;i hate milk&quot;</span>]<br><br><span class="hljs-comment"># 将空格加入到单词之间，然后按空格隔开组成一个list</span><br>word_list = <span class="hljs-string">&quot; &quot;</span>.join(sentences).split()  <span class="hljs-comment"># [&#x27;i&#x27;, &#x27;like&#x27;, &#x27;dog&#x27;, &#x27;i&#x27;, &#x27;love&#x27;, &#x27;coffee&#x27;, &#x27;i&#x27;, &#x27;hate&#x27;, &#x27;milk&#x27;]</span><br><br><span class="hljs-comment"># 构建词汇表，词汇表需要去重，再转换成list</span><br>word_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(word_list))  <span class="hljs-comment"># [&#x27;i&#x27;, &#x27;like&#x27;, &#x27;dog&#x27;, &#x27;love&#x27;, &#x27;coffee&#x27;, &#x27;hate&#x27;, &#x27;milk&#x27;]</span><br><br><br>word_dict = &#123;w: i <span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word_list)&#125;  <span class="hljs-comment"># &#123;&#x27;i&#x27;:0, &#x27;like&#x27;:1, &#x27;dog&#x27;:2, &#x27;love&#x27;:3, &#x27;coffee&#x27;:4, &#x27;hate&#x27;:5, &#x27;milk&#x27;:6&#125;</span><br>number_dict = &#123;i: w <span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word_list)&#125;  <span class="hljs-comment"># &#123;0:&#x27;i&#x27;, 1:&#x27;like&#x27;, 2:&#x27;dog&#x27;, 3:&#x27;love&#x27;, 4:&#x27;coffee&#x27;, 5:&#x27;hate&#x27;, 6:&#x27;milk&#x27;&#125;</span><br>n_class = <span class="hljs-built_in">len</span>(word_dict)  <span class="hljs-comment"># number of Vocabulary, just like |V|, in this task n_class=7（维度，有多少词就是多少行）</span><br><br><br><span class="hljs-comment"># NNLM(Neural Network Language Model) Parameter</span><br><span class="hljs-comment"># n_step = 2  # 输入数据的长度，这里就是两个单词</span><br>n_step = <span class="hljs-built_in">len</span>(sentences[<span class="hljs-number">0</span>].split())-<span class="hljs-number">1</span>  <span class="hljs-comment"># n-1 in paper, look back n_step words and predict next word. In this task n_step=2</span><br>n_hidden = <span class="hljs-number">2</span>  <span class="hljs-comment"># h in paper</span><br>m = <span class="hljs-number">2</span>  <span class="hljs-comment"># m in paper, word embedding dim</span><br></code></pre></td></tr></table></figure><p>由于 PyTorch 中输入数据是以 mini-batch 小批量进行的，下面的函数首先将原始数据（词）全部转为索引，然后通过 <code>TensorDataset()</code> 和 <code>DataLoader()</code> 编写一个实用的 mini-batch 迭代器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_batch</span>(<span class="hljs-params">sentences</span>):</span><br>    input_batch = []<br>    target_batch = []<br>    <br>    <span class="hljs-comment"># 遍历每一条句子</span><br>    <span class="hljs-keyword">for</span> sen <span class="hljs-keyword">in</span> sentences:<br>        word = sen.split()  <span class="hljs-comment"># [&#x27;i&#x27;, &#x27;like&#x27;, &#x27;dog&#x27;]</span><br>        <span class="hljs-comment"># 只要前两个词作为输入，最后一个词是预测的</span><br>        <span class="hljs-built_in">input</span> = [word_dict[n] <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> word[:-<span class="hljs-number">1</span>]]  <span class="hljs-comment"># [0, 1], [0, 3], [0, 5]</span><br>        target = word_dict[word[-<span class="hljs-number">1</span>]]  <span class="hljs-comment"># 2, 4, 6</span><br><br>        input_batch.append(<span class="hljs-built_in">input</span>)  <span class="hljs-comment"># [[0, 1], [0, 3], [0, 5]]</span><br>        target_batch.append(target)  <span class="hljs-comment"># [2, 4, 6]</span><br><br>    <span class="hljs-keyword">return</span> input_batch, target_batch<br><br><br>input_batch, target_batch = make_batch(sentences)<br><span class="hljs-comment"># 需要将 list 转换为 tensor</span><br>input_batch = torch.LongTensor(input_batch)<br>target_batch = torch.LongTensor(target_batch)<br><br>dataset = Data.TensorDataset(input_batch, target_batch)<br><br>loader = Data.DataLoader(dataset=dataset, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>定义网络结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义网络结构</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NNLM</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(NNLM, self).__init__()<br>        self.C = nn.Embedding(n_class, m)  <span class="hljs-comment"># 有多少行，</span><br>        self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).<span class="hljs-built_in">type</span>(dtype))<br>        self.W = nn.Parameter(torch.randn(n_step * m, n_class).<span class="hljs-built_in">type</span>(dtype))<br>        self.d = nn.Parameter(torch.randn(n_hidden).<span class="hljs-built_in">type</span>(dtype))<br>        self.U = nn.Parameter(torch.randn(n_hidden, n_class).<span class="hljs-built_in">type</span>(dtype))<br>        self.b = nn.Parameter(torch.randn(n_class).<span class="hljs-built_in">type</span>(dtype))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, X</span>):</span><br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        X: [batch_size, n_step]  batch_size：选几个数据，n_step 列</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        X = self.C(X) <span class="hljs-comment"># [batch_size, n_step] =&gt; [batch_size, n_step, m]</span><br>        X = X.view(-<span class="hljs-number">1</span>, n_step * m)  <span class="hljs-comment"># [batch_size, n_step * m] 维度变为2维</span><br>        hidden_out = torch.tanh(self.d + torch.mm(X, self.H))  <span class="hljs-comment"># [batch_size, n_hidden]</span><br>        output = self.b + torch.mm(X, self.W) + torch.mm(hidden_out, self.U)  <span class="hljs-comment"># [batch_size, n_class]</span><br>        <span class="hljs-keyword">return</span> output<br><br>    <br>model = NNLM()<br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.Adam(model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br></code></pre></td></tr></table></figure><p><code>nn.Parameter()</code> 的作用是将该参数添加进模型中，使其能够通过 <code>model.parameters()</code> 找到、管理、并且更新。更具体的来说就是：</p><ol type="1"><li><code>nn.Parameter()</code> 与 <code>nn.Module</code> 一起使用时会有一些特殊的属性，其会被自动加到 Module 的 <code>parameters()</code> 迭代器中</li><li>使用很简单：<code>torch.nn.Parameter(data, requires_grad=True)</code>，其中 data 为 tensor</li></ol><p>简单解释一下执行 <code>X=self.C(X)</code> 这一步之后 <code>X</code> 发生了什么变化，假设初始 <code>X=[[0, 1], [0, 3]]</code></p><p>通过 <code>Embedding()</code> 之后，会将每一个词的索引，替换为对应的词向量，例如 <code>love</code> 这个词的索引是 <code>3</code>，通过查询 Word Embedding 表得到行索引为 3 的向量为 <code>[0.2, 0.1]</code>，于是就会将原来 <code>X</code> 中 <code>3</code> 的值替换为该向量，所有值都替换完之后，<code>X=[[[0.3, 0.8], [0.2, 0.4]], [[0.3, 0.8], [0.2, 0.1]]]</code></p><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Training</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5000</span>):<br>    <span class="hljs-keyword">for</span> batch_x, batch_y <span class="hljs-keyword">in</span> loader:<br>        output = model(batch_x)  <span class="hljs-comment"># 预测值 pred</span><br>        optimizer.zero_grad()  <span class="hljs-comment"># 梯度清零</span><br><br>        <span class="hljs-comment"># output : [batch_size, n_class], batch_y : [batch_size] (LongTensor, not one-hot)</span><br>        <span class="hljs-comment"># 中间打印一下loss</span><br>        loss = criterion(output, batch_y)<br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>)%<span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>            print(<span class="hljs-string">&#x27;Epoch:&#x27;</span>, <span class="hljs-string">&#x27;%04d&#x27;</span> % (epoch + <span class="hljs-number">1</span>), <span class="hljs-string">&#x27;cost =&#x27;</span>, <span class="hljs-string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(loss))<br><br>        loss.backward()  <span class="hljs-comment"># 误差反向传播</span><br>        optimizer.step()  <span class="hljs-comment"># 更新参数</span><br><br><br><span class="hljs-comment"># Predict</span><br>predict = model(input_batch).data.<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]<br><br><br><span class="hljs-comment"># Test</span><br>print([sen.split()[:n_step] <span class="hljs-keyword">for</span> sen <span class="hljs-keyword">in</span> sentences], <span class="hljs-string">&#x27;-&gt;&#x27;</span>, [number_dict[n.item()] <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> predict.squeeze()])        <br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas mb-3"><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/PyTorch/">PyTorch</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2022/03/10/Word2Vec%20%E7%9A%84%20PyTorch%20%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B9%9E%E4%B8%90%E7%89%88%EF%BC%89/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Word2Vec 的 PyTorch 实现（乞丐版）</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/10/13/Linux%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E8%AE%B0%E5%BD%95/"><span class="hidden-mobile">Linux 常用指令记录</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>