<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn.jsdelivr.net/gh/stuxiaozhang/blogimage/img/20220330150132.png"><link rel="icon" href="https://cdn.jsdelivr.net/gh/stuxiaozhang/blogimage/img/20220330150132.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="Word2Vec 的 PyTorch 实现（乞丐版）
自然语言处理问题中，一般以词作为基本单元，例如我们想要分析 &amp;quot;我去过华盛顿州&amp;quot; 这句话的情感，一般的做法是先将这句话进行分词，变成我，去过，华盛顿州，由于神经网络无法处理词，所以我们需要将这些词通过某些办法映射成词向量。词向量是用来表示词的向量，也可被认为是词的特征向量。把词映射为实数域向量的技术也叫词嵌入（word em"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>Word2Vec 的 PyTorch 实现（乞丐版） - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://cdn.jsdelivr.net/gh/stuxiaozhang/blogimage/img/20220330150632.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="Word2Vec 的 PyTorch 实现（乞丐版）"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-03-10 16:37" pubdate>2022年3月10日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.2k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 11 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">Word2Vec 的 PyTorch 实现（乞丐版）</h1><p class="note note-info">本文最后更新于：2022年3月28日</p><div class="markdown-body"><h1 id="word2vec-的-pytorch-实现乞丐版">Word2Vec 的 PyTorch 实现（乞丐版）</h1><p>自然语言处理问题中，一般以词作为基本单元，例如我们想要分析 &quot;我去过华盛顿州&quot; 这句话的情感，一般的做法是先将这句话进行分词，变成<code>我</code>，<code>去过</code>，<code>华盛顿州</code>，由于神经网络无法处理词，所以我们需要将这些词通过某些办法映射成词向量。词向量是用来表示词的向量，也可被认为是词的特征向量。<strong>把词映射为实数域向量的技术也叫词嵌入（word embedding）</strong></p><p>源码来自于 <a target="_blank" rel="noopener" href="https://github.com/graykode/nlp-tutorial">nlp-tutorial</a>，<a target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1443/">王铃鑫大佬</a>在其基础上进行了修改：</p><p>导入包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optimizer<br><span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> Data<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>dtype = torch.FloatTensor<br></code></pre></td></tr></table></figure><p>文本预处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">sentences = [<span class="hljs-string">&quot;jack like dog&quot;</span>, <span class="hljs-string">&quot;jack like cat&quot;</span>, <span class="hljs-string">&quot;jack like animal&quot;</span>,<br>  <span class="hljs-string">&quot;dog cat animal&quot;</span>, <span class="hljs-string">&quot;banana apple cat dog like&quot;</span>, <span class="hljs-string">&quot;dog fish milk like&quot;</span>,<br>  <span class="hljs-string">&quot;dog cat animal like&quot;</span>, <span class="hljs-string">&quot;jack like apple&quot;</span>, <span class="hljs-string">&quot;apple like&quot;</span>, <span class="hljs-string">&quot;jack like banana&quot;</span>,<br>  <span class="hljs-string">&quot;apple banana jack movie book music like&quot;</span>, <span class="hljs-string">&quot;cat dog hate&quot;</span>, <span class="hljs-string">&quot;cat dog like&quot;</span>]<br><br>sentence_list = <span class="hljs-string">&quot; &quot;</span>.join(sentences).split()  <br><span class="hljs-comment"># [&#x27;jack&#x27;, &#x27;like&#x27;, &#x27;dog&#x27;, &#x27;jack&#x27;, &#x27;like&#x27;, &#x27;cat&#x27;, &#x27;animal&#x27;,...]</span><br><span class="hljs-comment">#     0      1       2       0      1       3       4</span><br><br><span class="hljs-comment"># 去个重，构建词汇的语料库</span><br>vocab = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(sentence_list))<br><span class="hljs-comment"># vocab = sorted(set(sentence_list), key=sentence_list.index)  # 我自己写的，为了保持原句子的顺序</span><br><br>word2idx = &#123;w:i <span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocab)&#125;<br><span class="hljs-comment"># &#123;&#x27;hate&#x27;: 0, &#x27;cat&#x27;: 1,...</span><br>vocab_size = <span class="hljs-built_in">len</span>(vocab)<br></code></pre></td></tr></table></figure><p>模型相关参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Word2Vec Parameters</span><br>C = <span class="hljs-number">2</span> <span class="hljs-comment"># window size</span><br>batch_size = <span class="hljs-number">8</span><br>embedding_size = <span class="hljs-number">2</span>  <span class="hljs-comment"># 2 dim vector represent one word</span><br></code></pre></td></tr></table></figure><p>数据预处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">skip_grams = []<br><span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(C, <span class="hljs-built_in">len</span>(sentence_list) - C):<br>    center = word2idx[sentence_list[idx]]  <span class="hljs-comment"># 中心词的位置</span><br>    context_idx = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(idx - C, idx)) + <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(idx + <span class="hljs-number">1</span>, idx + C + <span class="hljs-number">1</span>))  <span class="hljs-comment"># 左边的背景词 + 右边的背景词</span><br>    context = [word2idx[sentence_list[i]] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> context_idx]  <span class="hljs-comment"># 这个地方是 背景词索引 找出 背景词</span><br><span class="hljs-comment">#     print(context)</span><br>    <br>    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> context:<br>        skip_grams.append([center, w])<br></code></pre></td></tr></table></figure><p>假设所有文本分词，转为索引之后的 list 如下图所示. 根据论文所述，设定 window size=2，即每个中心词左右各取 2 个词作为背景词，那么对于上面的 list，窗口每次滑动，选定的中心词和背景词如下图.</p><p><img src="https://cdn.jsdelivr.net/gh/stuxiaozhang/blogimage/img/20220328130934.png" srcset="/img/loading.gif" lazyload></p><p>那么 skip_grams 变量里存的就是中心词和背景词一一配对后的 list，例如中心词 <code>2</code>，有背景词 <code>0,1,0,1，</code>一一配对以后就会产生 <code>[2,0],[2,1],[2,0],[2,1]</code>。skip_grams 如下图所示.</p><p>输入的是 one-hot 向量，“1”的位置就是中心词. 由于 Word2Vec 的输入是 one-hot 表示，所以我们先构建一个对角全 1 的矩阵，利用 <code>np.eye(rows)</code> 方法，其中的参数 rows 表示全 1 矩阵的行数，对于这个问题来说，语料库中总共有多少个单词，就有多少行</p><p>然后根据 skip_grams 每行第一列的值，取出相应全 1 矩阵的行。将这些取出的行，append 到一个 list 中去，最终的这个 list 就是所有的样本 X。标签不需要 one-hot 表示，只需要类别值，所以只用把 skip_grams 中每行的第二列取出来存起来即可</p><p>最后就是构建 dataset，然后定义 DataLoader</p><p><img src="https://cdn.jsdelivr.net/gh/stuxiaozhang/blogimage/img/20220328131035.png" srcset="/img/loading.gif" lazyload></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 用中心词生成背景词的概率</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_data</span>(<span class="hljs-params">skip_grams</span>):</span><br>    input_data = []<br>    output_data = []<br>    <br>    <span class="hljs-keyword">for</span> a, b <span class="hljs-keyword">in</span> skip_grams:<br>        input_data.append(np.eye(vocab_size)[a])  <span class="hljs-comment"># 就是数组里的数相当于在onehot数组里的位置是1</span><br>        output_data.append(b)<br><br>    <span class="hljs-keyword">return</span> input_data, output_data<br><br><br>input_data, output_data = make_data(skip_grams)<br>input_data, output_data = torch.Tensor(input_data), torch.LongTensor(output_data)<br>dataset = Data.TensorDataset(input_data, output_data)<br>loader = Data.DataLoader(dataset, batch_size, <span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><div class="note note-info"><p><code>np.eye()</code> 的函数，除了生成对角阵外，还可以将一个label数组，大小为(1,m)或者(m,1)的数组，转化成one-hot数组。</p><p>例如 可以将类别总数为6的 <code>labels=[1,2,3,0,1,1]</code> 的数组转化成数组 <code>[[0,1,0,0,0,0],[0,0,1,0,0,0],[0,0,0,1,0,0],[0,0,0,0,0,0],[0,1,0,0,0,0],[0,1,0,0,0,0]]</code> 这就是所谓的one-hot的形式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">labels=np.array([[<span class="hljs-number">1</span>],[<span class="hljs-number">2</span>],[<span class="hljs-number">0</span>],[<span class="hljs-number">1</span>]])<br>print(<span class="hljs-string">&quot;labels的大小：&quot;</span>,labels.shape,<span class="hljs-string">&quot;\n&quot;</span>)<br></code></pre></td></tr></table></figure><p>因为我们的类别是从0-2，所以这里是3个类</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a=np.eye(<span class="hljs-number">3</span>)[<span class="hljs-number">1</span>]<br>print(<span class="hljs-string">&quot;如果对应的类别号是1，那么转成one-hot的形式&quot;</span>,a,<span class="hljs-string">&quot;\n&quot;</span>)<br>--&gt; 如果对应的类别号是1，那么转成one-hot的形式 [0. 1. 0.]<br></code></pre></td></tr></table></figure></div><p>定义模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义模型</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Word2Vec</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(Word2Vec, self).__init__()<br>        self.W = nn.Parameter(torch.randn(vocab_size, embedding_size).<span class="hljs-built_in">type</span>(dtype))<br>        self.V = nn.Parameter(torch.randn(embedding_size, vocab_size).<span class="hljs-built_in">type</span>(dtype))<br>    <br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, X</span>):</span><br>        <span class="hljs-comment"># X: [batch_size, vocab_size]</span><br>        hidden = torch.mm(X, self.W)  <span class="hljs-comment"># [batch_size, m]</span><br>        output = torch.mm(hidden, self.V)  <span class="hljs-comment"># [batch_size, vocab_size]</span><br>        <span class="hljs-keyword">return</span> output<br>    <br>    <br>model = Word2Vec().to(device)<br>loss_fn = nn.CrossEntropyLoss().to(device)<br>optim = optimizer.Adam(model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br></code></pre></td></tr></table></figure><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># train</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2000</span>):<br>    <span class="hljs-keyword">for</span> i, (batch_x, batch_y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(loader):<br>        <span class="hljs-comment"># 先移到cuda上去</span><br>        batch_x = batch_x.to(device)<br>        batch_y = batch_y.to(device)<br>        pred = model(batch_x)<br>        loss = loss_fn(pred, batch_y)<br>        <br>        <span class="hljs-keyword">if</span>(epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>            print(epoch + <span class="hljs-number">1</span>, i, loss.item())<br>            <br>        optim.zero_grad()<br>        loss.backward()<br>        optim.step()<br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas mb-3"><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/PyTorch/">PyTorch</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2022/03/15/TextCNN%20%E7%9A%84%20PyTorch%20%E5%AE%9E%E7%8E%B0/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">TextCNN 的 PyTorch 实现</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2022/03/10/NNLM%20%E7%9A%84%20PyTorch%20%E5%AE%9E%E7%8E%B0/"><span class="hidden-mobile">NNLM 的 PyTorch 实现</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>