<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="Reasoning Like Human: Hierarchical Reinforcement Learning for Knowledge Graph Reasoning

关键词：Hierarchical Reinforcement Learning；KGR
IJCAI 2020

Basic Idea
1. 解决了什么问题？（what？）

多跳推理会遇到多个语义问题，即一"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>【RLH】Reasoning Like Human：Hierarchical Reinforcement Learning for Knowledge Graph Reasoning - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="【RLH】Reasoning Like Human：Hierarchical Reinforcement Learning for Knowledge Graph  Reasoning"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-03-02 09:43" pubdate>2022年3月2日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2.5k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 20 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">【RLH】Reasoning Like Human：Hierarchical Reinforcement Learning for Knowledge Graph Reasoning</h1><p class="note note-info">本文最后更新于：2022年3月4日</p><div class="markdown-body"><h1 id="reasoning-like-human-hierarchical-reinforcement-learning-for-knowledge-graph-reasoning">Reasoning Like Human: Hierarchical Reinforcement Learning for Knowledge Graph Reasoning</h1><ul><li>关键词：Hierarchical Reinforcement Learning；KGR</li><li>IJCAI 2020</li></ul><h2 id="basic-idea">Basic Idea</h2><h3 id="解决了什么问题what">1. 解决了什么问题？（what？）</h3><ul><li><p>多跳推理会遇到多个语义问题，即一个关系或实体具有多个含义。在链式推理过程中，语义歧义会累积，导致推理结果不理想。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220302110011268.png" srcset="/img/loading.gif" lazyload style="zoom:40%"></p></li><li><p><font color="red"><strong>解决了 KG 中的关系在多跳推理具有不同含义的多语义问题，这是一个基本但很少研究的问题</strong></font></p></li></ul><h3 id="用什么方法解决how">2. 用什么方法解决？（how？）</h3><ul><li><p>提出了一种新的分层强化学习框架：整个推理过程被分解为两级强化学习策略，用于编码历史信息和学习结构化动作空间。</p><p>HRL 将每个动作分解为子动作。通过学习多跳推理的每个子动作，agent 还可以通过推理链<strong>学习关系的潜在语义</strong>。因此，HRL公式能够训练具有高表达策略网络的代理来解决多语义问题。</p></li></ul><h2 id="rlhreasoning-like-human">RLH：Reasoning Like Human</h2><p>RLH 将推理的每个步骤分解为一个用于编码历史信息的高级 policy 和一个用于学习识别关系簇的低级 policy</p><ul><li>在高级策略中，由我们的模型训练的代理允许通过一个门控循环单元（GRU）编码和传输历史信息。</li><li>在底层策略中，agent 遵循分层决策，在不同的粒度级别学习关系簇的概念。</li><li>最后设计了一种联合训练方法来有效地优化我们模型的参数。</li></ul><h3 id="definations">Definations</h3><ul><li><p>KG: <span class="math inline">\(\mathcal{G}=(\mathcal{E}, \mathcal{R}, \mathcal{U})\)</span>，<span class="math inline">\(\mathcal{E}\)</span> 是实体集，<span class="math inline">\(\mathcal{R}\)</span> 是关系集，<span class="math inline">\(\mathcal{U}\)</span> 是 <span class="math inline">\((e_o, r, e_t)\)</span> 一条由头实体指向尾实体的边</p></li><li><p>RL：被描述为 MDP，<span class="math inline">\((S,A,P_a,R_a)\)</span>。在序列阶段的每个阶段，代理输入环境的状态 <span class="math inline">\(s\)</span>，从动作集 <span class="math inline">\(A_s\)</span> 中选择要执行的动作 $ a$。(环境会根据 agent 采取的 action) 输出给代理一个 Reward <span class="math inline">\(R(s,a)\)</span> 和状态转移概率 <span class="math inline">\(P(s&#39;|s,a)\)</span> 的即时奖励。</p></li><li><p>分层强化学习：被描述为 半MDP，<span class="math inline">\((S,A,P_a,R_a,\Phi)\)</span> 。 <span class="math display">\[ P_{a}\left(s, s^{\prime}\right)=\operatorname{Pr}\left(s_{t+1}=s^{\prime} \mid s_{t}=s, a_{t}=a\right) \prod_{i=1}^{K-1} \operatorname{Pr}\left(\phi_{i+1} \mid \Phi_{i}\right) \]</span></p><p><span class="math inline">\(\Phi\)</span> 是一个过渡函数空间，用于描述在动作a内过渡的K个阶段。每个 <span class="math inline">\(\phi\)</span> 都是动作 a 的一个子集，</p></li></ul><h3 id="overview">Overview</h3><p>为了应用分层认知机制，提出了一个基于 RL 推理的分层策略框架 <span class="math inline">\(\Phi=\{\phi_1,\phi_2,··,\phi_k\}\)</span>。图2的右框显示了一个三层分层策略。对于每个交互，代理观察一个动作空间 A，然后通过 <span class="math inline">\(\Phi\)</span> 从上下义概念中选择最有希望的子动作。在 KG Environment 中，动作空间的结构通常是层次结构。因此，复杂的动作空间被分层分解为搜索树等子任务。因此，关系的多重语义也被分解为更具体的表示。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220302215229287.png" srcset="/img/loading.gif" lazyload style="zoom:70%"></p><h3 id="high-level-policy-for-encoding-history-information">High Level Policy for Encoding History Information</h3><p>强化学习通过循序渐进的探索和开发，训练 agent 从与环境的交互中学习。在KG中，RL 用四元组 <span class="math inline">\((S,a,P,R)\)</span> 表示：</p><p><strong>States：</strong> <span class="math inline">\((e_{i-1}, r_i, e_i, e_t)\)</span>, <span class="math inline">\(e_i\)</span> 是当前实体，<span class="math inline">\(e_t\)</span> 是目标实体</p><p><strong>Actions：</strong>状态 <span class="math inline">\(s_i=(e_{i}, r_{i+1}, e_{i+1}, e_t)\)</span> 是当前实体 <span class="math inline">\(e_i\)</span> 的出边，动作集 <span class="math inline">\(A_{s_{i}}=\left\{(r, e) \mid\left(e_{i}, r, e\right) \in G, e \notin\left\{e_{o}, e_{1}, \cdots, e_{t}\right\}\right\}\)</span></p><p><strong>Transition：</strong>P 是用于确定下一状态概率分布的状态转移概率。策略网络对当前状态进行编码，以输出概率分布 <span class="math inline">\(P(s_{i+1} | s_i，a_i)\)</span></p><p><strong>Policy：</strong> <span class="math inline">\(\mu(s,A)=P(a|s;\theta)\)</span> (<span class="math inline">\(\mu\)</span> 是低级policy) <span class="math display">\[ \begin{array}{c} \mathbf{h}_{t}=G R U\left(\mathbf{h}_{t-1}, \mathbf{s}_{\mathbf{t}-\mathbf{1}}\right) \\ a \sim \pi\left(a_{t} \mid s_{t-1}\right)=\operatorname{softmax}\left(\mathbf{W}_{\pi} \mathbf{c}\right) \end{array} \]</span></p><ul><li><span class="math inline">\(c\)</span> 是低级 policy 的输出子动作（关系簇），<span class="math inline">\(\mathbf{c}\)</span> 是其向量表示。</li></ul><p><strong>Rewards：</strong>给定一对 <span class="math inline">\((e_o，e_t)\)</span>，如果代理到达目标实体，这条轨迹就成功。对于每一条的reward定义为： <span class="math display">\[ R_{H}\left(\tau_{i}\right)=\left\{\begin{array}{lr} 1 \cdot \gamma^{i}, &amp; \hat{e_{t}}=e_{t} \\ 0, &amp; \text { otherwise } \end{array}\right. \]</span></p><h3 id="low-level-policy-for-structured-action-space">Low Level Policy for Structured Action Space</h3><p>低级 policy 将复杂的动作空间分解为结构化的子动作。KGs 中的关系可以组成关系簇。可以通过层次聚类关系嵌入来构建层次关系聚类。这样，低级RL的所有状态都被组织为一个搜索树。每个关系的潜在多重语义都得到了很好的表达。</p><p><strong>Actions：</strong>首先对给定的数据集执行 TransE 并获得嵌入。然后用 k-means 算法对嵌入进行聚类，以初始化关系簇 <span class="math inline">\(C_1, C_2...C_n\)</span></p><p><strong>States：</strong>低级的 state <span class="math inline">\(s^l\)</span> 包含当前有效子动作的集合。对于一条轨迹，起始状态为 <span class="math inline">\(A_s\)</span>。如果成功，最终状态为 <span class="math inline">\(\{a_{t+1}\}\)</span>，否则为空</p><p><strong>Policy：</strong> 当 agent 观察状态 <span class="math inline">\(s^l_i\)</span> 下的子动作空间时，它启动当前的子任务 <span class="math display">\[ \begin{aligned} c &amp; \sim \mu\left(c_{t} \mid c_{t-1}, h_{t}, s_{t-1}\right) \\ &amp;=\operatorname{Softmax}\left(\operatorname{ReLU}\left(\sigma\left(\mathbf{W}_{\mathbf{s}}\left[\mathbf{h}_{\mathbf{t}} ; \mathbf{s}_{t-1}\right]\right)\left[s_{i}^{l}\right] \mathbf{W}_{\mathbf{c}} \mathbf{C}_{\mathbf{i}}\right)\right) \end{aligned} \]</span></p><ul><li>其中，<span class="math inline">\(c_t\)</span> 是 下一个子动作。</li></ul><p><strong>Reward：</strong>对于一条轨迹 <span class="math inline">\(\epsilon\)</span>，当最终状态包含正确的操作 <span class="math inline">\(a_{next}\)</span> 时，将获得奖励 <span class="math display">\[ R_{L}\left(\epsilon\right)=\left\{\begin{array}{lr} 1, &amp; a_{next} \in s^l \\ 0, &amp; \text { otherwise } \end{array}\right. \]</span></p><blockquote><p>没看懂。。。</p></blockquote><h3 id="optimization-and-training">Optimization and Training</h3><p>低级 policy 的目标函数是最大化层级决策累积回报的期望: <span class="math display">\[ J^{L}\left(\theta_{L}\right)=\mathbb{E}_{\epsilon \sim p_{\theta_{L} }(\epsilon)}\left[R_{L}(\epsilon)\right] \]</span></p><ul><li>其中，<span class="math inline">\(\epsilon\)</span> 是由低级 policy <span class="math inline">\(\mu\)</span> 的潜在分布 <span class="math inline">\(p_{\theta_{L} }(\epsilon)\)</span> 生成的 长度为M的轨迹。</li></ul><p>高级 policy 在低级 policy 下的目标函数为： <span class="math display">\[ J^{H}\left(\theta_{H}\right)=\mathbb{E}_{\theta_{L}, \tau \sim p_{\theta_{H}}(\tau)}\left[R_{H}(\tau)\right] \]</span></p><ul><li>其中，<span class="math inline">\(\tau\)</span> 由潜在分布 <span class="math inline">\(p_{\theta_{H} }(\tau)\)</span> 生成的 长度为N的轨迹。</li></ul><p>我们使用策略梯度法和 REINFORCE 算法来优化高层和底层策略。使用似然比技巧，策略的梯度产生： <span class="math display">\[ \frac{\partial J^{L}\left(\theta_{L}\right)}{\partial \theta_{L} } \approx \frac{1}{K} \sum_{j=1}^{K}\left[\sum_{\tau^{j}, i=1}^{M} \frac{\partial}{\partial \theta_{L}} \log \mu\left(a_{i}^{j} \mid s_{i-1}^{j}, a_{i-1}^{j}\right)\right] \]</span></p><p><span class="math display">\[ \frac{\partial J^{H}\left(\theta_{H}\right)}{\partial \theta_{H} } \approx \frac{1}{K} \sum_{j=1}^{K}\left[\sum_{\tau^ {j}, i=1}^{N} \frac{\partial}{\partial \theta_{H}} \log \pi\left(a_{i}^{j} \mid s_{i-1}^{j}, a_{i-1}^{j}, \theta_{L}\right) \gamma^{i}\right] \]</span></p><ul><li>其中，<span class="math inline">\(\pi\)</span> 和 <span class="math inline">\(\mu\)</span> 分别是高级 policy 和 低级 policy</li></ul><p>训练流程如下：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220302135837415.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><h2 id="experiments">Experiments</h2><h3 id="entity-link-prediction">Entity Link Prediction</h3><p>给出一个查询（h，r，？）或者（？，r，t），通过进行知识图推理来对实体进行排序，然后进行波束宽度为50的波束搜索，并根据轨迹到达正确实体的概率对实体进行排序。这样,，Hit@13，10和平均倒数排名（MRR）是从排名过程计算的，这是知识GRAH完成任务的标准度量。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220303152544741.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><ul><li>1对多 的比例 比 多对1 大，这表明 FB15K-237 具有多种语义关系。因此，在FB15K-237上，多跳推理方法（PRA、NeuraLP、MINERVA）的性能比单跳推理（TransE、ComplEX、Conv）差。<font color="red">表明多跳推理容易卡在具有高度中心性的本地节点上，导致无法到达正确的实体</font>。与 MINERVA 相比，我们的方法具有更好的性能，表明分层认知机制可以处理多个语义问题。</li></ul><h3 id="relationship-link-prediction">Relationship Link Prediction</h3><p>给定一个查询（h，？，t），关系预测是预测头部实体h和尾部实体t之间的关系。首先，我们删除KG中的地面真值关系r的所有链接。然后，代理尝试推断并通过KG到达目标实体。通过收集实体对之间的路径，我们将路径特征输入路径排序算法（PRA），该算法训练每关系分类器，以二元分类的方式预测基础真值关系r的存在。通过这种方式，对包含正查询对和负查询对的测试集进行评估，然后报告每个任务的平均精度（MAP）分数。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220303175540145.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220303185111754.png" srcset="/img/loading.gif" lazyload></p><ul><li><p>MINERVA添加了每条边的反向关系。因此，MINERVA训练的agent具有反向推理路径的能力。</p></li><li><p>当中间实体或关系是具有高度中心性的关键节点时，如表5中的“美利坚合众国”、“国家”和“电影”，这种操作会显著抑制推理过程。一旦代理位于此类节点，巨大的行动空间就会阻碍策略的决策。</p><p>因此，我们观察到在路径1-3、7-9中频繁出现具有高度中心性的关系。相比之下，我们的模型学习具有更广泛概念的推理链。例如，“国家”、“一级”和“州-省-地区”都具有“国家的子部分”的多重含义。简单来说，推理案例表明，我们的模型可以学习动作的结构语义来识别特定动作和子动作，从而实现改进。</p></li></ul><h2 id="summary">Summary</h2><p>本文研究了在多跳推理中，<strong>实体或关系具有多个语义</strong>的多语义问题。为了解决这个问题，设计了一个具有分层决策机制的 HRL 框架。该机制由高级 policy 和低级 policy 的层次结构实现。高级 policy了解历史信息。同时，低级 policy 负责学习子动作，并将每个整个动作空间划分为一个较小的动作空间。因此，还可以学习每个关系的多种语义。这样，我们提出的方法可以处理多跳推理任务中的多个语义问题。</p><p>我就没看懂他到底怎么处理的，还没开源。。。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/Papers/">Papers</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/KGR/">KGR</a> <a class="hover-with-bg" href="/tags/Reasoning-Like-Human/">Reasoning Like Human</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2022/03/10/NNLM%20%E7%9A%84%20PyTorch%20%E5%AE%9E%E7%8E%B0/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">NNLM 的 PyTorch 实现</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2022/03/01/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80_%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"><span class="hidden-mobile">强化学习基础_第一章 强化学习概述</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>