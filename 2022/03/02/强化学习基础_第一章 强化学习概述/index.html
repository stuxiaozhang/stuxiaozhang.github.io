<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="这是对 EasyRL 这个教程的学习记录。
第一章 强化学习概述Reinforcement Learning
强化学习讨论的问题是一个智能体(agent) 怎么在一个复杂不确定的环境(environment)里面去极大化它能获得的奖励。 示意图由两部分组成：agent 和 environment。在强化学习过程中，agent 跟 environment 一直在交互。Agent 在环境里面获取到状态"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>强化学习基础_第一章 强化学习概述 - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="强化学习基础_第一章 强化学习概述"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-03-02 17:02" pubdate>2022年3月2日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 6.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 48 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">强化学习基础_第一章 强化学习概述</h1><p class="note note-info">本文最后更新于：2022年3月3日</p><div class="markdown-body"><p>这是对 <a target="_blank" rel="noopener" href="https://datawhalechina.github.io/easy-rl/#/">EasyRL 这个教程</a>的学习记录。</p><h1 id="第一章-强化学习概述"><a href="#第一章-强化学习概述" class="headerlink" title="第一章 强化学习概述"></a>第一章 强化学习概述</h1><h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220302181732744.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><p><strong>强化学习讨论的问题是一个智能体(agent) 怎么在一个复杂不确定的环境(environment)里面去极大化它能获得的奖励。</strong> 示意图由两部分组成：agent 和 environment。在强化学习过程中，agent 跟 environment 一直在交互。Agent 在环境里面获取到状态，agent 会利用这个状态输出一个动作(action)，一个决策。然后这个决策会放到环境之中去，环境会根据 agent 采取的决策，输出下一个状态以及当前的这个决策得到的奖励。Agent 的目的就是为了尽可能多地从环境中获取奖励。</p><h3 id="对比-强化学习-和-监督学习"><a href="#对比-强化学习-和-监督学习" class="headerlink" title="对比 强化学习 和 监督学习"></a>对比 强化学习 和 监督学习</h3><ul><li>强化学习输入的是<strong>序列数据</strong>，而不是像监督学习里面这些样本都是<strong>独立</strong>的。</li><li><p>Agent 并没有被告诉你每一步正确的行为应该是什么。Agent 需要自己去发现哪些行为可以得到最多的奖励，只能通过不停地尝试来发现最有利的动作。Agent 获得自己能力的过程中，其实是通过不断地试错探索(trial-and-error exploration)。</p><ul><li>探索(exploration)和利用(exploitation)是强化学习里面非常核心的一个问题。</li><li>探索：你会去尝试一些新的行为，这些新的行为有可能会使你得到更高的奖励，也有可能使你一无所有。</li><li>利用：采取你已知的可以获得最大奖励的行为，你就重复执行这个动作就可以了，因为你已经知道可以获得一定的奖励。</li><li>因此，我们需要在探索和利用之间取得一个权衡，这也是在监督学习里面没有的情况。</li></ul></li><li><p>在强化学习过程中，没有非常强的监督者(supervisor)，只有一个 <strong>延迟的奖励信号</strong>(reward signal)，就是环境会在很久以后告诉你之前你采取的行为到底是不是有效的。Agent 在这个强化学习里面学习的话就非常困难，因为你没有得到即时反馈。</p><blockquote><p>当你采取一个行为过后，如果是监督学习，你就立刻可以获得一个指引，就说你现在做出了一个错误的决定，那么正确的决定应该是谁。而在强化学习里面，环境可能会告诉你这个行为是错误的，但是它并没有告诉你正确的行为是什么。而且更困难的是，它可能是在一两分钟过后告诉你错误，它再告诉你之前的行为到底行不行。所以这也是强化学习和监督学习不同的地方。</p></blockquote></li></ul><h3 id="强化学习的一些特征"><a href="#强化学习的一些特征" class="headerlink" title="强化学习的一些特征"></a>强化学习的一些特征</h3><ul><li><p>强化学习有这个 <strong>试错探索</strong>(trial-and-error exploration)，它需要通过探索环境来获取对环境的理解。</p></li><li><p>强化学习 agent 会从环境里面获得<strong>延迟的奖励</strong>。</p></li><li><p>在强化学习的训练过程中，时间非常重要。因为你得到的数据都是有<strong>时间关联的(sequential data)</strong>，而不是独立同分布的。</p><blockquote><p>在机器学习中，如果观测数据有非常强的关联，其实会使得这个<u>训练非常不稳定</u>。这也是为什么在监督学习中，我们希望数据尽量是独立同分布，这样就可以消除数据之间的相关性。</p></blockquote></li><li><p><strong>Agent 的行为会影响到它随后从环境得到的数据</strong>。在训练 agent 的过程中，是通过正在学习的 agent 和环境交互来得到数据。如果在训练过程中，agent 的模型很快死掉了，那会使得我们采集到的数据是非常糟糕的，这样整个训练过程就失败了。所以在强化学习里面一个非常重要的问题就是怎么让这个 agent 的行为一直稳定地提升。</p></li></ul><h3 id="深度强化学习-Deep-Reinforcemet-Learning"><a href="#深度强化学习-Deep-Reinforcemet-Learning" class="headerlink" title="深度强化学习(Deep Reinforcemet Learning)"></a>深度强化学习(Deep Reinforcemet Learning)</h3><p>深度强化学习 = 深度学习 + 强化学习。就是把神经网络放到强化学习里面。</p><ul><li>Standard RL：之前的强化学习，比如 TD-Gammon 玩 backgammon 这个游戏，它其实是设计特征，然后通过训练价值函数的一个过程，就是它先设计了很多手工的特征，这个手工特征可以描述现在整个状态。得到这些特征过后，它就可以通过训练一个分类网络或者分别训练一个价值估计函数来做出决策。</li><li>Deep RL：现在我们有了深度学习，有了神经网络，那么大家也把这个过程改进成一个端到端训练(end-to-end training)的过程。你直接输入这个状态，我们不需要去手工地设计这个特征，就可以让它直接输出动作。那么就可以用一个神经网络来拟合我们这里的价值函数或策略网络，省去了特征工程(feature engineering)的过程。</li></ul><h2 id="Introduction-to-Sequential-Decision-Making"><a href="#Introduction-to-Sequential-Decision-Making" class="headerlink" title="Introduction to Sequential Decision Making"></a>Introduction to Sequential Decision Making</h2><h3 id="Agent-and-Environment"><a href="#Agent-and-Environment" class="headerlink" title="Agent and Environment"></a>Agent and Environment</h3><p>强化学习研究的问题是 agent 跟环境交互，上图左边画的是一个 agent，agent 一直在跟环境进行交互。这个 agent 把它输出的动作给环境，环境取得这个动作过后，会进行到下一步，然后会把下一步的观测跟它上一步是否得到奖励返还给 agent。</p><p>通过这样的交互过程会产生很多观测，agent 的目的是从这些观测之中学到能极大化奖励的策略。</p><h3 id="Reward"><a href="#Reward" class="headerlink" title="Reward"></a>Reward</h3><p>奖励是由环境给的一个标量的反馈信号(scalar feedback signal)，这个信号显示了 agent 在某一步采取了某个策略的表现如何。</p><p>强化学习的目的就是为了最大化 agent 可以获得的奖励，agent 在这个环境里面存在的目的就是为了极大化它的期望的累积奖励(expected cumulative reward)。</p><h3 id="序列决策-Sequential-Decision-Making"><a href="#序列决策-Sequential-Decision-Making" class="headerlink" title="序列决策(Sequential Decision Making)"></a>序列决策(Sequential Decision Making)</h3><p>在一个强化学习环境里面，agent 的目的就是选取一系列的动作来极大化它的奖励，所以这些采取的动作必须有长期的影响。但在这个过程里面，它的奖励其实是被延迟了，就是说你现在采取的某一步决策可能要等到时间很久过后才知道这一步到底产生了什么样的影响。强化学习里面一个重要的课题就是近期奖励和远期奖励的一个<strong>权衡(trade-off)</strong>。怎么让 agent 取得更多的长期奖励是强化学习的问题。</p><p>在跟环境的交互过程中，agent 会获得很多观测。在每一个观测会采取一个动作，它也会得到一个奖励。<strong>所以历史是观测(observation)、行为、奖励的序列：</strong></p><script type="math/tex;mode=display">H_t = O_1,R_1,A_1, ... ,A_{t-1}, O_t,R_t</script><p>Agent 在采取当前动作的时候会依赖于它之前得到的这个历史，<strong>所以你可以把整个游戏的状态看成关于这个历史的函数：</strong></p><script type="math/tex;mode=display">S_t = f(H_t)</script><p>Q: 状态和观测有什么关系?</p><p>A: 状态(state) 是对世界的完整描述，不会隐藏世界的信息。观测(observation) o 是对状态的部分描述，可能会遗漏一些信息。在 deep RL 中，我们几乎总是用一个实值的向量、矩阵或者更高阶的张量来表示状态和观测。举个例子，我们可以用 RGB 像素值的矩阵来表示一个视觉的观测，我们可以用机器人关节的角度和速度来表示一个机器人的状态。</p><p>环境有自己的函数 $S<em>{t}^{e}=f^{e}\left(H</em>{t}\right)$ 来更新状态，在 agent 的内部也有一个函数 $S<em>{t}^{a}=f^{a}\left(H</em>{t}\right)$ 来更新状态。当 agent 的状态跟环境的状态等价的时候，我们就说这个环境是 <code>full observability</code>，就是全部可以观测。换句话说，当 agent 能够观察到环境的所有状态时，我们称这个环境是<code>完全可观测的(fully observed)</code>。在这种情况下面，强化学习通常被建模成一个 Markov decision process(MDP)的问题。在 MDP 中， $O<em>{t}=S</em>{t}^{e}=S_{t}^{a}$。</p><p>但是有一种情况是 agent 得到的观测并不能包含环境运作的所有状态，因为在这个强化学习的设定里面，环境的状态才是真正的所有状态。</p><ul><li>比如 agent 在玩这个 black jack 这个游戏，它能看到的其实是牌面上的牌。</li><li>或者在玩雅达利游戏的时候，观测到的只是当前电视上面这一帧的信息，你并没有得到游戏内部里面所有的运作状态。</li></ul><p>也就是说当 agent 只能看到部分的观测，我们就称这个环境是<code>部分可观测的(partially observed)</code>。在这种情况下面，强化学习通常被建模成一个 POMDP 的问题。</p><p><code>部分可观测马尔可夫决策过程(Partially Observable Markov Decision Processes, POMDP)</code> 是一个马尔可夫决策过程的泛化。POMDP 依然具有马尔可夫性质，但是假设智能体无法感知环境的状态 s，只能知道部分观测值 o。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。</p><p>POMDP 可以用一个 7 元组描述：$(S,A,T,R,\Omega,O,\gamma)$，其中 S 表示状态空间，为隐变量，A 为动作空间，$T(s’|s,a)$ 为状态转移概率，$R$ 为奖励函数，$\Omega(o|s,a)$ 为观测概率，$O$ 为观测空间，$\gamma$ 为折扣系数。</p><h2 id="Action-Spaces"><a href="#Action-Spaces" class="headerlink" title="Action Spaces"></a>Action Spaces</h2><p>不同的环境允许不同种类的动作。在给定的环境中，有效动作的集合经常被称为<code>动作空间(action space)</code>。像 Atari 和 Go 这样的环境有<code>离散动作空间(discrete action spaces)</code>，在这个动作空间里，agent 的动作数量是有限的。在其他环境，比如在物理世界中控制一个 agent，在这个环境中就有<code>连续动作空间(continuous action spaces)</code> 。在连续空间中，动作是实值的向量。</p><p>例如，</p><ul><li>走迷宫机器人如果只有东南西北这 4 种移动方式，则其为离散动作空间；</li><li>如果机器人向 $360^{\circ}$ 中的任意角度都可以移动，则为连续动作空间。</li></ul><h2 id="Major-Components-of-an-RL-Agent"><a href="#Major-Components-of-an-RL-Agent" class="headerlink" title="Major Components of an RL Agent"></a>Major Components of an RL Agent</h2><p>对于一个强化学习 agent，它可能有一个或多个如下的组成成分：</p><ul><li><code>策略函数(policy function)</code>，agent 会用这个函数来选取下一步的动作。</li><li><code>价值函数(value function)</code>，我们用价值函数来对当前状态进行估价，它就是说你进入现在这个状态，可以对你后面的收益带来多大的影响。当这个价值函数大的时候，说明你进入这个状态越有利。</li><li><code>模型(model)</code>，模型表示了 agent 对这个环境的状态进行了理解，它决定了这个世界是如何进行的。</li></ul><h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><p>Policy 是 agent 的行为模型，它决定了这个 agent 的行为，它其实是一个函数，把输入的状态变成行为。这里有两种 policy：</p><ul><li>一种是 <code>stochastic policy(随机性策略)</code>，它就是 $\pi$ 函数 $\pi(a | s)=P\left[A<em>{t}=a | S</em>{t}=s\right]$。当你输入一个状态 s 的时候，输出是一个概率。这个概率就是你所有行为的一个概率，然后你可以进一步对这个概率分布进行采样，得到真实的你采取的行为。比如说这个概率可能是有 70% 的概率往左，30% 的概率往右，那么你通过采样就可以得到一个 action。</li><li>一种是 <code>deterministic policy(确定性策略)</code>，就是说你这里有可能只是采取它的极大化，采取最有可能的动作，即 $a^{*}=\arg \underset{a}{\max} \pi(a \mid s)$。 你现在这个概率就是事先决定好的。</li></ul><p>通常情况下，强化学习一般使用 <code>随机性策略</code>。随机性策略有很多优点：</p><ul><li>在学习时可以通过引入一定随机性来更好地探索环境；</li><li>随机性策略的动作具有多样性，这一点在多个智能体博弈时也非常重要。采用确定性策略的智能体总是对同样的环境做出相同的动作，会导致它的策略很容易被对手预测。</li></ul><h3 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value Function"></a>Value Function</h3><p>价值函数的定义其实是一个期望，如下式所示：</p><script type="math/tex;mode=display">v_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s\right], \text { for all } s \in \mathcal{S}</script><p>这里有一个期望 $\mathbb{E}_{\pi}$，这里有个小角标是 $\pi$ 函数，这个 $\pi$ 函数就是说在我们已知某一个策略函数的时候，到底可以得到多少的奖励。$\gamma$ 是 <code>discount factor(折扣因子)</code>，我们希望尽可能在短的时间里面得到尽可能多的奖励，所以就通过把这个折扣因子放到价值函数的定义里面。</p><p>还有一种价值函数：Q 函数。Q 函数里面包含两个变量：状态和动作，其定义如下式所示：</p><script type="math/tex;mode=display">q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s, A_{t}=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s, A_{t}=a\right]</script><p>所以未来可以获得多少的奖励，这个期望取决于你当前的状态和当前的行为。这个 Q 函数是强化学习算法里面要学习的一个函数。因为当我们得到这个 Q 函数后，进入某一种状态，它最优的行为就可以通过这个 Q 函数来得到。</p><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p><strong>模型决定了下一个状态会是什么样的，就是说下一步的状态取决于你当前的状态以及你当前采取的行为。</strong>它由两个部分组成，</p><ul><li>概率：这个转移状态之间是怎么转移的，如下式所示：<script type="math/tex;mode=display">\mathcal{P}_{s s^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right]</script></li></ul><ul><li>奖励函数：当你在当前状态采取了某一个行为，可以得到多大的奖励，如下式所示：<script type="math/tex;mode=display">\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]</script></li></ul><p>当我们有了这三个组成部分过后，就形成了一个 <code>马尔可夫决策过程(Markov Decision Process)</code>。这个决策过程可视化了状态之间的转移以及采取的行为。</p><h2 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h2><h3 id="1-Keywords"><a href="#1-Keywords" class="headerlink" title="1 Keywords"></a>1 Keywords</h3><ul><li><strong>强化学习（Reinforcement Learning）</strong>：Agent可以在与复杂且不确定的Environment进行交互时，尝试使所获得的Reward最大化的计算算法。</li><li><strong>Action</strong>: Environment接收到的Agent当前状态的输出。</li><li><strong>State</strong>：Agent从Environment中获取到的状态。</li><li><strong>Reward</strong>：Agent从Environment中获取的反馈信号，这个信号指定了Agent在某一步采取了某个策略以后是否得到奖励</li><li><strong>Exploration</strong>：在当前的情况下，继续尝试<strong>新的</strong>Action，其有可能会使你得到更高的这个奖励，也有可能使你一无所有。</li><li><p><strong>Exploitation</strong>：在当前的情况下，继续尝试<strong>已知的</strong>可以获得最大Reward的过程，即重复执行这个 Action 就可以了。</p></li><li><p><strong>深度强化学习（Deep Reinforcement Learning）</strong>：不需要手工设计特征，仅需要输入State让系统直接输出Action的一个end-to-end training的强化学习方法。通常使用神经网络来拟合 value function 或者 policy network。</p></li><li><strong>Full observability、fully observed和partially observed</strong>：当Agent的状态跟Environment的状态等价的时候，我们就说现在Environment是full observability（全部可观测），当Agent能够观察到Environment的所有状态时，我们称这个环境是fully observed（完全可观测）。一般我们的Agent不能观察到Environment的所有状态时，我们称这个环境是partially observed（部分可观测）。</li><li><strong>POMDP（Partially Observable Markov Decision Processes）</strong>：部分可观测马尔可夫决策过程，即马尔可夫决策过程的泛化。POMDP 依然具有马尔可夫性质，但是假设智能体无法感知环境的状态 s，只能知道部分观测值 o。</li><li><strong>Action space（discrete action spaces and continuous action spaces）</strong>：在给定的Environment中，有效动作的集合经常被称为动作空间（Action space），Agent的动作数量是有限的动作空间为离散动作空间（discrete action spaces），反之，称为连续动作空间（continuous action spaces）。</li><li><strong>policy-based（基于策略的）</strong>：Agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。</li><li><strong>valued-based（基于价值的）</strong>：Agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。</li><li><strong>model-based（有模型结构）</strong>：Agent通过学习状态的转移来采取措施。</li><li><strong>model-free（无模型结构）</strong>：Agent没有去直接估计状态的转移，也没有得到Environment的具体转移变量。它通过学习 value function 和 policy function 进行决策。</li></ul><h3 id="2-Questions"><a href="#2-Questions" class="headerlink" title="2 Questions"></a>2 Questions</h3><ul><li><p>强化学习的基本结构是什么？</p><p>答：本质上是Agent和Environment间的交互。具体地，当Agent在Environment中得到当前时刻的State，Agent会基于此状态输出一个Action。然后这个Action会加入到Environment中去并输出下一个State和当前的这个Action得到的Reward。Agent在Environment里面存在的目的就是为了极大它的期望积累的Reward。</p></li><li><p>强化学习相对于监督学习为什么训练会更加困难？（强化学习的特征）</p><ol><li>强化学习处理的多是序列数据，其很难像监督学习的样本一样满足<strong>IID（独立同分布）</strong>条件。</li><li>强化学习有奖励的延迟（Delay Reward），即在Agent的action作用在Environment中时，Environment对于Agent的State的<strong>奖励的延迟</strong>（Delayed Reward），使得反馈不及时。</li><li>相比于监督学习有正确的label，可以通过其修正自己的预测，强化学习相当于一个“试错”的过程，其完全根据Environment的“<strong>反馈</strong>”更新对自己最有利的Action。</li></ol></li><li><p>强化学习的基本特征有哪些？</p><ol><li>有<strong>trial-and-error exploration</strong>的过程，即需要通过探索Environment来获取对这个Environment的理解。</li><li>强化学习的Agent会从Environment里面获得<strong>延迟</strong>的Reward。</li><li>强化学习的训练过程中<strong>时间</strong>非常重要，因为数据都是有时间关联的，而不是像监督学习一样是IID分布的。</li><li>强化学习中Agent的Action会<strong>影响</strong>它随后得到的<strong>反馈</strong>。</li></ol></li><li><p>近几年强化学习发展迅速的原因？</p><ol><li><strong>算力（GPU、TPU）的提升</strong>，我们可以更快地做更多的 trial-and-error 的尝试来使得Agent在Environment里面获得很多信息，取得更大的Reward。</li><li>我们有了深度强化学习这样一个端到端的训练方法，可以把特征提取和价值估计或者决策一起优化，这样就可以得到一个更强的决策网络。</li></ol></li><li><p>状态和观测有什么关系？</p><p>答：状态（state）是对世界的<strong>完整描述</strong>，不会隐藏世界的信息。观测（observation）是对状态的<strong>部分描述</strong>，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用一个实值向量、矩阵或者更高阶的张量来表示状态和观测。</p></li><li><p>对于一个强化学习 Agent，它由什么组成？</p><ol><li><strong>策略函数（policy function）</strong>，Agent会用这个函数来选取它下一步的动作，包括<strong>随机性策略（stochastic policy）</strong>和<strong>确定性策略（deterministic policy）</strong>。</li><li><strong>价值函数（value function）</strong>，我们用价值函数来对当前状态进行评估，即进入现在的状态，到底可以对你后面的收益带来多大的影响。当这个价值函数大的时候，说明你进入这个状态越有利。</li><li><strong>模型（model）</strong>，其表示了 Agent 对这个Environment的状态进行的理解，它决定了这个系统是如何进行的。</li></ol></li><li><p>根据强化学习 Agent 的不同，我们可以将其分为哪几类？</p><ol><li><strong>基于价值函数的Agent</strong>。 显式学习的就是价值函数，隐式的学习了它的策略。因为这个策略是从我们学到的价值函数里面推算出来的。</li><li><strong>基于策略的Agent</strong>。它直接去学习 policy，就是说你直接给它一个 state，它就会输出这个动作的概率。然后在这个 policy-based agent 里面并没有去学习它的价值函数。</li><li>然后另外还有一种 Agent 是把这两者结合。把 value-based 和 policy-based 结合起来就有了 <strong>Actor-Critic agent</strong>。这一类 Agent 就把它的策略函数和价值函数都学习了，然后通过两者的交互得到一个更佳的状态。</li></ol></li><li><p>基于策略迭代和基于价值迭代的强化学习方法有什么区别?</p><ol><li><p>基于策略迭代的强化学习方法，agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励；基于价值迭代的强化学习方法，agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。</p></li><li><p>基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于行为集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)</p></li><li>基于价值迭代的强化学习算法有 Q-learning、 Sarsa 等，而基于策略迭代的强化学习算法有策略梯度算法等。</li><li>此外， Actor-Critic 算法同时使用策略和价值评估来做出决策，其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。</li></ol></li><li><p>有模型（model-based）学习和免模型（model-free）学习有什么区别？</p><p>答：针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习。 有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。总的来说，<strong>有模型学习相比于免模型学习仅仅多出一个步骤，即对真实环境进行建模</strong>。免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。免模型学习的泛化性要优于有模型学习，原因是有模型学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。</p></li><li><p>强化学习的通俗理解</p><p>答：environment 跟 reward function 不是我们可以控制的，<strong>environment 跟 reward function 是在开始学习之前，就已经事先给定的</strong>。我们唯一能做的事情是<strong>调整 actor 里面的 policy，使得 actor 可以得到最大的 reward</strong>。Actor 里面会有一个 policy， 这个 policy 决定了actor 的行为。Policy 就是给一个外界的输入，然后它会输出 actor 现在应该要执行的行为。</p></li></ul><h3 id="3-Something-About-Interview"><a href="#3-Something-About-Interview" class="headerlink" title="3 Something About Interview"></a>3 Something About Interview</h3><ul><li><p>高冷的面试官: 看来你对于RL还是有一定了解的,那么可以用一句话谈一下你对于强化学习的认识吗?</p><p>答: 强化学习包含环境, 动作和奖励三部分, 其本质是agent通过与环境的交互,使得其作出的action所得到的决策得到的总的奖励达到最大,或者说是期望最大。</p></li><li><p>高冷的面试官: 你认为强化学习与监督学习和无监督学习有什么区别?</p><p>答：</p><ul><li>首先强化学习和无监督学习是不需要标签的,而监督学习需要许多有标签的样本来进行模型的构建；</li><li>对于强化学习与无监督学习,无监督学习是直接对于给定的数据进行建模,寻找数据(特征)给定的隐藏的结构,一般对应的聚类问题,而强化学习需要通过<u>延迟奖励学习策略</u>来得到”模型”对于正确目标的远近(通过奖励惩罚函数进行判断),这里我们可以将奖励惩罚函数视为正确目标的一个稀疏、延迟形式。</li><li>另外强化学习处理的多是序列数据,样本之间通常具有强相关性，但其很难像监督学习的样本一样满足 IID 条件。</li></ul></li><li><p>高冷的面试官: 强化学习中所谓的损失函数与DL中的损失函数有什么区别呀?</p><p>答: DL中的loss function目的是使预测值和真实值之间的差距最小,而RL中的loss function是是奖励和期望最大。</p></li><li><p>高冷的面试官: 你了解model-free和model-based吗?两者有什么区别呢?</p><p>答: 两者的区别主要在于是否需要对于真实的环境进行建模, model-free不需要对于环境进行建模,直接与真实环境进行交互即可,所以其通常需要较大的数据或者采样工作来优化策略,这也帮助model-free对于真实环境具有更好的泛化性能; 而model-based 需要对于环境进行建模,同时再真实环境与虚拟环境中进行学习,如果建模的环境与真实环境的差异较大,那么会限制其泛化性能。现在通常使用model-free进行模型的构建工作。</p></li></ul></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/RL/">RL</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/2022/02/27/%E3%80%90DeepPath%E3%80%91A%20Reinforcement%20Learning%20Method%20for%20Knowledge%20Graph%20Reasoning/"><span class="hidden-mobile">【DeepPath】A Reinforcement Learning Method for Knowledge Graph Reasoning</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>