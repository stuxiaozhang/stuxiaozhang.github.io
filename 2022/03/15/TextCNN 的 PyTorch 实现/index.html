<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="TextCNN 的 PyTorch 实现
本文主要介绍一篇将 CNN 应用到 NLP 领域的一篇论文 Convolutional Neural Networks for Sentence Classification，然后给出 PyTorch 实现。
论文比较短，总体流程不太复杂，最主要的是下面这张图，只要理解了这张图，就知道如何写代码了。

下图的 feature map 是将一句话中的"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>sa - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="sa"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-03-15 21:07" pubdate>2022年3月15日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.2k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 11 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">sa</h1><p class="note note-info">本文最后更新于：2022年3月15日</p><div class="markdown-body"><h1 id="textcnn-的-pytorch-实现">TextCNN 的 PyTorch 实现</h1><p>本文主要介绍一篇将 CNN 应用到 NLP 领域的一篇论文 <a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/D14-1181.pdf">Convolutional Neural Networks for Sentence Classification</a>，然后给出 PyTorch 实现。</p><p>论文比较短，总体流程不太复杂，最主要的是下面这张图，只要理解了这张图，就知道如何写代码了。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220315211019818.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><p>下图的 feature map 是将一句话中的各个词通过 Word Embedding 得到的，feature map 的宽为 embedding 的维度，长为一句话的单词数量。例如下图中，很明显就是用一个 6 维的向量去编码每个词，并且一句话中有 9 个词。</p><p>之所以有两张 feature map，可以理解为 batchsize 为 2。</p><p>其中，红色的框代表的就是卷积核。而且很明显可以看出，这是一个长宽不等的卷积核。有意思的是，卷积核的宽可以认为是 n-gram，比方说下图卷积核宽为 2，所以同时考虑了 &quot;wait&quot; 和 &quot;for&quot; 两个单词的词向量，因此可以认为该卷积是一个类似于 bigram 的模型</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220315211347361.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>后面的部分就是传统 CNN 的步骤，激活、池化、Flatten</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220315211722668.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><h3 id="代码实现pytorch-版">代码实现（PyTorch 版）</h3><p>源码来自于 <a target="_blank" rel="noopener" href="https://github.com/graykode/nlp-tutorial">nlp-tutorial</a>，<a target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1445/">王铃鑫同学</a>在其基础上进行了修改。</p><p>导入库.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> Data<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br>dtype = torch.FloatTensor<br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br></code></pre></td></tr></table></figure><p>定义一些数据，以及设置一些常规参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 3 words sentences (=sequence_length is 3)</span><br>sentences = [<span class="hljs-string">&quot;i love you&quot;</span>, <span class="hljs-string">&quot;he loves me&quot;</span>, <span class="hljs-string">&quot;she likes baseball&quot;</span>, <span class="hljs-string">&quot;i hate you&quot;</span>, <span class="hljs-string">&quot;sorry for that&quot;</span>, <span class="hljs-string">&quot;this is awful&quot;</span>]<br>labels = [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]  <span class="hljs-comment"># 1 is good, 0 is not good.</span><br><br><span class="hljs-comment"># TextCNN</span><br>embedding_size = <span class="hljs-number">2</span><br>sequence_length = <span class="hljs-built_in">len</span>(sentences[<span class="hljs-number">0</span>])  <span class="hljs-comment"># every sentences contains sequence_length(=3) words</span><br>num_classes =<span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(labels))  <span class="hljs-comment"># 几分类，这里是二分类</span><br>batch_size = <span class="hljs-number">3</span><br><br>word_list = <span class="hljs-string">&quot; &quot;</span>.join(sentences).split()<br>vocab = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(word_list))<br>vocab_size = <span class="hljs-built_in">len</span>(vocab)<br>word2idx = &#123;w:i <span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocab)&#125;<br></code></pre></td></tr></table></figure><p>数据预处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_data</span>(<span class="hljs-params">sentences, labels</span>):</span><br>    inputs = []<br>    <span class="hljs-keyword">for</span> sen <span class="hljs-keyword">in</span> sentences:<br>        inputs.append([word2idx[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sen.split()])  <span class="hljs-comment"># 把sentences中的每个句子拿出来，分成单词，找到对应idx加入 inputs</span><br>        <br>    targets = []<br>    <span class="hljs-keyword">for</span> out <span class="hljs-keyword">in</span> labels:<br>        targets.append(out)   <span class="hljs-comment"># To using Torch Softmax Loss function</span><br>        <br>    <span class="hljs-keyword">return</span> inputs, targets<br><br>input_batch, targets_batch = make_data(sentences, labels)<br>input_batch, targets_batch = torch.LongTensor(input_batch), torch.LongTensor(targets_batch)<br><br>dataset = Data.TensorDataset(input_batch, targets_batch)<br>loader = Data.DataLoader(dataset, batch_size, <span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><div class="note note-primary"><ul><li><code>TensorDataset</code> 可以用来对 tensor 进行打包，把数据放在数据库中, 就好像 python 中的 zip 功能。该类通过每一个 tensor 的第一个维度进行索引。</li><li><code>DataLoader</code> 从数据库中每次抽出 batch size 个样本</li></ul></div><p>构建 TextCNN 模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TextCNN</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(TextCNN, self).__init__()<br>        self.W = nn.Embedding(vocab_size, embedding_size)<br>        output_channel = <span class="hljs-number">3</span><br>        self.conv = nn.Sequential(<br>            <span class="hljs-comment"># conv : [input_channel(=1), output_channel, (filter_height, filter_width), stride=1]</span><br>            nn.Conv2d(<span class="hljs-number">1</span>, output_channel, (<span class="hljs-number">2</span>, embedding_size)),  <span class="hljs-comment"># [batch_size, output_channel * 2 * 1]</span><br>            nn.ReLU(),<br>            <span class="hljs-comment"># pool : ((filter_height, filter_width))</span><br>            nn.MaxPool2d((<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)),<br>        )<br>        <span class="hljs-comment"># fc</span><br>        self.fc = nn.Linear(output_channel, num_classes)<br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, X</span>):</span><br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        X: [batch_size, sequence_length]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        batch_size = X.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 因为测试的时候不知道有几个句子，所以在这里动态的获取一下</span><br>        embedding_X = self.W(X)  <span class="hljs-comment"># [batch_size, sequence_length, embedding_size]</span><br>        <br>        embedding_X = embedding_X.unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># add channel(=1) [batch, channel(=1), sequence_length, embedding_size]</span><br>        conved = self.conv(embedding_X)  <span class="hljs-comment"># 卷积之后：[batch_size, output_channel，1，1]</span><br>        <br>        flatten = conved.view(batch_size, -<span class="hljs-number">1</span>)  <span class="hljs-comment"># [batch_size, output_channel * 1 * 1]</span><br>        output = self.fc(flatten)<br>        <br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><p>详细介绍一下数据在网络中流动的过程中维度的变化:</p><p>输入数据是个矩阵，矩阵维度为 <code>[batch_size, sequence_length]</code>，输入矩阵的数字代表的是某个词在整个词库中的索引（下标）</p><p>首先通过 Embedding 层，也就是查表，将每个索引转为一个向量，比方说 <code>12</code> 可能会变成 <code>[0.3,0.6,0.12,...]</code>，因此整个数据无形中就增加了一个维度，变成了 <code>[batch_size, sequence_length, embedding_size]</code></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220315212613306.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>之后使用 <code>unsqueeze(1)</code> 函数使数据增加一个维度，变成 <code>[batch_size, 1, sequence_length, embedding_size]</code>。现在的数据才能做卷积，因为在传统 CNN 中，输入数据就应该是 <code>[batch_size, in_channel, height, width]</code> 这种维度</p><p><code>[batch_size, 1, 3, 2]</code> 的输入数据通过 <code>nn.Conv2d(1, 3, (2, 2))</code> 的卷积之后，得到的就是 <code>[batch_size, 3, 2, 1]</code> 的数据，由于经过 ReLU 激活函数是不改变维度的，所以就没画出来。最后经过一个 <code>nn.MaxPool2d((2, 1))</code> 池化，得到的数据维度就是 <code>[batch_size, 3, 1, 1]</code></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220315212700591.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">model = TextCNN().to(device)<br>criterion = nn.CrossEntropyLoss().to(device)<br>optimizer = optim.Adam(model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5000</span>):<br>    <span class="hljs-keyword">for</span> batch_x, batch_y <span class="hljs-keyword">in</span> loader:<br>        batch_x, batch_y = batch_x.to(device), batch_y.to(device)<br>        pred = model(batch_x)<br>        loss = criterion(pred, batch_y)<br>        <br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>            print(<span class="hljs-string">&#x27;Epoch:&#x27;</span>, <span class="hljs-string">&#x27;%04d&#x27;</span> % (epoch + <span class="hljs-number">1</span>), <span class="hljs-string">&#x27;loss=&#x27;</span>, <span class="hljs-string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(loss))<br>            <br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br></code></pre></td></tr></table></figure><p>测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Test</span><br>test_text = <span class="hljs-string">&#x27;i hate me&#x27;</span><br>tests = [[word2idx[n] <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> test_text.split()]]<br>test_batch = torch.LongTensor(tests).to(device)<br><br><span class="hljs-comment"># Predict</span><br>model = model.<span class="hljs-built_in">eval</span>()<br>predict = model(test_batch).data.<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]<br><span class="hljs-keyword">if</span> predict[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] == <span class="hljs-number">0</span>:<br>    print(test_text,<span class="hljs-string">&quot;is Bad Mean...&quot;</span>)<br><span class="hljs-keyword">else</span>:<br>    print(test_text,<span class="hljs-string">&quot;is Good Mean!!&quot;</span>)<br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas mb-3"></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/2022/03/10/Word2Vec%20%E7%9A%84%20PyTorch%20%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B9%9E%E4%B8%90%E7%89%88%EF%BC%89/"><span class="hidden-mobile">Word2Vec 的 PyTorch 实现（乞丐版）</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>