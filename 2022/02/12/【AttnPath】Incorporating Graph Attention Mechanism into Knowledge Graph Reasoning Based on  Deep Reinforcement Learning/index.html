<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="【AttnPath】Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning
本文发表在 EMNLP 2019 上。
Basic Idea
1. 解决了什么问题？（what？）

以前的许多基于路径的方法，如 PRA 和 DeepP"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>【AttnPath】Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="【AttnPath】Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on  Deep Reinforcement Learning"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-02-12 20:12" pubdate>2022年2月12日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 23 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">【AttnPath】Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning</h1><p class="note note-info">本文最后更新于：2022年3月6日</p><div class="markdown-body"><h1 id="attnpathincorporating-graph-attention-mechanism-into-knowledge-graph-reasoning-based-on-deep-reinforcement-learning">【AttnPath】Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning</h1><p>本文发表在 EMNLP 2019 上。</p><h2 id="basic-idea">Basic Idea</h2><h3 id="解决了什么问题what">1. 解决了什么问题？（what？）</h3><ul><li>以前的许多基于路径的方法，如 PRA 和 DeepPath，都存在缺少记忆组件，或者陷入训练过程的问题。因此，他们的表现<strong>依赖于预训练</strong>这个过程。预训练操作需要为模型训练提供许多已知路径。这种蛮力操作可能会使模型在预训练的给定路径上<strong>容易过拟合</strong></li><li>在训练时，为 KG 中的不同关系设置相同的超参数是不合适的，这<strong>忽略了实体之间联系的多样性</strong>。最后，当代理选择一条无效路径时，它会停止并重新选择，这会导致不断选择该无效路径，最终卡在一个节点上。</li></ul><h3 id="用什么方法解决how">2. 用什么方法解决？（how？）</h3><ul><li>提出了一个基于深度强化学习的 <strong>AttnPath</strong> 模型，该模型结合了 LSTM 和 图注意机制 作为记忆组件，不需要预训练。</li><li>提出了一种<strong>新的强化学习机制</strong>改进训练过程: <strong>强制 agent 每一步都向前走</strong>，来避免 agent 在同一实体节点上停滞不前。</li><li><strong>定义了两个度量: 平均选择率(MSR) 和平均替换率(MRR)</strong>，来定量衡量 查询关系 的学习难度，并利用它们在 RL framework 下对模型进行微调。</li></ul><h2 id="attnpath">AttnPath</h2><h3 id="rl-framework-for-kg-reasoning">1. RL framework for KG reasoning</h3><ul><li><p><strong>Environment:</strong> 环境指的是整个 KG，不包括查询关系及其逆关系。</p></li><li><p><strong>State:</strong> agent 的状态由三部分连接：<strong>嵌入 部分、LSTM 部分和 图注意力部分。</strong>嵌入部分包括：</p><ul><li><p><span class="math inline">\(\mathbf{e}_t\)</span> : 当前实体节点的嵌入</p></li><li><p><span class="math inline">\(\mathbf{e}_{target} - \mathbf{e}_t\)</span> : 尾实体节点和当前节点之间的距离</p></li><li><p>用了 <a href="https://stuxiaozhang.github.io/2021/09/11/TransD%EF%BC%9AKnowledge%20Graph%20Embedding%20via%20Dynamic%20Mapping%20Matrix/#transd">TransD</a>，将所有实体投影到该查询关系的向量空间上。实体的投影嵌入 <span class="math inline">\(\mathbf{e}_{\perp}\)</span> 变成： <span class="math display">\[ \mathbf{e}_{\perp}=\left(\mathbf{r}_{\mathbf{p}} \mathbf{e}_{\mathbf{p}}^{\top}+\mathbf{I}\right) \mathbf{e} \]</span></p></li><li><p><span class="math inline">\(\mathbf{m}_t\)</span> : 定义为 <span class="math inline">\(\mathbf{e}_t\)</span> 和 <span class="math inline">\(\mathbf{e}_{target} - \mathbf{e}_t\)</span> 拼接： <span class="math display">\[ \mathbf{m}_t = \left[\mathbf{e}_{t \perp} ; \mathbf{e}_{\text {target } \perp}-\mathbf{e}_{t \perp}\right] \]</span></p></li><li><p>状态的 LSTM 部分和图注意力部分在后文描述。</p></li></ul></li><li><p><strong>Action:</strong> 对于 KGR 任务，action 指的是一个智能体选择一条关系路径前进。基于深度强化学习的框架，它根据模型提供的概率选择关系。行为可能是有效的或无效的：</p><ul><li>有效的行为表示有输出关系是与当前实体相连的关系</li><li>无效的关系表示该实体没有对应的关系。</li></ul></li><li><p><strong>Reward:</strong> 奖励是根据 action 是否有效，以及一系列 action 是否能在指定次数内到 ground truth 尾实体，向 agent 提供的反馈。</p><ul><li><p>invalid path: reward = -1</p></li><li><p>有效动作：</p><ul><li><p>没有到 ground truth：采用 ConvE 的输出作为奖励。因为 ConvE 输出概率在 (0, 1) 之间，作者使用对数操作将奖励的扩大并提升可辨别性。</p></li><li><p>到 ground truth：取决于全局精度、路径效率和路径多样性的加权和。</p><ul><li>全局精度设置为1</li><li>路径效率: 是路径长度 <span class="math inline">\(|F|\)</span> 的倒数，因为鼓励 agent 尽可能减少步数。</li><li>路径多样性定义为：</li></ul><p><span class="math display">\[ r_{\text {DIVERSITY }}=-\frac{1}{|F|} \sum_{i=1}^{|F|} \cos \left(\mathbf{p}, \mathbf{p}_{i}\right) \]</span></p><p>​ 其中， <span class="math inline">\(\mathbf{p}\)</span> 是路径嵌入，简单地定义为路径中所有关系嵌入的和。</p></li></ul></li></ul></li></ul><p>reward 的定义保证了有效动作的奖励总是大于无效动作的奖励，而成功的事件的奖励总是大于不成功的事件的奖励。</p><h3 id="lstm-and-graph-attention-as-memory-components">2. LSTM and Graph Attention as Memory Components</h3><p>下图是整个 AttnPath 模型的过程：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220217160338101.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><p>1、首先，使用了一个三层 LSTM，<strong>使代理能够记忆和学习之前采取的行动</strong>。用 <span class="math inline">\(h_t\)</span> 表示步骤 <span class="math inline">\(t\)</span> 处 LSTM 的隐藏状态，用 0 表示初始隐藏状态 <span class="math inline">\(h_0\)</span>。可以得到 <span class="math display">\[ \mathbf{h}_{t}=\operatorname{LSTM}\left(\mathbf{h}_{\mathrm{t}-1}, \mathbf{m}_{\mathrm{t}}\right) \]</span> 2、<strong>对于不同的查询关系，代理最好更多地关注与 查询关系 高度相关的关系和邻居。</strong>因此，作者在模型中引入了图注意机制：</p><p>从 <span class="math inline">\(e_i\)</span> 到 <span class="math inline">\(e_j\)</span> 的注意力权重，然后权重归一化的计算如下： <span class="math display">\[ a_{i j}=\operatorname{LeakyReLU}\left(\overrightarrow{\mathbf{a}}^{\top}\left[\mathbf{W e}_{i \perp} ; \mathbf{W e}_{j \perp}\right]\right) \\ \alpha_{i j}=\frac{\exp \left(a_{i j}\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(a_{i k}\right)} \]</span> 则可以计算 hidden state 的图注意部分，是所有邻居嵌入图注意权重的加权和: <span class="math display">\[ \mathbf{a}_{i}=\sum_{k \in \mathcal{N}_{i}} \alpha_{i k} \mathbf{W e}_{k} \]</span> 因此，时间 <span class="math inline">\(t\)</span> 中实体 <span class="math inline">\(i\)</span> 的状态向量 <span class="math inline">\(\mathbf{s}_{i, t}\)</span> 为 <span class="math display">\[ \mathbf{s}_{i, t}=\left[\mathbf{m}_{i, t} ; \mathbf{h}_{t} ; \mathbf{a}_{i}\right] \]</span> 3、接着实体 <span class="math inline">\(i\)</span> 的状态向量 <span class="math inline">\(\mathbf{s}_{i, t}\)</span> 输入到一个三层前馈神经网络，其最终输出为 Softmax 概率。agent 选择一个动作并获得 reward。成功到达尾实体或在指定次数内未到达后，整个的这个过程的奖励用于更新所有参数。</p><p>使用 REINFORCE 算法完成优化，并用以下随机梯度更新 <span class="math inline">\(θ\)</span>： <span class="math display">\[ \nabla_{\theta} J(\theta) \approx \nabla_{\theta} \sum_{t=1}^{T} R\left(\mathbf{s}_{T} \mid e_{s}, r\right) \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) \]</span> 其中，<span class="math inline">\(e_s\)</span> 是头实体，<span class="math inline">\(r\)</span> 是查询关系，$ <em>{}(a</em>{t} s_{t})$ 是所有关系的概率。</p><div class="note note-info"><p>相比于 DeepPath 的关键创新是集成了 LSTM 和图注意力机制。</p></div><h3 id="mean-selection-replacement-rate">3. Mean Selection / Replacement Rate</h3><p>对于不同的查询关系，需要为每个查询关系训练不同的模型。然而在实践中，每种关系的难度值都是不同的。比如说，某些关系可能有更多替换关系，这表明代理可以轻松地选择从头实体到尾实体的替换路径。因此，作者发明了两个指标，平均选择率(MSR) 和平均替换率(MRR)，来定量测量每个关系的难度值。</p><p>用 <span class="math inline">\(Tr=\{(h,r_o,t)|r_o=r\}\)</span> 的集合表示与关系 <span class="math inline">\(r\)</span> 有关的所有三元组。</p><p><strong>选择率 Selection Rate</strong> 定义为关系 <span class="math inline">\(r\)</span> 占据 <span class="math inline">\(h\)</span> 的传出路径的比例： <span class="math display">\[ S R((h, r, t))=\frac{\left|\left\{\left(h_{o}, r_{o}, t_{o}\right) \mid h_{o}=h, r_{o}=r\right\}\right|}{\left|\left\{\left(h_{o}, r_{o}, t_{o}\right) \mid h_{o}=h\right\}\right|} \]</span> <strong>MSR</strong> 是 SR 的平均值，为: <span class="math display">\[ M S R(r)=\sum_{\left(h_{o}, r, t_{o}\right) \in \mathcal{T}_{r}} S R\left(h_{o}, r, t_{o}\right) /\left|\mathcal{T}_{r}\right| \]</span></p><ul><li><strong>较低的 MSR 表示学习查询关系 <span class="math inline">\(r\)</span> 更困难，因为与关系 <span class="math inline">\(r\)</span> 相连的实体可能有很多。</strong></li></ul><p><strong>替换率 Replacement Rate</strong> 定义为除了关系 <span class="math inline">\(r\)</span> 之外直接连接 <span class="math inline">\(h\)</span> 和 <span class="math inline">\(t\)</span> 的关系的比例: <span class="math display">\[ R R((h, r, t))=\frac{\left|\left\{\left(h_{o}, r_{o}, t_{o}\right) \mid h_{o}=h, r_{o} \neq r, t_{o}=t\right\}\right|}{\left|\left\{\left(h_{o}, r_{o}, t_{o}\right) \mid h_{o}=h, t_{o}=t\right\}\right|} \]</span> <strong>MRR</strong> 是 SR 的平均值，为: <span class="math display">\[ M R R(r)=\sum_{\left(h_{o}, r, t_{o}\right) \in \mathcal{T}_{r}} R R\left(h_{o}, r, t_{o}\right) /\left|\mathcal{T}_{r}\right| \]</span></p><ul><li><strong>较高的 MRR 表示一个关系可能有更多的替换关系，因此更容易学习</strong>，因为代理可以直接选择一个替代关系到达目的地。</li></ul><p>模型中，有三种方法来防止过度拟合：L2 正则化、dropout 和 action dropout。</p><ul><li>高MSR和MRR: 说明更容易学习关系。增加正则化，鼓励代理寻找更多不同的路径，但也不能太多导致过拟合.</li><li>低MSR和MRR: 说明更难学习关系。最好关注找到路径的成功率，因此应该减少正则化.</li></ul><p>所以定义公式计算查询关系 r 的<em>(学习)</em>难度系数: <span class="math inline">\(exp(MSR(r)+MRR(r))\)</span>，并分别乘以三种正则化方法的 rate。正则化方法的基本速率基于KG，在同一KG中的所有关系中共享。</p><h3 id="overall-training-algorithm新的rl机制">4. Overall Training Algorithm（新的RL机制）</h3><p>作者提出一个新的强化学习机制：<strong>当智能体选择无效路径时，模型不仅会对其进行惩罚，还会强制其选择有效关系以向前迈进。来自神经网络的概率在所有有效关系中均被归一化，这反过来又影响了强制行为的概率。</strong></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220226204520102.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220227112710615.png" srcset="/img/loading.gif" lazyload></p><p>初始化之后，在指定次数内进行循环：</p><ul><li>根据网络的输出对一个动作进行采样。(判断动作是否有效)<ul><li>当代理选择无效动作时，会强迫 agent 根据概率从 valid actions 中随机 sample 一个 有效 action</li><li>当代理选择有效动作时，直接记录 action</li></ul></li></ul><p>若成功找到 target 或者达到指定次数，就终止循环</p><ul><li>无效 action：reward= -1</li><li>是否在次数内找到 target：<ul><li>成功找到 target： 成功事件中的有效操作，<span class="math inline">\(R_{total}=\lambda_1 r_{\text {GLOBAL}} + \lambda_2 r_{\text {EFFIENCY}} + \lambda_3 r_{\text {DIVERSITY }}\)</span></li><li>没找到target：失败事件中的有效操作，<span class="math inline">\(R_{shaping}\)</span> 用 ConvE 输出的概率，其在所有有效关系中均被归一化</li></ul></li></ul><p>下图是整个训练过程：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220227113046448.png" srcset="/img/loading.gif" lazyload></p><h2 id="experiments">Experiments</h2><h3 id="fact-prediction">Fact Prediction</h3><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220227133615922.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>结果如表2所示。可以看出：</p><ul><li>AttnPath的表现明显优于 TransE/R 和 DeepPath</li><li>AttnPath MS/RR 是用 MSR 和 MRR 微调超参数，也获得了性能改进</li><li>AttnPath Force也是有效的。通过强制代理每一步向前走，它提高了查找路径的速度，从而丰富了下游任务的特性。这对于缺乏直接连接的替换路径且需要具有长期依赖性的路径的关系尤其重要。</li></ul><p>作者的方法在这两个数据集上都实现了SOTA结果。</p><h3 id="link-prediction">Link Prediction</h3><p>链路预测（LP）旨在预测目标实体。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20220227135016271.png" srcset="/img/loading.gif" lazyload></p><p>从表中结果发现，有一些结果不太理想：</p><ul><li>它们有更多指向实体的其他关系的外向边，而这些实体不是真正的尾部，所以这些查询关系的MSR很低。</li><li>尾实体仅与查询关系的边及其逆边连接，而这些边在训练期间被移除，因此尾部实体变得孤立，没有任何可能的可替换路径。它还将降低这些查询关系的MRR。以Birthplace和bornLocation为例。如果一个人出生在一个偏远的地方，这个地方很难与其他实体联系起来，所以很容易被孤立。然而，这种一对一的关系是 TransX 方法的优势。</li></ul><h2 id="summary">Summary</h2><p>在本文中，作者的贡献：</p><ul><li><p>提出了 AttnPath，这是一种基于 DRL 的 KG 推理任务模型，该模型将 LSTM 和图注意力机制作为记忆组件，以减轻模型的预训练。</p></li><li><p>发明了两个指标 MSR 和 MRR 来衡量关系的学习难度，并将其用于更好地微调训练超参数。</p></li><li><p>改进了训练过程，以防止智能体陷入毫无意义的状态。</p></li></ul><p>定性实验和定量分析表明，作者的方法明显优于DeepPath和基于嵌入的方法，证明了其有效性。</p><p>在未来，作者有兴趣于使用多任务学习，使模型能同时学习多个查询关系。作者也感兴趣于研究如何使用 GAT、MSR 和 MRR 于其他 KG 相关的任务，例如 KG 的表示、关系聚类和 KBQA。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/Papers/">Papers</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/KGR/">KGR</a> <a class="hover-with-bg" href="/tags/GAT/">GAT</a> <a class="hover-with-bg" href="/tags/AttnPath/">AttnPath</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2022/02/27/%E3%80%90DeepPath%E3%80%91A%20Reinforcement%20Learning%20Method%20for%20Knowledge%20Graph%20Reasoning/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">【DeepPath】A Reinforcement Learning Method for Knowledge Graph Reasoning</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2022/02/05/%E3%80%90KBGAT%E3%80%91Learning%20Attention-based%20Embeddings%20for%20Relation%20Prediction%20in%20Knowledge%20Graphs/"><span class="hidden-mobile">【KBGAT】Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>