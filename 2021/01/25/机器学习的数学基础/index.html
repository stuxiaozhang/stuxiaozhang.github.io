<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.3.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="这篇文章整理了机器学习相关的一些数学知识，包括线性代数、概率论等。"><meta property="og:type" content="article"><meta property="og:title" content="机器学习的数学基础"><meta property="og:url" content="http://stuxiaozhang.github.io/2021/01/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/index.html"><meta property="og:site_name" content="小张同学的博客"><meta property="og:description" content="这篇文章整理了机器学习相关的一些数学知识，包括线性代数、概率论等。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210125162154830.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210125221333329.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210125110117322.png"><meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7B\varpi%7D+"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210127224821542.png"><meta property="article:published_time" content="2021-01-25T02:34:32.000Z"><meta property="article:modified_time" content="2021-01-27T15:00:26.197Z"><meta property="article:author" content="小张同学"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210125162154830.png"><link rel="canonical" href="http://stuxiaozhang.github.io/2021/01/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>机器学习的数学基础 | 小张同学的博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">小张同学的博客</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">天道酬勤</p></div><div class="site-nav-right"><div class="toggle popup-trigger"></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li></ul></nav></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://stuxiaozhang.github.io/2021/01/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="小张同学"><meta itemprop="description" content="不要停止奔跑 不要回顾来路"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="小张同学的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习的数学基础</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-01-25 10:34:32" itemprop="dateCreated datePublished" datetime="2021-01-25T10:34:32+08:00">2021-01-25</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-01-27 23:00:26" itemprop="dateModified" datetime="2021-01-27T23:00:26+08:00">2021-01-27</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>这篇文章整理了机器学习相关的一些数学知识，包括线性代数、概率论等。</p><p><em><a id="more"></a></em></p><hr><h1 id="线性代数">线性代数</h1><p>线性代数主要包含向量、向量空间（或称线性空间）以及向量的线性变换和有限维的线性方程组。</p><h2 id="向量和向量空间">1. 向量和向量空间</h2><h3 id="向量">1.1 向量</h3><p><strong>标量 (Scalar)</strong> ：就是一个单独的实数，只有大小，没有方向。一般用小写的的变量名称表示。</p><p><strong>向量 (Vector)</strong>： 是有一组实数组成的有序数组，同时具有大小和方向。通常会赋予向量粗体的小写名称。 <span class="math display">\[ x=\left[\begin{array}{c} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{array}\right] \]</span></p><h3 id="向量空间">1.2 向量空间</h3><p><strong>向量空间（Vector Space）</strong> 也称线性空间（Linear Space），是指由向量组成的集合，并满足以下两个条件：</p><ol type="1"><li>向量加法 <span class="math inline">\(+\)</span>：向量空间 𝒱 中的两个向量 𝒂 和 𝒃，它们的和 𝒂 + 𝒃 也属于空间 𝒱</li><li>标量乘法 <span class="math inline">\(⋅\)</span>：向量空间 𝒱 中的任一向量 𝒂 和任一标量 𝑐 ，它们的乘积 𝑐 ⋅ 𝒂 也属 于空间 𝒱.</li></ol><p><strong>线性子空间</strong> 向量空间 𝒱 的线性子空间 𝒰 是 𝒱 的一个子集，并且满足向量空间的条件（向量加法和标量乘法）</p><p><strong>线性无关</strong> 线性空间 <span class="math inline">\(\nu\)</span> 中的 <span class="math inline">\(M\)</span> 个向量 <span class="math inline">\({\nu_1,\nu_2, \cdots, \nu_M}\)</span>，如果对任意的一组标量 <span class="math inline">\(\lambda_{1}, \lambda_{2}, \cdots, \lambda_{M}\)</span> 满足 <span class="math inline">\(\lambda_{1} \boldsymbol{v}_{1}+\lambda_{2} \boldsymbol{v}_{2}+\cdots+\lambda_{M} \boldsymbol{v}_{M}=0\)</span>，则必然 <span class="math inline">\(\lambda_{1}=\lambda_{2}=\cdots=\lambda_{M}=0\)</span> ，那么 <span class="math inline">\(\left\{\boldsymbol{v}_{1}, \boldsymbol{v}_{2}, \cdots, \boldsymbol{v}_{M}\right\}\)</span> 是<strong>线性无关</strong>的，也称为线性独立的。</p><p><strong>内积</strong> 一个 <span class="math inline">\(𝑁\)</span> 维线性空间中的两个向量 𝒂 和 𝒃，其内积（Inner Product）为 <span class="math display">\[ \langle\boldsymbol{a}, \boldsymbol{b}\rangle=\sum_{n=1}^{N} a_{n} b_{n} \]</span> 内积也称为点积（Dot Product）或标量积（Scalar Product）.</p><p><strong>正交</strong> 如果向量空间中两个向量的内积为0，则它们正交（Orthogonal）。如果向量空间中一个向量 𝒗 与子空间 𝒰 中的每个向量都正交，那么向量 𝒗 和子空间 𝒰 正交。</p><h3 id="范数">1.3 范数</h3><p><strong>范数（Norm）：</strong>是<u>一个表示向量“长度”的函数，为向量空间内的所有向量赋予非零的正长度或大小。</u> 对于一个 𝑁 维向量 𝒗 ，一个常见的范数函数为 <span class="math inline">\(ℓ_p\)</span> 范数， <span class="math display">\[ \ell_{p}(\boldsymbol{v}) \equiv\|\boldsymbol{v}\|_{p}=\left(\sum_{n=1}^{N}\left|v_{n}\right|^{p}\right)^{1 / p} \]</span> 其中 <span class="math inline">\(𝑝 ≥ 0\)</span>为一个标量的参数. 常用的𝑝的取值有 <span class="math inline">\(1，2，∞\)</span> 等.</p><p><strong><span class="math inline">\(ℓ_1\)</span> 范数：</strong> <span class="math inline">\(ℓ_1\)</span> 范数为向量的各个元素的绝对值之和. <span class="math display">\[ \|\boldsymbol{v}\|_{1}=\sum_{n=1}^{N}\left|v_{n}\right| \]</span> <strong><span class="math inline">\(ℓ_2\)</span> 范数：</strong> <span class="math inline">\(ℓ_2\)</span> 范数为向量的各个元素的平方和再开平方. <span class="math display">\[ \|\boldsymbol{v}\|_{2}=\sqrt{\sum_{n=1}^{N} v_{n}^{2}}=\sqrt{\boldsymbol{v}^{\top} \boldsymbol{v}} \]</span> c 范数又称为 Euclidean 范数或者 Frobenius 范数。从几何角度，向量也可以表示为从原点出发的一个带箭头的有向线段，其 <span class="math inline">\(ℓ_2\)</span> 范数为线段的长度，也常称为 <u>向量的模</u>.</p><p><strong><span class="math inline">\(ℓ_∞\)</span> 范数：</strong> <span class="math inline">\(ℓ_∞\)</span> 范数为向量的各个元素的最大绝对值， <span class="math display">\[ \|\boldsymbol{v}\|_{\infty}=\max \left\{v_{1}, v_{2}, \cdots, v_{n}\right\} \]</span></p><center><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210125162154830.png" alt="pokeman-parameters" style="width:80%"></center><h3 id="常见的向量">1.4 常见的向量</h3><p><strong>全 0 向量：</strong>指所有元素都为 0 的向量，用 0 表示. 全 0 向量为笛卡尔坐标系中的原点.</p><p><strong>全1向量：</strong>指所有元素都为1的向量，用1表示.</p><p><strong>one-hot 向量：</strong>有且只有一个元素为1，其余元素都为0的向量。one-hot 向量 是在数字电路中的一种状态编码，指对任意给定的状态，状态寄存器中只有1位为 1，其余位都为0.</p><h2 id="矩阵">2. 矩阵</h2><h3 id="线性映射">2.1 线性映射</h3><p><strong>线性映射（Linear Mapping）：</strong>是指从线性空间 𝒱 到线性空间 𝒲 的一个映射函数 𝑓 ∶ 𝒱 → 𝒲 ，并满足：对于 𝒱 中任何两个向量 𝒖 和 𝒗 以及任何标量 𝑐 ，有 <span class="math display">\[ \begin{array}{c} f(\boldsymbol{u}+\boldsymbol{v})=f(\boldsymbol{u})+f(\boldsymbol{v}) \\ f(c \boldsymbol{v})=c f(\boldsymbol{v}) \end{array} \]</span> 两个有限维欧氏空间的映射函数𝑓 ∶ ℝ𝑁 → ℝ𝑀 可以表示为 <span class="math display">\[ \boldsymbol{y}=\boldsymbol{A x} \triangleq\left[\begin{array}{c} a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 N} x_{N} \\ a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 N} x_{N} \\ \vdots \\ a_{M 1} x_{1}+a_{M 2} x_{2}+\cdots+a_{M N} x_{N} \end{array}\right] \]</span> 其中 <span class="math inline">\(𝑨\)</span> 是一个由 <span class="math inline">\(𝑀\)</span> 行 <span class="math inline">\(𝑁\)</span> 列元素排列成的矩形阵列，称为 <span class="math inline">\(M × N\)</span> 的矩阵（Matrix） ： <span class="math display">\[ \boldsymbol{A}=\left[\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1 N} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2 N} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ a_{M 1} &amp; a_{M 2} &amp; \cdots &amp; a_{M N} \end{array}\right] \]</span> 向量 <span class="math inline">\(𝒙 ∈ ℝ^𝑁\)</span> 和 <span class="math inline">\(𝒚 ∈ ℝ^𝑀\)</span> 为两个空间中的向量， 𝒙 和 𝒚 可以分别表示为 <span class="math inline">\(𝑁 × 1\)</span> 的矩阵和 <span class="math inline">\(𝑀 × 1\)</span> 的矩阵. <span class="math display">\[ \boldsymbol{x}=\left[\begin{array}{c} x_{1} \\ x_{2} \\ \vdots \\ x_{N} \end{array}\right], \quad \boldsymbol{y}=\left[\begin{array}{c} y_{1} \\ y_{2} \\ \vdots \\ y_{M} \end{array}\right] . \]</span> 这种表示形式称为<strong>列向量</strong>，即只有一列的矩阵</p><h3 id="矩阵操作">2.2 矩阵操作</h3><p><strong>转置：</strong> <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩阵 <span class="math inline">\(𝑨\)</span> 的转置（Transposition）是一个<span class="math inline">\(𝑁 × 𝑀\)</span> 的矩阵，记为 <span class="math inline">\(𝑨^T\)</span> ，<span class="math inline">\(𝑨^T\)</span> 的第 i 行第 j 列的元素是原矩阵 <span class="math inline">\(𝑨\)</span> 的第 j 行第 i 列的元素， <span class="math display">\[ [A^T]_{ij} = [A]_{ji} \]</span> <strong>哈达玛积（Hadamard）：</strong> 矩阵 𝑨 和矩阵 𝑩 的 Hadamard 积（Hadamard Product）也称为逐点乘积，为 𝑨 和 𝑩 中对应的元素相乘. <span class="math display">\[ [\boldsymbol{A} \odot \boldsymbol{B}]_{i j}=a_{i j} b_{i j} \]</span></p><blockquote><p>举栗： <span class="math display">\[ \begin{bmatrix} a_{1}&amp; a_{2} \\ a_{3}&amp; a_{4} \\ \end{bmatrix}\cdot \begin{bmatrix} b_{1}&amp; b_{2} \\ b_{3}&amp; b_{4} \\ \end{bmatrix} = \begin{bmatrix} a_{1}b_{1}&amp; a_{2}b_{2} \\ a_{3}b_{3}&amp; a_{4}b_{4} \\ \end{bmatrix} \]</span></p></blockquote><p><strong>克罗内克积（Kronecker）</strong> 如果 𝑨 是 <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩阵，𝑩 是 <span class="math inline">\(𝑝 × 𝑞\)</span> 的矩阵，那么它们的Kronecker 积（Kronecker Product）是一个 <span class="math inline">\(𝑚𝑝 × 𝑛𝑞\)</span> 的矩阵： <span class="math display">\[ [\boldsymbol{A} \otimes \boldsymbol{B}]=\left[\begin{array}{cccc} a_{11} \boldsymbol{B} &amp; a_{12} \boldsymbol{B} &amp; \cdots &amp; a_{1 n} \boldsymbol{B} \\ a_{21} \boldsymbol{B} &amp; a_{22} \boldsymbol{B} &amp; \cdots &amp; a_{2 n} \boldsymbol{B} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ a_{M 1} \boldsymbol{B} &amp; a_{M 2} \boldsymbol{B} &amp; \cdots &amp; a_{m n} \boldsymbol{B} \end{array}\right] \]</span></p><blockquote><p>举栗： <span class="math display">\[ \begin{bmatrix} a_{1}&amp; a_{2} \\ a_{3}&amp; a_{4} \\ \end{bmatrix} \otimes \begin{bmatrix} b_{1}&amp; b_{2} \\ b_{3}&amp; b_{4} \\ \end{bmatrix} = \begin{bmatrix} a_{1}b_{1}&amp; a_{2}b_{2} &amp; a_{2}b_{1}&amp; a_{2}b_{2}\\ a_{1}b_{3}&amp; a_{2}b_{4} &amp; a_{2}b_{3}&amp; a_{2}b_{4}\\ a_{3}b_{1}&amp; a_{3}b_{2} &amp; a_{4}b_{1}&amp; a_{4}b_{2}\\ a_{3}b_{3}&amp; a_{3}b_{4} &amp; a_{4}b_{3}&amp; a_{4}b_{4}\\ \end{bmatrix} \]</span></p></blockquote><p><strong>外积</strong> 两个向量 <span class="math inline">\(𝒂 ∈ ℝ^𝑀\)</span> 和 <span class="math inline">\(𝒃 ∈ ℝ^𝑁\)</span> 的外积（Outer Product）是一个 <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩阵，定义为 <span class="math display">\[ \boldsymbol{a} \otimes \boldsymbol{b}=\left[\begin{array}{cccc} a_{1} b_{1} &amp; a_{1} b_{2} &amp; \ldots &amp; a_{1} b_{n} \\ a_{2} b_{1} &amp; a_{2} b_{2} &amp; \ldots &amp; a_{2} b_{n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{M} b_{1} &amp; a_{M} b_{2} &amp; \ldots &amp; a_{M} b_{n} \end{array}\right]=\boldsymbol{a} \boldsymbol{b^T} \]</span></p><blockquote><p><span class="math display">\[ \boldsymbol{a}=\left[\begin{array}{c} a_{1} \\ a_{2} \\ \vdots \\ a_{M} \end{array}\right], \quad \boldsymbol{b}=\left[\begin{array}{c} b_{1} \\ b_{2} \\ \vdots \\ b_{N} \end{array}\right] \]</span></p><p>外积：<span class="math inline">\(a \otimes b = ab^T\)</span> (MX1, Nx1-&gt;MxN)</p></blockquote><blockquote><p>外积通常看作是矩阵的Kronecker积的一种特例，但两者并不等价.</p></blockquote><p><strong>向量化：</strong> 矩阵的向量化是将矩阵表示为一个列向量. 令 <span class="math inline">\(𝑨 = [𝑎_{𝑖𝑗}]_{𝑀×𝑁}\)</span>，向量化算 <span class="math inline">\(vec(⋅)\)</span> 定义为 <span class="math display">\[ \operatorname{vec}(\boldsymbol{A})=\left[a_{11}, a_{21}, \cdots, a_{M 1}, a_{12}, a_{22}, \cdots, a_{M 2}, \cdots, a_{1 N}, a_{2 N}, \cdots, a_{M N}\right]^{\top} \]</span> <strong>迹：</strong> 方块矩阵 <span class="math inline">\(𝑨\)</span> 的对角线元素之和称为它的迹（Trace），记为 <span class="math inline">\(𝑡𝑟(𝑨)\)</span>. 尽管矩阵的 乘法不满足交换律，但它们的迹相同，即 <span class="math inline">\(𝑡𝑟(𝑨𝑩) = 𝑡𝑟(𝑩𝑨)\)</span>.</p><p><strong>秩：</strong> 一个矩阵 <span class="math inline">\(𝑨\)</span> 的列秩是 <span class="math inline">\(𝑨\)</span> 的线性无关的列向量数量，行秩是 <span class="math inline">\(𝑨\)</span> 的线性无关的行 向量数量. 一个矩阵的列秩和行秩总是相等的，简称为秩（Rank）.</p><h3 id="矩阵类型">2.3 矩阵类型</h3><p><strong>对称矩阵（Symmetric Matrix）</strong>指其转置等于自己的矩阵，即满足 <span class="math inline">\(𝑨 = 𝑨^T\)</span> .</p><p><strong>对角矩阵（Diagonal Matrix）</strong> 是一个主对角线之外的元素皆为 0 的矩阵.</p><p><strong>单位矩阵（Identity Matrix）</strong>是一种特殊的对角矩阵，其主对角线元素为 1，其余元素为 0. 𝑁 阶单位矩阵 <span class="math inline">\(𝑰_𝑁\)</span> ，是一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的方块矩阵. 可以记为 <span class="math inline">\(𝐼𝑁 = diag(1, 1, ..., 1)\)</span> .一个 <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩阵 A 和单位矩阵的乘积等于其本身</p><p><strong>逆矩阵</strong> 对于一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的方块矩阵 𝑨，如果存在另一个方块矩阵𝑩 使得 <span class="math display">\[ AB=BA=I_N \]</span> 其中 <span class="math inline">\(𝑰_𝑁\)</span> 为单位阵，则称 𝑨 是可逆的. 矩阵 𝑩 称为矩阵 𝑨 的逆矩阵（Inverse Matrix），记为 <span class="math inline">\(𝑨^{−1}\)</span> .</p><p><strong>正定矩阵</strong> 对于一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的对称矩阵 𝑨，如果对于所有的非零向量 <span class="math inline">\(𝒙 ∈ ℝ^𝑁\)</span> 都满足 <span class="math display">\[ x^TAx&gt;0 \]</span> 则 𝑨 为正定矩阵（Positive-Definite Matrix）。如果 <span class="math inline">\(𝒙^T𝑨𝒙 ≥ 0\)</span>，则𝑨是半正定矩阵</p><blockquote><p>我们发现，所有的<strong>二次齐次式</strong>都可以表示为矩阵的形式，例如：<span class="math inline">\(f=x_{1}^{2}+2 x_{1} x_{2}+4 x_{2}^{2}+6 x_{2} x_{3}+4 x_{3}^{2}\)</span> ，</p><p>就可以表示为：<span class="math inline">\(\left[\begin{array}{lll}x_{1} &amp; x_{2} &amp; x_{3}\end{array}\right]\left[\begin{array}{lll}1 &amp; 1 &amp; 0 \\ 1 &amp; 4 &amp; 3 \\ 0 &amp; 3 &amp; 4\end{array}\right]\left[\begin{array}{l}x_{1} \\ x_{2} \\ x_{3}\end{array}\right]=X^{\prime} A X\)</span></p><p>显然，这个表示是唯一的：每一个二次型都唯一对应一个<strong>对称矩阵</strong> <span class="math inline">\(A\)</span> ，反之亦如此. 无论是这个二次齐次式，还是代表它的矩阵，我们都称之为<strong>二次型</strong>，因为他们指向的是同一件事.</p><p>也许你发现了这样一个事实，<span class="math inline">\(f=\left(x_{1}+x_{1}\right)^{2}+3\left(x_{2}+x_{3}\right)^{2}+x_{3}^{2}=y_{1}^{2}+3 y_{2}^{2}+y_{3}^{2} \geq 0\)</span></p><p><span class="math inline">\(y_{1}, y_{2}, y_{3} \in \mathbb{R}\)</span>，当 <span class="math inline">\(y_{1}, y_{2}, y_{3} \in \mathbb{R}\)</span> 不全为 0 时，这个二次型严格大于 0.</p><p>定义：<strong>当 <span class="math inline">\(X\)</span> 不是零向量的时候，就会有：<span class="math display">\[f=X^{\prime} A X&gt;0 \]</span></strong> 。我们将这样的二次型称为正定的，对称矩阵 <span class="math inline">\(A\)</span> 称为正定矩阵****</p></blockquote><p><strong>正交矩阵</strong> 如果一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的方块矩阵𝑨的逆矩阵等于其转置矩阵，即 <span class="math display">\[ A^T=A^{-1} \]</span> 则𝑨为正交矩阵（Orthogonal Matrix）</p><p>正交矩阵满足 <span class="math inline">\(𝑨^T𝑨 = 𝑨𝑨^T = 𝑰_𝑁\)</span>，即正交矩阵的每一行（列）向量和自身的内积为1，和其他行（列）向量的内积为0.</p><blockquote><p>假设A是一个列向量矩阵，标识为 <span class="math inline">\(A=[\alpha_1,\alpha_2, ...,\alpha_n]\)</span>，那么按照定义就是：</p><center><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210125221333329.png" style="width:60%"></center></blockquote><h3 id="特征值与特征向量">2.4 特征值与特征向量</h3><p>对一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的矩阵 <span class="math inline">\(𝑨\)</span> ，如果存在一个标量 𝜆 和一个非零向量 𝒗 满足 <span class="math display">\[ A𝒗 =𝜆𝒗 \]</span> 则 𝜆 和 𝒗 分别称为矩阵 <span class="math inline">\(𝑨\)</span> 的 <strong>特征值（Eigenvalue）</strong>和 <strong>特征向量（Eigenvector）</strong>.</p><p>当用矩阵 𝑨 对它的特征向量 𝒗 进行线性映射时，得到的新向量只是在 𝒗 的长度上缩放 𝜆 倍. 给定一个矩阵的特征值，其对应的特征向量的数量是无限多的. 令 𝒖 和 𝒗 是矩阵 𝑨 的特征值 𝜆 对应的特征向量，则 𝛼𝒖 和 𝒖 + 𝒗 也是特征值 𝜆 对应的特征向量.</p><p>如果矩阵 𝑨 是一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的实对称矩阵，则存在实数 <span class="math inline">\(𝜆_1 , ⋯ , 𝜆_𝑁\)</span>，以及 <span class="math inline">\(𝑁\)</span> 个互相正交的单位向量 <span class="math inline">\(𝒗_1 , ⋯ , 𝒗_𝑁\)</span>，使得 <span class="math inline">\(𝒗_𝑛\)</span> 为矩阵 𝑨 的特征值为 𝜆𝑛 的特征向量 （1 ≤ 𝑛 ≤ 𝑁）.</p><h3 id="矩阵分解">2.5 矩阵分解</h3><h4 id="特征分解">特征分解</h4><p>一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的方块矩阵 𝑨 的特征分解（Eigendecomposition）定义为 <span class="math display">\[ 𝑨 = 𝑸𝚲𝑸^{−1} \]</span> 其中 𝑸 为 <span class="math inline">\(𝑁 × 𝑁\)</span> 的方块矩阵，其每一列都为 𝑨 的特征向量，𝚲 为对角阵，其每一个对角元素分别为 𝑨 的一个特征值.</p><p>如果 𝑨 为实对称矩阵，那么其不同特征值对应的特征向量相互正交. 𝑨 可以被分解为<br><span class="math display">\[ 𝑨 = 𝑸𝚲𝑸^{T} \]</span> 其中𝑸 为正交阵.</p><h4 id="奇异值分解">奇异值分解</h4><p>一个 <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩阵𝑨的奇异值分解（Singular Value Decomposition，SVD） 定义为 <span class="math display">\[ 𝑨 = 𝑼𝚺𝑽^T \]</span> 其中 𝑼 和 𝑽 分别为 <span class="math inline">\(𝑀 × 𝑀\)</span> 和 <span class="math inline">\(𝑁 × 𝑁\)</span> 的正交矩阵，𝚺 为 <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩形对角矩阵. 𝚺 对角线上的元素称为奇异值（Singular Value），一般按从大到小排列.</p><blockquote><p>奇异值分解在后面学到机器学习算法的时候再详细学习。</p><p>留个坑。</p></blockquote><h3 id="moore-penrose伪逆">==Moore-Penrose伪逆==</h3><p>对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵A的左逆B来求解线性方程： <span class="math display">\[ A x=y \]</span> 等式两边同时左乘左逆B后，得到： <span class="math display">\[ x=By \]</span> 是否存在唯一的映射将A映射到B取决于问题的形式。</p><p>如果矩阵A的行数大于列数，那么上述方程可能没有解；如果矩阵A的行数小于列数，那么上述方程可能有多个解。</p><p>Moore-Penrose伪逆使我们能够解决这种情况，矩阵A的伪逆定义为： <span class="math display">\[ \boldsymbol{A}^{+}=\lim _{a \searrow 0}\left(\boldsymbol{A}^{\top} \boldsymbol{A}+\alpha \boldsymbol{I}\right)^{-1} \boldsymbol{A}^{\top} \]</span> 但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式： <span class="math display">\[ A^{+}=V D^{+} U^{\top} \]</span> 其中，矩阵U，D 和V 是矩阵A奇异值分解后得到的矩阵。对角矩阵D 的伪逆D+ 是其非零元素取倒之后再转置得到的。</p><h3 id="张量">张量</h3><p>几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。</p><p>例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格：</p><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210125110117322.png" alt="image-20210125110117322"><figcaption>image-20210125110117322</figcaption></figure><p>其中表的横轴表示图片的宽度值，这里只截取0 ~ 319；表的纵轴表示图片的高度值，这里只截取0~4；表格中每个方格代表一个像素点，比如第一行第一列的表格数据为[1.0,1.0,1.0]，代表的就是RGB三原色在图片的这个位置的取值情况（即R=1.0，G=1.0，B=1.0）为白色。</p><blockquote><p>如图所示，前面5行的数据全是白色。</p></blockquote><p>当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。</p><p>张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。</p><h3 id="几种常用的距离">几种常用的距离</h3><p>上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有100个特征，那么他对应的就是一个100维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的KNN算法和K-means算法中很明显。</p><p>设有两个n维变量 <span class="math inline">\(A=\left[x_{11}, x_{12}, \ldots, x_{1 n}\right]\)</span> 和 <span class="math inline">\(B=\left[x_{21}, x_{22}, \ldots, x_{2 n}\right]\)</span>，则一些常用的距离公式定义如下：</p><h4 id="曼哈顿距离">1、曼哈顿距离</h4><p>曼哈顿距离也称为城市街区距离，数学定义如下： <span class="math display">\[ d_{12}=\sum_{k=1}^{n}\left|x_{1 k}-x_{2 k}\right| \]</span> 曼哈顿距离的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">sum</span>(<span class="built_in">abs</span>(vector1-vector2))</span><br></pre></td></tr></table></figure><h4 id="欧氏距离">2、欧氏距离</h4><p>欧氏距离其实就是L2范数，数学定义如下： <span class="math display">\[ d_{12}=\sqrt{\sum_{k=1}^{n}\left(x_{1 k}-x_{2 k}\right)^{2}} \]</span> 欧氏距离的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span> sqrt((vector1-vector2)*(vector1-vector2).T)</span><br></pre></td></tr></table></figure><h4 id="闵可夫斯基距离">3、闵可夫斯基距离</h4><p>从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义： <span class="math display">\[ d_{12}=\sqrt[p]{\sum_{k=1}^{n}\left(x_{1 k}-x_{2 k}\right)^{p}} \]</span> 实际上，当p=1时，就是曼哈顿距离；当p=2时，就是欧式距离。</p><h4 id="切比雪夫距离">4、切比雪夫距离</h4><p>切比雪夫距离就是<img data-src="https://www.zhihu.com/equation?tex=L_%7B\varpi%7D+" alt="[公式]">，即无穷范数，数学表达式如下： <span class="math display">\[ d_{12}=\max \left(\left|x_{1 k}-x_{2 k}\right|\right) \]</span> 切比雪夫距离的Python实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span> sqrt(<span class="built_in">abs</span>(vector1-vector2).<span class="built_in">max</span>)</span><br></pre></td></tr></table></figure><h4 id="夹角余弦">5、夹角余弦</h4><p>夹角余弦的取值范围为[-1,1]，可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值1；当两个向量的方向完全相反时，夹角余弦取最小值-1。</p><p>机器学习中用这一概念来衡量样本向量之间的差异，其数学表达式如下： <span class="math display">\[ \cos \theta=\frac{A B}{|A||B|}=\frac{\sum_{k=1}^{n} x_{1 k} x_{2 k}}{\sqrt{\sum_{k=1}^{n} x_{1 k}^{2}} \sqrt{\sum_{k=1}^{n} x_{2 k}^{2}}} \]</span> 夹角余弦的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span> dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))</span><br></pre></td></tr></table></figure><h4 id="汉明距离">6、汉明距离</h4><p>汉明距离定义的是两个字符串中不相同位数的数目。</p><blockquote><p>例如：字符串‘1111’与‘1001’之间的汉明距离为2。</p></blockquote><p>信息编码中一般应使得编码间的汉明距离尽可能的小。</p><p>汉明距离的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">smstr = nonzero(matV[<span class="number">0</span>]-matV[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span> smstr</span><br></pre></td></tr></table></figure><h4 id="杰卡德相似系数">7、杰卡德相似系数</h4><p>两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数，用符号J(A,B)表示，数学表达式为： <span class="math display">\[ J(A, B)=\frac{|A \cap B|}{|A \cup B|} \]</span> 杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。</p><h4 id="杰卡德距离">==8、杰卡德距离==</h4><p>与杰卡德相似系数相反的概念是杰卡德距离，其定义式为： <span class="math display">\[ J_{\sigma}=1-J(A, B)=\frac{|A \cup B|-|A \cap B|}{|A \cup B|} \]</span> 杰卡德距离的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> scipy.spatial.distance <span class="keyword">as</span> dist</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span> dist.pdist(matV,<span class="string">&#x27;jaccard&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="概率论">概率论</h1><h2 id="随机变量">1.1 随机变量</h2><p><strong>随机变量</strong> ：在随机试验中，试验的结果可以用一个数 𝑋 来表示，这个数 𝑋 是随着试验 结果的不同而变化的，是样本点的一个函数. 我们把这种数称为随机变量</p><p><strong>离散随机变量</strong>：如果随机变量 <span class="math inline">\(𝑋\)</span> 所可能取的值为有限可列举的，有 <span class="math inline">\(𝑁\)</span> 个有限取值 <span class="math display">\[ {𝑥_1, ⋯ , 𝑥_𝑁 }, \]</span> 则称 <span class="math inline">\(𝑋\)</span> 为离散随机变量</p><p>要了解 <span class="math inline">\(𝑋\)</span> 的统计规律，就必须知道它取每种可能值 <span class="math inline">\(𝑥_𝑛\)</span> 的概率，即 <span class="math display">\[ 𝑃(𝑋 = 𝑥_𝑛) = 𝑝(𝑥_𝑛), ∀𝑛 ∈ \{ 1, ⋯ , 𝑁 \} \]</span> 其中 <span class="math inline">\(𝑝(𝑥_1 ), ⋯ , 𝑝(𝑥_𝑁 )\)</span> 称为 <strong>离散随机变量 <span class="math inline">\(𝑋\)</span> 的概率分布</strong>（Probability Distribution） 或分布，并且满足 <span class="math display">\[ \begin{array}{l} \sum_{n=1}^{N} p\left(x_{n}\right)=1 \end{array} \]</span></p><p><span class="math display">\[ p\left(x_{n}\right) \geq 0, \quad \forall n \in\{1, \cdots, N\} \]</span></p><p><u>常见的离散随机变量</u>的概率分布有：</p><p><strong>伯努利分布</strong> 在一次试验中，事件 A 出现的概率为 <span class="math inline">\(𝜇\)</span> ，不出现的概率为 <span class="math inline">\(1 − 𝜇\)</span>. 若用 变量 <span class="math inline">\(𝑋\)</span> 表示事件<span class="math inline">\(𝑨\)</span> 出现的次数，则 <span class="math inline">\(𝑋\)</span> 的取值为 0 和 1，其相应的分布为 <span class="math display">\[ p(x)=\mu^{x}(1-\mu)^{(1-x)} \]</span> 这个分布称为 <u>伯努利分布（Bernoulli Distribution）, 又名 两点分布 或者 0-1 分布</u>.</p><strong>二项分布</strong> 在n次伯努利试验中，若以变量 <span class="math inline">\(𝑋\)</span> 表示事件A出现的次数，则𝑋 的取值 为{0, ⋯ , 𝑁}，其相应的分布为二项分布（Binomial Distribution）. <span class="math display">\[ P(X=k)=\left(\begin{array}{c} N \\ k \end{array}\right) \mu^{k}(1-\mu)^{N-k}, \quad k=0 \cdots, N \]</span> 其中 $ <span class="math display">\[\begin{pmatrix} N\\K \end{pmatrix}\]</span><p>$ 为二项式系数，表示从 <span class="math inline">\(𝑁\)</span> 个元素中取出 <span class="math inline">\(𝑘\)</span> 个元素而不考虑其顺序的组 合的总数.</p><p><strong>连续随机变量</strong>：与离散随机变量不同，一些随变量 <span class="math inline">\(𝑋\)</span> 的取值是不可列举的，由全部实数或者由一部分区间组成，比如 <span class="math display">\[ 𝑋 = \{𝑥|𝑎 ≤ 𝑥 ≤ 𝑏\}, −∞ &lt; 𝑎 &lt; 𝑏 &lt; ∞ \]</span> 则称 <span class="math inline">\(𝑋\)</span> 为连续随机变量. 连续随机变量的值是不可数及无穷尽的.</p><p>连续随机变量 <span class="math inline">\(𝑋\)</span> 的概率分布一般用概率密度函数（Probability Density Function，PDF）<span class="math inline">\(𝑝(𝑥)\)</span> 来描述. <span class="math inline">\(𝑝(𝑥)\)</span> 为可积函数，并满足 <span class="math display">\[ \int_{-∞}^{+∞} p(x)dx=1 \]</span> <u>常见的连续随机变量</u>的概率分布有：</p><p><strong>均匀分布</strong> 若𝑎, 𝑏为有限数，[𝑎, 𝑏]上的均匀分布（Uniform Distribution）的概率密度函数定义为 $$</p><p><span class="math display">\[ **正态分布** 正态分布（Normal Distribution），又名高斯分布（Gaussian Distribution），是自然界最常见的一种分布，并且具有很多良好的性质，其概率密度函数为 \]</span></p><p><span class="math display">\[ 其中 $𝜎 &gt; 0$，$𝜇$ 和 $𝜎$ 均为常数. 若随机变量 $𝑋$ 服从一个参数为 $𝜇$ 和 $𝜎$ 的概率分布， 简记为 \]</span> 𝑋 ∼ N(𝜇, 𝜎^2) $$ 当 <span class="math inline">\(𝜇 = 0，𝜎 = 1\)</span> 时，称为标准正态分布（Standard Normal Distribution）</p><center><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210127224821542.png" alt="pokeman-parameters" style="width:70%"></center><h3 id="条件概率分布">1.2 条件概率分布</h3><p><strong>条件分布</strong> 对于离散随机向量 (𝑋, 𝑌 )，已知 𝑋 = 𝑥 的条件下，随机变量 𝑌 = 𝑦 的条件概 率（Conditional Probability）为</p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2021/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/" rel="prev" title="机器学习介绍"><i class="fa fa-chevron-left"></i> 机器学习介绍</a></div><div class="post-nav-item"><a href="/2021/01/29/Python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/" rel="next" title="Python多线程编程">Python多线程编程 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="nav-number">1.</span> <span class="nav-text">线性代数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%92%8C%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4"><span class="nav-number">1.1.</span> <span class="nav-text">1. 向量和向量空间</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%91%E9%87%8F"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 向量空间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%8C%83%E6%95%B0"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 范数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E5%90%91%E9%87%8F"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 常见的向量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5"><span class="nav-number">1.2.</span> <span class="nav-text">2. 矩阵</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 线性映射</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E6%93%8D%E4%BD%9C"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 矩阵操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E7%B1%BB%E5%9E%8B"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 矩阵类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%80%BC%E4%B8%8E%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 特征值与特征向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="nav-number">1.2.5.</span> <span class="nav-text">2.5 矩阵分解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3"><span class="nav-number">1.2.5.1.</span> <span class="nav-text">特征分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="nav-number">1.2.5.2.</span> <span class="nav-text">奇异值分解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#moore-penrose%E4%BC%AA%E9%80%86"><span class="nav-number">1.2.6.</span> <span class="nav-text">&#x3D;&#x3D;Moore-Penrose伪逆&#x3D;&#x3D;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F"><span class="nav-number">1.2.7.</span> <span class="nav-text">张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%A0%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.2.8.</span> <span class="nav-text">几种常用的距离</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.2.8.1.</span> <span class="nav-text">1、曼哈顿距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.2.8.2.</span> <span class="nav-text">2、欧氏距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%B5%E5%8F%AF%E5%A4%AB%E6%96%AF%E5%9F%BA%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.2.8.3.</span> <span class="nav-text">3、闵可夫斯基距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.2.8.4.</span> <span class="nav-text">4、切比雪夫距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%B9%E8%A7%92%E4%BD%99%E5%BC%A6"><span class="nav-number">1.2.8.5.</span> <span class="nav-text">5、夹角余弦</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.2.8.6.</span> <span class="nav-text">6、汉明距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%B0%E5%8D%A1%E5%BE%B7%E7%9B%B8%E4%BC%BC%E7%B3%BB%E6%95%B0"><span class="nav-number">1.2.8.7.</span> <span class="nav-text">7、杰卡德相似系数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%B0%E5%8D%A1%E5%BE%B7%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.2.8.8.</span> <span class="nav-text">&#x3D;&#x3D;8、杰卡德距离&#x3D;&#x3D;</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="nav-number">2.</span> <span class="nav-text">概率论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="nav-number">2.1.</span> <span class="nav-text">1.1 随机变量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="nav-number">2.1.1.</span> <span class="nav-text">1.2 条件概率分布</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">小张同学</p><div class="site-description" itemprop="description">不要停止奔跑 不要回顾来路</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">52</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">25</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">小张同学</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">540k</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script></body></html>