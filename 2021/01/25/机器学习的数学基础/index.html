<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="这篇文章整理了机器学习相关的一些数学知识，包括线性代数、概率论等。
"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>机器学习的数学基础 - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/vs.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:85,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,0)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="机器学习的数学基础"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-01-25 10:34" pubdate>2021-01-25</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 5.2k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 83 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">机器学习的数学基础</h1><div class="markdown-body"><p>这篇文章整理了机器学习相关的一些数学知识，包括线性代数、概率论等。</p><p><em><a id="more"></a></em></p><hr><h1 id="线性代数">线性代数</h1><p>线性代数主要包含向量、向量空间（或称线性空间）以及向量的线性变换和有限维的线性方程组。</p><h2 id="向量和向量空间">1. 向量和向量空间</h2><h3 id="向量">1.1 向量</h3><p><strong>标量 (Scalar)</strong> ：就是一个单独的实数，只有大小，没有方向。一般用小写的的变量名称表示。</p><p><strong>向量 (Vector)</strong>： 是有一组实数组成的有序数组，同时具有大小和方向。通常会赋予向量粗体的小写名称。 <span class="math display">\[ x=\left[\begin{array}{c} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{array}\right] \]</span></p><h3 id="向量空间">1.2 向量空间</h3><p><strong>向量空间（Vector Space）</strong> 也称线性空间（Linear Space），是指由向量组成的集合，并满足以下两个条件：</p><ol type="1"><li>向量加法 <span class="math inline">\(+\)</span>：向量空间 𝒱 中的两个向量 𝒂 和 𝒃，它们的和 𝒂 + 𝒃 也属于空间 𝒱</li><li>标量乘法 <span class="math inline">\(⋅\)</span>：向量空间 𝒱 中的任一向量 𝒂 和任一标量 𝑐 ，它们的乘积 𝑐 ⋅ 𝒂 也属 于空间 𝒱.</li></ol><p><strong>线性子空间</strong> 向量空间 𝒱 的线性子空间 𝒰 是 𝒱 的一个子集，并且满足向量空间的条件（向量加法和标量乘法）</p><p><strong>线性无关</strong> 线性空间 <span class="math inline">\(\nu\)</span> 中的 <span class="math inline">\(M\)</span> 个向量 <span class="math inline">\({\nu_1,\nu_2, \cdots, \nu_M}\)</span>，如果对任意的一组标量 <span class="math inline">\(\lambda_{1}, \lambda_{2}, \cdots, \lambda_{M}\)</span> 满足 <span class="math inline">\(\lambda_{1} \boldsymbol{v}_{1}+\lambda_{2} \boldsymbol{v}_{2}+\cdots+\lambda_{M} \boldsymbol{v}_{M}=0\)</span>，则必然 <span class="math inline">\(\lambda_{1}=\lambda_{2}=\cdots=\lambda_{M}=0\)</span> ，那么 <span class="math inline">\(\left\{\boldsymbol{v}_{1}, \boldsymbol{v}_{2}, \cdots, \boldsymbol{v}_{M}\right\}\)</span> 是<strong>线性无关</strong>的，也称为线性独立的。</p><p><strong>内积</strong> 一个 <span class="math inline">\(𝑁\)</span> 维线性空间中的两个向量 𝒂 和 𝒃，其内积（Inner Product）为 <span class="math display">\[ \langle\boldsymbol{a}, \boldsymbol{b}\rangle=\sum_{n=1}^{N} a_{n} b_{n} \]</span> 内积也称为点积（Dot Product）或标量积（Scalar Product）.</p><p><strong>正交</strong> 如果向量空间中两个向量的内积为0，则它们正交（Orthogonal）。如果向量空间中一个向量 𝒗 与子空间 𝒰 中的每个向量都正交，那么向量 𝒗 和子空间 𝒰 正交。</p><h3 id="范数">1.3 范数</h3><p><strong>范数（Norm）：</strong>是<u>一个表示向量“长度”的函数，为向量空间内的所有向量赋予非零的正长度或大小。</u> 对于一个 𝑁 维向量 𝒗 ，一个常见的范数函数为 <span class="math inline">\(ℓ_p\)</span> 范数， <span class="math display">\[ \ell_{p}(\boldsymbol{v}) \equiv\|\boldsymbol{v}\|_{p}=\left(\sum_{n=1}^{N}\left|v_{n}\right|^{p}\right)^{1 / p} \]</span> 其中 <span class="math inline">\(𝑝 ≥ 0\)</span>为一个标量的参数. 常用的𝑝的取值有 <span class="math inline">\(1，2，∞\)</span> 等.</p><p><strong><span class="math inline">\(ℓ_1\)</span> 范数：</strong> <span class="math inline">\(ℓ_1\)</span> 范数为向量的各个元素的绝对值之和. <span class="math display">\[ \|\boldsymbol{v}\|_{1}=\sum_{n=1}^{N}\left|v_{n}\right| \]</span> <strong><span class="math inline">\(ℓ_2\)</span> 范数：</strong> <span class="math inline">\(ℓ_2\)</span> 范数为向量的各个元素的平方和再开平方. <span class="math display">\[ \|\boldsymbol{v}\|_{2}=\sqrt{\sum_{n=1}^{N} v_{n}^{2}}=\sqrt{\boldsymbol{v}^{\top} \boldsymbol{v}} \]</span> c 范数又称为 Euclidean 范数或者 Frobenius 范数。从几何角度，向量也可以表示为从原点出发的一个带箭头的有向线段，其 <span class="math inline">\(ℓ_2\)</span> 范数为线段的长度，也常称为 <u>向量的模</u>.</p><p><strong><span class="math inline">\(ℓ_∞\)</span> 范数：</strong> <span class="math inline">\(ℓ_∞\)</span> 范数为向量的各个元素的最大绝对值， <span class="math display">\[ \|\boldsymbol{v}\|_{\infty}=\max \left\{v_{1}, v_{2}, \cdots, v_{n}\right\} \]</span></p><center><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210125162154830.png" srcset="/img/loading.gif" lazyload alt="pokeman-parameters" style="width:80%"></center><h3 id="常见的向量">1.4 常见的向量</h3><p><strong>全 0 向量：</strong>指所有元素都为 0 的向量，用 0 表示. 全 0 向量为笛卡尔坐标系中的原点.</p><p><strong>全1向量：</strong>指所有元素都为1的向量，用1表示.</p><p><strong>one-hot 向量：</strong>有且只有一个元素为1，其余元素都为0的向量。one-hot 向量 是在数字电路中的一种状态编码，指对任意给定的状态，状态寄存器中只有1位为 1，其余位都为0.</p><h2 id="矩阵">2. 矩阵</h2><h3 id="线性映射">2.1 线性映射</h3><p><strong>线性映射（Linear Mapping）：</strong>是指从线性空间 𝒱 到线性空间 𝒲 的一个映射函数 𝑓 ∶ 𝒱 → 𝒲 ，并满足：对于 𝒱 中任何两个向量 𝒖 和 𝒗 以及任何标量 𝑐 ，有 <span class="math display">\[ \begin{array}{c} f(\boldsymbol{u}+\boldsymbol{v})=f(\boldsymbol{u})+f(\boldsymbol{v}) \\ f(c \boldsymbol{v})=c f(\boldsymbol{v}) \end{array} \]</span> 两个有限维欧氏空间的映射函数𝑓 ∶ ℝ𝑁 → ℝ𝑀 可以表示为 <span class="math display">\[ \boldsymbol{y}=\boldsymbol{A x} \triangleq\left[\begin{array}{c} a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 N} x_{N} \\ a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 N} x_{N} \\ \vdots \\ a_{M 1} x_{1}+a_{M 2} x_{2}+\cdots+a_{M N} x_{N} \end{array}\right] \]</span> 其中 <span class="math inline">\(𝑨\)</span> 是一个由 <span class="math inline">\(𝑀\)</span> 行 <span class="math inline">\(𝑁\)</span> 列元素排列成的矩形阵列，称为 <span class="math inline">\(M × N\)</span> 的矩阵（Matrix） ： <span class="math display">\[ \boldsymbol{A}=\left[\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1 N} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2 N} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ a_{M 1} &amp; a_{M 2} &amp; \cdots &amp; a_{M N} \end{array}\right] \]</span> 向量 <span class="math inline">\(𝒙 ∈ ℝ^𝑁\)</span> 和 <span class="math inline">\(𝒚 ∈ ℝ^𝑀\)</span> 为两个空间中的向量， 𝒙 和 𝒚 可以分别表示为 <span class="math inline">\(𝑁 × 1\)</span> 的矩阵和 <span class="math inline">\(𝑀 × 1\)</span> 的矩阵. <span class="math display">\[ \boldsymbol{x}=\left[\begin{array}{c} x_{1} \\ x_{2} \\ \vdots \\ x_{N} \end{array}\right], \quad \boldsymbol{y}=\left[\begin{array}{c} y_{1} \\ y_{2} \\ \vdots \\ y_{M} \end{array}\right] . \]</span> 这种表示形式称为<strong>列向量</strong>，即只有一列的矩阵</p><h3 id="矩阵操作">2.2 矩阵操作</h3><p><strong>转置：</strong> <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩阵 <span class="math inline">\(𝑨\)</span> 的转置（Transposition）是一个<span class="math inline">\(𝑁 × 𝑀\)</span> 的矩阵，记为 <span class="math inline">\(𝑨^T\)</span> ，<span class="math inline">\(𝑨^T\)</span> 的第 i 行第 j 列的元素是原矩阵 <span class="math inline">\(𝑨\)</span> 的第 j 行第 i 列的元素， <span class="math display">\[ [A^T]_{ij} = [A]_{ji} \]</span> <strong>哈达玛积（Hadamard）：</strong> 矩阵 𝑨 和矩阵 𝑩 的 Hadamard 积（Hadamard Product）也称为逐点乘积，为 𝑨 和 𝑩 中对应的元素相乘. <span class="math display">\[ [\boldsymbol{A} \odot \boldsymbol{B}]_{i j}=a_{i j} b_{i j} \]</span></p><blockquote><p>举栗： <span class="math display">\[ \begin{bmatrix} a_{1}&amp; a_{2} \\ a_{3}&amp; a_{4} \\ \end{bmatrix}\cdot \begin{bmatrix} b_{1}&amp; b_{2} \\ b_{3}&amp; b_{4} \\ \end{bmatrix} = \begin{bmatrix} a_{1}b_{1}&amp; a_{2}b_{2} \\ a_{3}b_{3}&amp; a_{4}b_{4} \\ \end{bmatrix} \]</span></p></blockquote><p><strong>克罗内克积（Kronecker）</strong> 如果 𝑨 是 <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩阵，𝑩 是 <span class="math inline">\(𝑝 × 𝑞\)</span> 的矩阵，那么它们的Kronecker 积（Kronecker Product）是一个 <span class="math inline">\(𝑚𝑝 × 𝑛𝑞\)</span> 的矩阵： <span class="math display">\[ [\boldsymbol{A} \otimes \boldsymbol{B}]=\left[\begin{array}{cccc} a_{11} \boldsymbol{B} &amp; a_{12} \boldsymbol{B} &amp; \cdots &amp; a_{1 n} \boldsymbol{B} \\ a_{21} \boldsymbol{B} &amp; a_{22} \boldsymbol{B} &amp; \cdots &amp; a_{2 n} \boldsymbol{B} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ a_{M 1} \boldsymbol{B} &amp; a_{M 2} \boldsymbol{B} &amp; \cdots &amp; a_{m n} \boldsymbol{B} \end{array}\right] \]</span></p><blockquote><p>举栗： <span class="math display">\[ \begin{bmatrix} a_{1}&amp; a_{2} \\ a_{3}&amp; a_{4} \\ \end{bmatrix} \otimes \begin{bmatrix} b_{1}&amp; b_{2} \\ b_{3}&amp; b_{4} \\ \end{bmatrix} = \begin{bmatrix} a_{1}b_{1}&amp; a_{2}b_{2} &amp; a_{2}b_{1}&amp; a_{2}b_{2}\\ a_{1}b_{3}&amp; a_{2}b_{4} &amp; a_{2}b_{3}&amp; a_{2}b_{4}\\ a_{3}b_{1}&amp; a_{3}b_{2} &amp; a_{4}b_{1}&amp; a_{4}b_{2}\\ a_{3}b_{3}&amp; a_{3}b_{4} &amp; a_{4}b_{3}&amp; a_{4}b_{4}\\ \end{bmatrix} \]</span></p></blockquote><p><strong>外积</strong> 两个向量 <span class="math inline">\(𝒂 ∈ ℝ^𝑀\)</span> 和 <span class="math inline">\(𝒃 ∈ ℝ^𝑁\)</span> 的外积（Outer Product）是一个 <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩阵，定义为 <span class="math display">\[ \boldsymbol{a} \otimes \boldsymbol{b}=\left[\begin{array}{cccc} a_{1} b_{1} &amp; a_{1} b_{2} &amp; \ldots &amp; a_{1} b_{n} \\ a_{2} b_{1} &amp; a_{2} b_{2} &amp; \ldots &amp; a_{2} b_{n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{M} b_{1} &amp; a_{M} b_{2} &amp; \ldots &amp; a_{M} b_{n} \end{array}\right]=\boldsymbol{a} \boldsymbol{b^T} \]</span></p><blockquote><p><span class="math display">\[ \boldsymbol{a}=\left[\begin{array}{c} a_{1} \\ a_{2} \\ \vdots \\ a_{M} \end{array}\right], \quad \boldsymbol{b}=\left[\begin{array}{c} b_{1} \\ b_{2} \\ \vdots \\ b_{N} \end{array}\right] \]</span></p><p>外积：<span class="math inline">\(a \otimes b = ab^T\)</span> (MX1, Nx1-&gt;MxN)</p></blockquote><blockquote><p>外积通常看作是矩阵的Kronecker积的一种特例，但两者并不等价.</p></blockquote><p><strong>向量化：</strong> 矩阵的向量化是将矩阵表示为一个列向量. 令 <span class="math inline">\(𝑨 = [𝑎_{𝑖𝑗}]_{𝑀×𝑁}\)</span>，向量化算 <span class="math inline">\(vec(⋅)\)</span> 定义为 <span class="math display">\[ \operatorname{vec}(\boldsymbol{A})=\left[a_{11}, a_{21}, \cdots, a_{M 1}, a_{12}, a_{22}, \cdots, a_{M 2}, \cdots, a_{1 N}, a_{2 N}, \cdots, a_{M N}\right]^{\top} \]</span> <strong>迹：</strong> 方块矩阵 <span class="math inline">\(𝑨\)</span> 的对角线元素之和称为它的迹（Trace），记为 <span class="math inline">\(𝑡𝑟(𝑨)\)</span>. 尽管矩阵的 乘法不满足交换律，但它们的迹相同，即 <span class="math inline">\(𝑡𝑟(𝑨𝑩) = 𝑡𝑟(𝑩𝑨)\)</span>.</p><p><strong>秩：</strong> 一个矩阵 <span class="math inline">\(𝑨\)</span> 的列秩是 <span class="math inline">\(𝑨\)</span> 的线性无关的列向量数量，行秩是 <span class="math inline">\(𝑨\)</span> 的线性无关的行 向量数量. 一个矩阵的列秩和行秩总是相等的，简称为秩（Rank）.</p><h3 id="矩阵类型">2.3 矩阵类型</h3><p><strong>对称矩阵（Symmetric Matrix）</strong>指其转置等于自己的矩阵，即满足 <span class="math inline">\(𝑨 = 𝑨^T\)</span> .</p><p><strong>对角矩阵（Diagonal Matrix）</strong> 是一个主对角线之外的元素皆为 0 的矩阵.</p><p><strong>单位矩阵（Identity Matrix）</strong>是一种特殊的对角矩阵，其主对角线元素为 1，其余元素为 0. 𝑁 阶单位矩阵 <span class="math inline">\(𝑰_𝑁\)</span> ，是一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的方块矩阵. 可以记为 <span class="math inline">\(𝐼𝑁 = diag(1, 1, ..., 1)\)</span> .一个 <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩阵 A 和单位矩阵的乘积等于其本身</p><p><strong>逆矩阵</strong> 对于一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的方块矩阵 𝑨，如果存在另一个方块矩阵𝑩 使得 <span class="math display">\[ AB=BA=I_N \]</span> 其中 <span class="math inline">\(𝑰_𝑁\)</span> 为单位阵，则称 𝑨 是可逆的. 矩阵 𝑩 称为矩阵 𝑨 的逆矩阵（Inverse Matrix），记为 <span class="math inline">\(𝑨^{−1}\)</span> .</p><p><strong>正定矩阵</strong> 对于一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的对称矩阵 𝑨，如果对于所有的非零向量 <span class="math inline">\(𝒙 ∈ ℝ^𝑁\)</span> 都满足 <span class="math display">\[ x^TAx&gt;0 \]</span> 则 𝑨 为正定矩阵（Positive-Definite Matrix）。如果 <span class="math inline">\(𝒙^T𝑨𝒙 ≥ 0\)</span>，则𝑨是半正定矩阵</p><blockquote><p>我们发现，所有的<strong>二次齐次式</strong>都可以表示为矩阵的形式，例如：<span class="math inline">\(f=x_{1}^{2}+2 x_{1} x_{2}+4 x_{2}^{2}+6 x_{2} x_{3}+4 x_{3}^{2}\)</span> ，</p><p>就可以表示为：<span class="math inline">\(\left[\begin{array}{lll}x_{1} &amp; x_{2} &amp; x_{3}\end{array}\right]\left[\begin{array}{lll}1 &amp; 1 &amp; 0 \\ 1 &amp; 4 &amp; 3 \\ 0 &amp; 3 &amp; 4\end{array}\right]\left[\begin{array}{l}x_{1} \\ x_{2} \\ x_{3}\end{array}\right]=X^{\prime} A X\)</span></p><p>显然，这个表示是唯一的：每一个二次型都唯一对应一个<strong>对称矩阵</strong> <span class="math inline">\(A\)</span> ，反之亦如此. 无论是这个二次齐次式，还是代表它的矩阵，我们都称之为<strong>二次型</strong>，因为他们指向的是同一件事.</p><p>也许你发现了这样一个事实，<span class="math inline">\(f=\left(x_{1}+x_{1}\right)^{2}+3\left(x_{2}+x_{3}\right)^{2}+x_{3}^{2}=y_{1}^{2}+3 y_{2}^{2}+y_{3}^{2} \geq 0\)</span></p><p><span class="math inline">\(y_{1}, y_{2}, y_{3} \in \mathbb{R}\)</span>，当 <span class="math inline">\(y_{1}, y_{2}, y_{3} \in \mathbb{R}\)</span> 不全为 0 时，这个二次型严格大于 0.</p><p>定义：<strong>当 <span class="math inline">\(X\)</span> 不是零向量的时候，就会有：<span class="math display">\[f=X^{\prime} A X&gt;0 \]</span></strong> 。我们将这样的二次型称为正定的，对称矩阵 <span class="math inline">\(A\)</span> 称为正定矩阵****</p></blockquote><p><strong>正交矩阵</strong> 如果一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的方块矩阵𝑨的逆矩阵等于其转置矩阵，即 <span class="math display">\[ A^T=A^{-1} \]</span> 则𝑨为正交矩阵（Orthogonal Matrix）</p><p>正交矩阵满足 <span class="math inline">\(𝑨^T𝑨 = 𝑨𝑨^T = 𝑰_𝑁\)</span>，即正交矩阵的每一行（列）向量和自身的内积为1，和其他行（列）向量的内积为0.</p><blockquote><p>假设A是一个列向量矩阵，标识为 <span class="math inline">\(A=[\alpha_1,\alpha_2, ...,\alpha_n]\)</span>，那么按照定义就是：</p><center><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210125221333329.png" srcset="/img/loading.gif" lazyload style="width:60%"></center></blockquote><h3 id="特征值与特征向量">2.4 特征值与特征向量</h3><p>对一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的矩阵 <span class="math inline">\(𝑨\)</span> ，如果存在一个标量 𝜆 和一个非零向量 𝒗 满足 <span class="math display">\[ A𝒗 =𝜆𝒗 \]</span> 则 𝜆 和 𝒗 分别称为矩阵 <span class="math inline">\(𝑨\)</span> 的 <strong>特征值（Eigenvalue）</strong>和 <strong>特征向量（Eigenvector）</strong>.</p><p>当用矩阵 𝑨 对它的特征向量 𝒗 进行线性映射时，得到的新向量只是在 𝒗 的长度上缩放 𝜆 倍. 给定一个矩阵的特征值，其对应的特征向量的数量是无限多的. 令 𝒖 和 𝒗 是矩阵 𝑨 的特征值 𝜆 对应的特征向量，则 𝛼𝒖 和 𝒖 + 𝒗 也是特征值 𝜆 对应的特征向量.</p><p>如果矩阵 𝑨 是一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的实对称矩阵，则存在实数 <span class="math inline">\(𝜆_1 , ⋯ , 𝜆_𝑁\)</span>，以及 <span class="math inline">\(𝑁\)</span> 个互相正交的单位向量 <span class="math inline">\(𝒗_1 , ⋯ , 𝒗_𝑁\)</span>，使得 <span class="math inline">\(𝒗_𝑛\)</span> 为矩阵 𝑨 的特征值为 𝜆𝑛 的特征向量 （1 ≤ 𝑛 ≤ 𝑁）.</p><h3 id="矩阵分解">2.5 矩阵分解</h3><h4 id="特征分解">特征分解</h4><p>一个 <span class="math inline">\(𝑁 × 𝑁\)</span> 的方块矩阵 𝑨 的特征分解（Eigendecomposition）定义为 <span class="math display">\[ 𝑨 = 𝑸𝚲𝑸^{−1} \]</span> 其中 𝑸 为 <span class="math inline">\(𝑁 × 𝑁\)</span> 的方块矩阵，其每一列都为 𝑨 的特征向量，𝚲 为对角阵，其每一个对角元素分别为 𝑨 的一个特征值.</p><p>如果 𝑨 为实对称矩阵，那么其不同特征值对应的特征向量相互正交. 𝑨 可以被分解为<br><span class="math display">\[ 𝑨 = 𝑸𝚲𝑸^{T} \]</span> 其中𝑸 为正交阵.</p><h4 id="奇异值分解">奇异值分解</h4><p>一个 <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩阵𝑨的奇异值分解（Singular Value Decomposition，SVD） 定义为 <span class="math display">\[ 𝑨 = 𝑼𝚺𝑽^T \]</span> 其中 𝑼 和 𝑽 分别为 <span class="math inline">\(𝑀 × 𝑀\)</span> 和 <span class="math inline">\(𝑁 × 𝑁\)</span> 的正交矩阵，𝚺 为 <span class="math inline">\(𝑀 × 𝑁\)</span> 的矩形对角矩阵. 𝚺 对角线上的元素称为奇异值（Singular Value），一般按从大到小排列.</p><blockquote><p>奇异值分解在后面学到机器学习算法的时候再详细学习。</p><p>留个坑。</p></blockquote><h3 id="moore-penrose伪逆">==Moore-Penrose伪逆==</h3><p>对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵A的左逆B来求解线性方程： <span class="math display">\[ A x=y \]</span> 等式两边同时左乘左逆B后，得到： <span class="math display">\[ x=By \]</span> 是否存在唯一的映射将A映射到B取决于问题的形式。</p><p>如果矩阵A的行数大于列数，那么上述方程可能没有解；如果矩阵A的行数小于列数，那么上述方程可能有多个解。</p><p>Moore-Penrose伪逆使我们能够解决这种情况，矩阵A的伪逆定义为： <span class="math display">\[ \boldsymbol{A}^{+}=\lim _{a \searrow 0}\left(\boldsymbol{A}^{\top} \boldsymbol{A}+\alpha \boldsymbol{I}\right)^{-1} \boldsymbol{A}^{\top} \]</span> 但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式： <span class="math display">\[ A^{+}=V D^{+} U^{\top} \]</span> 其中，矩阵U，D 和V 是矩阵A奇异值分解后得到的矩阵。对角矩阵D 的伪逆D+ 是其非零元素取倒之后再转置得到的。</p><h3 id="张量">张量</h3><p>几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。</p><p>例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格：</p><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210125110117322.png" srcset="/img/loading.gif" lazyload alt="image-20210125110117322"><figcaption>image-20210125110117322</figcaption></figure><p>其中表的横轴表示图片的宽度值，这里只截取0 ~ 319；表的纵轴表示图片的高度值，这里只截取0~4；表格中每个方格代表一个像素点，比如第一行第一列的表格数据为[1.0,1.0,1.0]，代表的就是RGB三原色在图片的这个位置的取值情况（即R=1.0，G=1.0，B=1.0）为白色。</p><blockquote><p>如图所示，前面5行的数据全是白色。</p></blockquote><p>当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。</p><p>张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。</p><h3 id="几种常用的距离">几种常用的距离</h3><p>上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有100个特征，那么他对应的就是一个100维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的KNN算法和K-means算法中很明显。</p><p>设有两个n维变量 <span class="math inline">\(A=\left[x_{11}, x_{12}, \ldots, x_{1 n}\right]\)</span> 和 <span class="math inline">\(B=\left[x_{21}, x_{22}, \ldots, x_{2 n}\right]\)</span>，则一些常用的距离公式定义如下：</p><h4 id="曼哈顿距离">1、曼哈顿距离</h4><p>曼哈顿距离也称为城市街区距离，数学定义如下： <span class="math display">\[ d_{12}=\sum_{k=1}^{n}\left|x_{1 k}-x_{2 k}\right| \]</span> 曼哈顿距离的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">sum</span>(<span class="built_in">abs</span>(vector1-vector2))</span><br></pre></td></tr></table></figure><h4 id="欧氏距离">2、欧氏距离</h4><p>欧氏距离其实就是L2范数，数学定义如下： <span class="math display">\[ d_{12}=\sqrt{\sum_{k=1}^{n}\left(x_{1 k}-x_{2 k}\right)^{2}} \]</span> 欧氏距离的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span> sqrt((vector1-vector2)*(vector1-vector2).T)</span><br></pre></td></tr></table></figure><h4 id="闵可夫斯基距离">3、闵可夫斯基距离</h4><p>从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义： <span class="math display">\[ d_{12}=\sqrt[p]{\sum_{k=1}^{n}\left(x_{1 k}-x_{2 k}\right)^{p}} \]</span> 实际上，当p=1时，就是曼哈顿距离；当p=2时，就是欧式距离。</p><h4 id="切比雪夫距离">4、切比雪夫距离</h4><p>切比雪夫距离就是<img data-src="https://www.zhihu.com/equation?tex=L_%7B\varpi%7D+" srcset="/img/loading.gif" lazyload alt="[公式]">，即无穷范数，数学表达式如下： <span class="math display">\[ d_{12}=\max \left(\left|x_{1 k}-x_{2 k}\right|\right) \]</span> 切比雪夫距离的Python实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span> sqrt(<span class="built_in">abs</span>(vector1-vector2).<span class="built_in">max</span>)</span><br></pre></td></tr></table></figure><h4 id="夹角余弦">5、夹角余弦</h4><p>夹角余弦的取值范围为[-1,1]，可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值1；当两个向量的方向完全相反时，夹角余弦取最小值-1。</p><p>机器学习中用这一概念来衡量样本向量之间的差异，其数学表达式如下： <span class="math display">\[ \cos \theta=\frac{A B}{|A||B|}=\frac{\sum_{k=1}^{n} x_{1 k} x_{2 k}}{\sqrt{\sum_{k=1}^{n} x_{1 k}^{2}} \sqrt{\sum_{k=1}^{n} x_{2 k}^{2}}} \]</span> 夹角余弦的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span> dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))</span><br></pre></td></tr></table></figure><h4 id="汉明距离">6、汉明距离</h4><p>汉明距离定义的是两个字符串中不相同位数的数目。</p><blockquote><p>例如：字符串‘1111’与‘1001’之间的汉明距离为2。</p></blockquote><p>信息编码中一般应使得编码间的汉明距离尽可能的小。</p><p>汉明距离的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">smstr = nonzero(matV[<span class="number">0</span>]-matV[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span> smstr</span><br></pre></td></tr></table></figure><h4 id="杰卡德相似系数">7、杰卡德相似系数</h4><p>两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数，用符号J(A,B)表示，数学表达式为： <span class="math display">\[ J(A, B)=\frac{|A \cap B|}{|A \cup B|} \]</span> 杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。</p><h4 id="杰卡德距离">==8、杰卡德距离==</h4><p>与杰卡德相似系数相反的概念是杰卡德距离，其定义式为： <span class="math display">\[ J_{\sigma}=1-J(A, B)=\frac{|A \cup B|-|A \cap B|}{|A \cup B|} \]</span> 杰卡德距离的Python实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> scipy.spatial.distance <span class="keyword">as</span> dist</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span> dist.pdist(matV,<span class="string">&#x27;jaccard&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="概率论">概率论</h1><h2 id="随机变量">1.1 随机变量</h2><p><strong>随机变量</strong> ：在随机试验中，试验的结果可以用一个数 𝑋 来表示，这个数 𝑋 是随着试验 结果的不同而变化的，是样本点的一个函数. 我们把这种数称为随机变量</p><p><strong>离散随机变量</strong>：如果随机变量 <span class="math inline">\(𝑋\)</span> 所可能取的值为有限可列举的，有 <span class="math inline">\(𝑁\)</span> 个有限取值 <span class="math display">\[ {𝑥_1, ⋯ , 𝑥_𝑁 }, \]</span> 则称 <span class="math inline">\(𝑋\)</span> 为离散随机变量</p><p>要了解 <span class="math inline">\(𝑋\)</span> 的统计规律，就必须知道它取每种可能值 <span class="math inline">\(𝑥_𝑛\)</span> 的概率，即 <span class="math display">\[ 𝑃(𝑋 = 𝑥_𝑛) = 𝑝(𝑥_𝑛), ∀𝑛 ∈ \{ 1, ⋯ , 𝑁 \} \]</span> 其中 <span class="math inline">\(𝑝(𝑥_1 ), ⋯ , 𝑝(𝑥_𝑁 )\)</span> 称为 <strong>离散随机变量 <span class="math inline">\(𝑋\)</span> 的概率分布</strong>（Probability Distribution） 或分布，并且满足 <span class="math display">\[ \begin{array}{l} \sum_{n=1}^{N} p\left(x_{n}\right)=1 \end{array} \]</span></p><p><span class="math display">\[ p\left(x_{n}\right) \geq 0, \quad \forall n \in\{1, \cdots, N\} \]</span></p><p><u>常见的离散随机变量</u>的概率分布有：</p><p><strong>伯努利分布</strong> 在一次试验中，事件 A 出现的概率为 <span class="math inline">\(𝜇\)</span> ，不出现的概率为 <span class="math inline">\(1 − 𝜇\)</span>. 若用 变量 <span class="math inline">\(𝑋\)</span> 表示事件<span class="math inline">\(𝑨\)</span> 出现的次数，则 <span class="math inline">\(𝑋\)</span> 的取值为 0 和 1，其相应的分布为 <span class="math display">\[ p(x)=\mu^{x}(1-\mu)^{(1-x)} \]</span> 这个分布称为 <u>伯努利分布（Bernoulli Distribution）, 又名 两点分布 或者 0-1 分布</u>.</p><strong>二项分布</strong> 在n次伯努利试验中，若以变量 <span class="math inline">\(𝑋\)</span> 表示事件A出现的次数，则𝑋 的取值 为{0, ⋯ , 𝑁}，其相应的分布为二项分布（Binomial Distribution）. <span class="math display">\[ P(X=k)=\left(\begin{array}{c} N \\ k \end{array}\right) \mu^{k}(1-\mu)^{N-k}, \quad k=0 \cdots, N \]</span> 其中 $ <span class="math display">\[\begin{pmatrix} N\\K \end{pmatrix}\]</span><p>$ 为二项式系数，表示从 <span class="math inline">\(𝑁\)</span> 个元素中取出 <span class="math inline">\(𝑘\)</span> 个元素而不考虑其顺序的组 合的总数.</p><p><strong>连续随机变量</strong>：与离散随机变量不同，一些随变量 <span class="math inline">\(𝑋\)</span> 的取值是不可列举的，由全部实数或者由一部分区间组成，比如 <span class="math display">\[ 𝑋 = \{𝑥|𝑎 ≤ 𝑥 ≤ 𝑏\}, −∞ &lt; 𝑎 &lt; 𝑏 &lt; ∞ \]</span> 则称 <span class="math inline">\(𝑋\)</span> 为连续随机变量. 连续随机变量的值是不可数及无穷尽的.</p><p>连续随机变量 <span class="math inline">\(𝑋\)</span> 的概率分布一般用概率密度函数（Probability Density Function，PDF）<span class="math inline">\(𝑝(𝑥)\)</span> 来描述. <span class="math inline">\(𝑝(𝑥)\)</span> 为可积函数，并满足 <span class="math display">\[ \int_{-∞}^{+∞} p(x)dx=1 \]</span> <u>常见的连续随机变量</u>的概率分布有：</p><p><strong>均匀分布</strong> 若𝑎, 𝑏为有限数，[𝑎, 𝑏]上的均匀分布（Uniform Distribution）的概率密度函数定义为 $$</p><p><span class="math display">\[ **正态分布** 正态分布（Normal Distribution），又名高斯分布（Gaussian Distribution），是自然界最常见的一种分布，并且具有很多良好的性质，其概率密度函数为 \]</span></p><p><span class="math display">\[ 其中 $𝜎 &gt; 0$，$𝜇$ 和 $𝜎$ 均为常数. 若随机变量 $𝑋$ 服从一个参数为 $𝜇$ 和 $𝜎$ 的概率分布， 简记为 \]</span> 𝑋 ∼ N(𝜇, 𝜎^2) $$ 当 <span class="math inline">\(𝜇 = 0，𝜎 = 1\)</span> 时，称为标准正态分布（Standard Normal Distribution）</p><center><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210127224821542.png" srcset="/img/loading.gif" lazyload alt="pokeman-parameters" style="width:70%"></center><h3 id="条件概率分布">1.2 条件概率分布</h3><p><strong>条件分布</strong> 对于离散随机向量 (𝑋, 𝑌 )，已知 𝑋 = 𝑥 的条件下，随机变量 𝑌 = 𝑦 的条件概 率（Conditional Probability）为</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a><br>转载请注明出处来源：<a href="https://stuxiaozhang.github.io">小张的宇宙空间站</a> ！</p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/01/29/Python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Python多线程编程</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/"><span class="hidden-mobile">机器学习介绍</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.staticfile.org/valine/1.4.14/Valine.min.js",function(){var i=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论我鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:null,emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"],appid:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appkey:"CgnvRL262D07ied40NiXm2VL"},{el:"#valine",path:window.location.pathname});new Valine(i)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js"></script><script src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js"></script><script src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js"></script><script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>