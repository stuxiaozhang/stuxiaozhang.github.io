<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="BERT：Pre-training of Deep Bidirectional Transformers for Language Understanding
还没写完 没时间细看了….只是肤浅的了解了一下

BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它强调了不再像以往一样采用传统的单向"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>【BERT】Pre-training of Deep Bidirectional Transformers for Language Understanding - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="【BERT】Pre-training of Deep Bidirectional Transformers for Language Understanding"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-09-23 16:00" pubdate>2021年9月23日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2.8k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 21 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">【BERT】Pre-training of Deep Bidirectional Transformers for Language Understanding</h1><p class="note note-info">本文最后更新于：2022年1月28日</p><div class="markdown-body"><h1 id="BERT：Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><a href="#BERT：Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding" class="headerlink" title="BERT：Pre-training of Deep Bidirectional Transformers for Language Understanding"></a>BERT：Pre-training of Deep Bidirectional Transformers for Language Understanding</h1><blockquote><p>还没写完 没时间细看了….只是肤浅的了解了一下</p></blockquote><p>BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的<strong>masked language model（MLM）</strong>，以致能生成<strong>深度的双向</strong>语言表征。BERT论文发表时提及在11个NLP（Natural Language Processing，自然语言处理）任务中获得了新的state-of-the-art的结果，令人目瞪口呆。</p><p>该模型有以下主要优点：</p><p>1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。</p><p>2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。</p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>论文为 BERT 提供了两种模型尺寸：</p><ul><li>BERT BASE – 在大小上与 OpenAI Transformer 相当以比较性能</li><li>BERT LARGE – 一个巨大模型，达到了论文中报告的最先进的结果</li></ul><p>BERT 基本上是经过训练的 Transformer Encoder 堆栈。两种 BERT 模型大小都有大量的编码器层（论文称之为 Transformer Blocks）——Base 版本有 12 个，Large 版本有 24 个。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210924153318372.png" srcset="/img/loading.gif" lazyload alt="image-20210924153318372" style="zoom:50%"></p><p>以往的预训练模型的结构会受到单向语言模型<em>（从左到右或者从右到左）</em>的限制，因而也限制了模型的表征能力，使其只能获取单方向的上下文信息。而 BERT 利用 MLM 进行预训练并且采用深层的双向 Transformer 组件来构建整个模型，因此最终生成<strong>能融合左右上下文信息</strong>的深层双向语言表征。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210924085521642.png" srcset="/img/loading.gif" lazyload alt=""></p><h3 id="BERT的输入输出"><a href="#BERT的输入输出" class="headerlink" title="BERT的输入输出"></a>BERT的输入输出</h3><h4 id="BERT的输入"><a href="#BERT的输入" class="headerlink" title="BERT的输入"></a>BERT的输入</h4><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210924100223494.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>BERT 的输入为每一个 token 对应的表征<em>（图中的粉红色块就是 token，橙色块就是 token 对应的表征）</em>，并且单词字典是采用 WordPiece 算法来进行构建的。为了完成具体的分类任务，除了单词的 token 之外，作者还在输入的每一个序列开头都插入特定的<strong>分类token <code>[CLS]</code></strong>，该分类 token 对应的最后一个Transformer 层输出被用来起到聚集整个序列表征信息的作用。</p><p>由于 BERT 是一个预训练模型，其必须要适应各种各样的自然语言任务，因此模型所输入的序列必须有能力包含一句话<em>（文本情感分类，序列标注任务）</em>或者两句话以上<em>（文本摘要，自然语言推断，问答任务）</em>。BERT 采用了两种方法来区分输入的两个句子：</p><ol><li>在序列 tokens 中把<strong>分割 token <code>[SEP]</code></strong>插入到每个句子后，以分开不同的句子 tokens。</li><li>为每一个 token 表征都添加一个可学习的分割 embedding 来指示其属于句子A还是句子B。</li></ol><p>因此最后模型的输入序列 tokens 如上图<em>（如果输入序列只包含一个句子的话，则没有 <code>[SEP]</code> 及之后的 token）</em>.</p><p>BERT 的输入为每一个 token 对应的表征，实际上该表征是由三部分组成的，分别是<strong>对应的token</strong>，<strong>分割</strong>和<strong>位置</strong> embeddings</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210923160242048.png" srcset="/img/loading.gif" lazyload alt="token表征的组成"></p><p>输出向量接了一个分类器。<code>[CLS]</code> 向量不能代表整个句子的语义信息？？？</p><ul><li><code>Token Embeddings</code>：是词向量，对所有词汇做 embedding</li><li><code>Segment Embeddings</code>：用来区别两种句子，因为预训练不光做 LM 还要做以两个句子为输入的分类任务</li><li><code>Position Embeddings</code>：随机初始化，模型自己去学（和 Transformer 不一样）。</li></ul><h4 id="BERT-的输出"><a href="#BERT-的输出" class="headerlink" title="BERT 的输出"></a>BERT 的输出</h4><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210924101528545.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p><strong>C</strong> 为分类 token <code>[CLS]</code> 对应最后一个 Transformer 的输出，$T_i$ 则代表其他 token 对应最后一个 Transformer 的输出。对于一些 token 级别的任务<em>（如，序列标注和问答任务）</em>，就把 $T_i$ 输入到额外的输出层中进行预测。对于一些句子级别的任务<em>（如，自然语言推断和情感分类任务）</em>，就把 <strong>C</strong> 输入到额外的输出层中，这里也就解释了为什么要在每一个 token 序列前都要插入特定的分类 token。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210924083719931.png" srcset="/img/loading.gif" lazyload alt=""></p><h3 id="BERT-的预训练任务"><a href="#BERT-的预训练任务" class="headerlink" title="BERT 的预训练任务"></a>BERT 的预训练任务</h3><h4 id="Masked-Language-Model（MLM）"><a href="#Masked-Language-Model（MLM）" class="headerlink" title="Masked Language Model（MLM）"></a>Masked Language Model（MLM）</h4><p>MLM 是 BERT 能够不受单向语言模型所限制的原因。简单来说就是以15%的概率用 mask token <code>[MASK]</code> 随机地对每一个训练序列中的 token 进行替换，然后预测出 <code>[MASK]</code> 位置原有的单词。然而，由于 <code>[MASK]</code> 并不会出现在下游任务的微调（fine-tuning）阶段，因此预训练阶段和微调阶段之间产生了<strong>不匹配</strong>（这里很好解释，就是预训练的目标会令产生的语言表征对 <code>[MASK]</code> 敏感，但是却对其他token不敏感）。因此 BERT 采用了以下策略来解决这个问题：</p><p>首先在每一个训练序列中以 15% 的概率随机地选中某个 token 位置用于预测，假如是第 $i$ 个 token 被选中，则会被替换成以下三个 token 之一</p><ol><li><p>80% 的时候是[MASK]。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>[MASK]</strong></p></li><li><p>10% 的时候是随机的其他 token。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>apple</strong></p></li><li><p>10% 的时候是原来的 token<em>（保持不变，有博主认为是作为 2. 所对应的负类）</em>。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>hairy</strong></p></li></ol><p>再用该位置对应的 $T_i$ 去预测出原来的 token，即输入到全连接，然后用 softmax 输出每个 token 的概率，最后用交叉熵计算 loss）。该策略令到 BERT 不再只对 <code>[MASK]</code> 敏感，而是对所有的 token都敏感，以致能抽取出任何 token 的表征信息。</p><p><strong>那么为啥要以一定的概率使用随机词呢？</strong></p><p>这是因为 transformer 要保持对每个输入 token 分布式的表征，否则 Transformer 很可能会记住这个 <code>[MASK]</code> 就是 “hairy”。至于使用随机词带来的负面影响，文章中解释说,所有其他的 token (即非 “hairy” 的 token)共享 15%*10% = 1.5% 的概率，其影响是可以忽略不计的。Transformer 全局的可视，又增加了信息的获取，但是不让模型获取全量信息。</p><blockquote><p>论文中有关于该策略的实验数据，可以去<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf">原文</a>查看详情。</p></blockquote><p>注意：</p><ul><li>有参数 <code>dupe_factor</code> 决定数据 duplicate 的次数。</li><li>其中，<code>create_instance_from_document</code> 函数，是构造了一个 sentence-pair 的样本。对每一句，先生成 <code>[CLS]+A+[SEP]+B+[SEP]</code>，有长（0.9）有短（0.1），再加上 mask，然后做成样本类 object。</li><li><code>create_masked_lm_predictions</code> 函数返回的 tokens 是已经被遮挡词替换之后的 tokens</li><li><code>masked_lm_labels</code> 则是遮挡词对应位置真实的 label</li></ul><h4 id="Next-Sentence-Prediction（NSP）"><a href="#Next-Sentence-Prediction（NSP）" class="headerlink" title="Next Sentence Prediction（NSP）"></a>Next Sentence Prediction（NSP）</h4><p>一些如问答、自然语言推断等任务需要理解两个句子之间的关系，而 MLM 任务倾向于抽取 <strong>token 层次</strong>的表征，因此不能直接获取<strong>句子层次</strong>的表征。为了使模型能够有能力理解句子间的关系，BERT 使用了 NSP 任务来预训练，简单来说就是预测两个句子是否连在一起。具体的做法是：对于每一个训练样例，在语料库中挑选出句子A和句子B来组成，50% 的时候句子B就是句子A的下一句<em>（标注为 IsNext）</em>，剩下 50% 的时候句子B是语料库中的随机句子<em>（标注为 NotNext）</em>。接下来把训练样例输入到 BERT 模型中，用 <code>[CLS]</code> 对应的 C 信息去进行二分类的预测。</p><ul><li><p>Bert 先是用 Mask 来提高视野范围的信息获取量，增加 duplicate 再随机 Mask，这样跟RNN类方法依次训练预测没什么区别了除了mask不同位置外；</p></li><li><p>全局视野极大地降低了学习的难度，然后再用 <code>A + B or C</code> 来作为样本，这样每条样本都有 50% 的概率看到一半左右的噪声；</p></li><li><p>但直接学习 <code>Mask A + B or C</code> 是没法学习的，因为不知道哪些是噪声，所以又加上 next_sentence 预测任务，与 MLM 同时进行训练，这样用 next 来辅助模型对噪声/非噪声的辨识，用 MLM 来完成语义的大部分的学习。</p></li></ul><h4 id="预训练任务总结"><a href="#预训练任务总结" class="headerlink" title="预训练任务总结"></a>预训练任务总结</h4><p>最后训练样例长这样：</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs inform7">Input1 = <span class="hljs-comment">[CLS]</span> the <span class="hljs-keyword">man</span> went to <span class="hljs-comment">[MASK]</span> store <span class="hljs-comment">[SEP]</span> he bought a gallon <span class="hljs-comment">[MASK]</span> milk <span class="hljs-comment">[SEP]</span><br><br>Label1 = IsNext<br><br>Input2 = <span class="hljs-comment">[CLS]</span> the <span class="hljs-keyword">man</span> <span class="hljs-comment">[MASK]</span> to the store <span class="hljs-comment">[SEP]</span> penguin <span class="hljs-comment">[MASK]</span> <span class="hljs-keyword">are</span> flight ##less birds <span class="hljs-comment">[SEP]</span><br><br>Label2 = NotNext<br></code></pre></td></tr></table></figure><p>把每一个训练样例输入到 BERT 中可以相应获得两个任务对应的 loss，再把这两个 loss 加在一起就是整体的预训练 loss。<em>（也就是两个任务<strong>同时</strong>进行训练）</em></p><blockquote><p>可以明显地看出，这两个任务所需的数据其实都可以从<strong>无标签的</strong>文本数据中构建（自监督性质），比 CV 中需要人工标注的 ImageNet 数据集可简单多了。</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>总结下 BERT 的<strong>主要贡献</strong>：</p><ul><li>引入了Masked LM，使用双向 LM 做模型预训练。</li><li>为预训练引入了新目标 NSP，它可以学习句子与句子间的关系。</li><li>进一步验证了更大的模型效果更好： 12 —&gt; 24 层。</li><li>为下游任务引入了很通用的求解框架，不再为任务做模型定制。</li><li>刷新了多项NLP任务的记录，引爆了NLP无监督预训练技术。</li></ul><p><strong>BERT优点</strong>：</p><ul><li><p>Transformer Encoder 因为有 Self-attention 机制，因此 BERT 自带双向功能</p></li><li><p>因为双向功能以及多层 Self-attention 机制的影响，使得 BERT 必须使用 Cloze 版的语言模型 Masked-LM 来完成 token 级别的预训练</p></li><li><p>为了获取比词更高级别的句子级别的语义表征，BERT 加入了 Next Sentence Prediction 来和 Masked-LM 一起做联合训练</p></li><li><p>为了适配多任务下的迁移学习，BERT 设计了更通用的输入层和输出层</p></li><li><p>微调 fine-tine 成本小</p></li></ul><p><strong>BERT缺点：</strong></p><ul><li><p>task1: MLM 的随机遮挡策略略显粗犷，推荐阅读《Data Nosing As Smoothing In Neural Network Language Models》</p></li><li><p><code>[MASK]</code> 标记在实际预测中不会出现，训练时用过多 <code>[MASK]</code> 影响模型表现;</p></li><li><p>每个 batch 只有15%的 token 被预测，所以 BERT 收敛得比 left-to-right 模型要慢（它们会预测每个 token）</p></li><li><p>BERT 对硬件资源的消耗巨大（大模型需要16个 tpu，历时四天；更大的模型需要64个 tpu，历时四天。)</p></li></ul><h5 id="训练-Bert："><a href="#训练-Bert：" class="headerlink" title="训练 Bert："></a>训练 Bert：</h5><ol><li>找一个预训练 Language Model。（例如 中文谷歌BERT）</li><li>在相同领域上 继续训练 LM（Domain transfer）。</li><li>在任务相关的小数据 继续训练 LM（Task transfer）。</li><li>在任务相关数据上做具体任务（Fine-tune）。</li></ol><h5 id="如何在相同领域数据中进行-further-pre-training？"><a href="#如何在相同领域数据中进行-further-pre-training？" class="headerlink" title="如何在相同领域数据中进行 further pre-training？"></a>如何在相同领域数据中进行 further pre-training？</h5><ol><li>动态 mask：就是每次 epoch 训练前去 mask，不是一直使用同一个</li><li>n-gram mask：比如做 ERNIE 和 SpanBert 都是做了类似词的 mask</li></ol></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/papers/">papers</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/NLP/">NLP</a> <a class="hover-with-bg" href="/tags/BERT/">BERT</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/09/25/A%20Survey%20on%20Dialogue%20Summarization%EF%BC%9ARecent%20Advances%20and%20New%20Frontiers/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">A Survey on Dialogue Summarization：Recent Advances and New Frontiers</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/09/19/ELMo%EF%BC%9ADeep%20contextualized%20word%20representations/"><span class="hidden-mobile">ELMo：Deep contextualized word representations</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>