<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="A Survey on Dialogue Summarization: Recent Advances and New Frontiers
对话摘要旨在将对话中最重要的信息提取到较短的段落中，帮助人们快速捕捉半结构化、多参与者对话的亮点，而无需回顾复杂的对话背景。
单参与者文档摘要已经取得了成功，但这些方法很难推广到多参与者对话摘要。问题在于：

一个对话的关键信息往往是分散的，跨越了不同"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>A Survey on Dialogue Summarization：Recent Advances and New Frontiers - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="A Survey on Dialogue Summarization：Recent Advances and New Frontiers"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-09-25 23:38" pubdate>2021年9月25日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 4k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 28 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">A Survey on Dialogue Summarization：Recent Advances and New Frontiers</h1><p class="note note-info">本文最后更新于：2021年9月25日</p><div class="markdown-body"><h1 id="a-survey-on-dialogue-summarization-recent-advances-and-new-frontiers">A Survey on Dialogue Summarization: Recent Advances and New Frontiers</h1><p>对话摘要旨在将对话中最重要的信息提取到较短的段落中，帮助人们快速捕捉半结构化、多参与者对话的亮点，而无需回顾复杂的对话背景。</p><p>单参与者文档摘要已经取得了成功，但这些方法很难推广到多参与者对话摘要。问题在于：</p><ol type="1"><li>一个对话的关键信息往往是分散的，跨越了不同参与者的多个话语和话轮，导致信息密度低。</li><li>对话包含多个参与者、固有的话题转移、频繁的共同引用、多样的交互信号和领域术语。</li></ol><p>有关对话摘要，过去的五年中有60多篇paper涵盖了不同的对话领域，如图。</p><p><img src="C:%5CUsers%5CHP%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210924220315084.png" srcset="/img/loading.gif" lazyload alt="image-20210924220315084" style="zoom:67%"></p><h2 id="background">Background</h2><h3 id="overview-of-summarization">Overview of Summarization</h3><p>自动摘要的目的是将原始输入压缩成包含显著信息的较短版本，帮助人们快速掌握核心内容，而不必深入细节。主要分为两种：</p><ul><li>抽取式摘要：选取重要句子作为摘要，更准确、更忠实</li><li>生成式摘要：采用新颖的词语生成摘要，提高了摘要的简洁性和流畅性。</li></ul><h3 id="evaluation-metrics">Evaluation Metrics</h3><p><strong>ROUGE</strong>：通常被用作评估总结任务的标准度量，主要包括 ROUGE-1、ROUGE-2 和 ROUGE-L 的 F1 分数，分别测量 ground truth 和生成的摘要之间的单词重叠、二元重叠和最长公共序列。</p><p>最近一些工作提出了基于情景化表示的新指标，比如 BERTScore 和 MoverScore，而不是精确的表面形式匹配，它们与人类判断具有更高的相关性。</p><h2 id="对话摘要-按领域分类">对话摘要 按领域分类</h2><h3 id="meeting-summarization">Meeting Summarization</h3><p>以前用的是抽取式摘要生成，但是由于多方参与的性质，会议中的信息分散且不连贯。所以后人转向自动摘要抽取。</p><p>随着神经网络的发展，许多研究探索了深度学习在满足总结任务中的应用，并取得了显著的成功。尽管基于深度学习的方法具有很强的建模能力，但仅仅考虑字面上的信息是不够的。这是因为在会议发言中有不同的交互信号，长时间的会议记录进一步对传统的顺序模型提出了挑战。为此，一些作品致力于将<u>辅助信息</u>纳入更好的建模会议，如 <code>dialogue discourse</code> [Ganesh and Dingliwal，2019；Feng et al.，2020a]、<code>dialogue acts</code> [Goo and Chen，2018；Di et al.，2020]和 <code>domain terminologies</code> [Koay et al.，2020]。为了处理长时间的会议记录，Zhao等人[2019b]提出了一种分层自适应分段编码器-解码器网络，该网络可自动将会议分段为局部一致的部分。Zhu等人[2020]和Rohde等人[2021]采用分层架构，从 token-level 级别到 turn-level 对会议进行建模。Koay等人[2021]提出了一种滑动窗口方法来逐步处理冗长的会议。</p><p>除了会议摘要，也可以生成会议特定方面的摘要，比如决策、行动、想法和假设等等。最近，提出了基于查询的会议摘要任务，其目的是根据给定的一般查询和特定查询对会议的特定部分进行摘要。除了多方和多流特征外，会议摘要也是一项多模态任务[Renals，2011]。会议可以包括参与者显示的各种类型的非语言信息，如音频、视频和运动功能。因此，大多数工作通过融合言语和非言语特征来研究抽取式多模态会议摘要问题，以丰富话语的表达[Erol等人，2003年；Murray等人，2005年；Nihei等人，2018年]。最近，Li 等人[2019a]研究了抽象<u>多模态会议摘要问题</u>。他们建议使用非语言特征，即视觉注意力焦点（VFOA），来强调一个话语的重要性。</p><p><strong>Leaderboard</strong>：Fabbri 等人[2021]已经尝试创建一个 benchmark。基于他们的努力，SCIR 提供了更全面的 benchmark。</p><p><strong>Highlight</strong>：会议总是有几个特定角色的参与者参与。因此，有必要对这种独特的角色特征进行建模。此外，long transcripts 还需要模型能够处理长序列。此外，会议的视听记录提供了利用多种方式信息的机会。然而，这是一把双刃剑。自动语音识别系统和视觉工具的错误率也对当前的模型提出了挑战，这要求它们更加健壮。</p><h3 id="chat-summarization">Chat Summarization</h3><p>SAMSum 是第一个高质量、手动注释的聊天摘要语料库，并进行了 baseline 实验，迅速激发了这一研究方向。Chen和Yang[2020]迈出了第一步，提出了一种结合主题段和对话阶段的多视图摘要器。更重要的是，他们对这项任务中的挑战进行了全面的研究，如多个参与者之间的不同互动以及频繁出现的共同引用，这可以推动这一方向。</p><p>为了对交互进行建模，一些工作采用了图建模策略。赵等人[2020]利用细粒度主题词作为话语之间的桥梁，构建主题词引导的对话图。陈和杨（2021）认为 话语间对话话语结构 和 话语内作用三元组 明确地塑造了相互作用。Feng等人[2020b]将 常识知识 视为对话背后的认知交互信号，并展示了知识整合和异构建模对不同类型数据的有效性</p><p>由于多个参与者的性质和频繁出现的共指，模型生成的对话摘要总是存在事实不一致的问题[Gabriel et al.，2020]。为此，Lei等人[2021]强调通过说话人感知的自我注意机制模拟参与者及其相关人称代词之间的复杂关系。Liu等人[2021b]明确地将共指信息纳入对话摘要模型中。值得注意的是，他们进行数据后处理，以减少由文档共指解析模型导致的不正确的共指分配。从另一个角度来看，Narayan等人[2021]和Wu等人[2021]都通过从粗到精的生成改进了对话摘要的事实一致性。最终对话摘要由先例控制，例如草图或实体链。</p><p>由于当前的对话摘要系统通常使用附加信息对文本进行编码，Feng等人[2021]提出了一种无监督的DialoGPT注释器，它可以执行三个注释任务，包括关键词提取、冗余检测和主题分割。</p><p>尽管 SAMSum 已经成为对话总结的 benchmark，但它也可以扩展到其他研究方向。Gunasekara等人[2021]探讨了 summary-to-dialogue generation 的问题，并验证了增强的 dialogue-summary 对能够很好地进行对话摘要。Mehnaz等人[2021]将SAMSum中的英语对话转换为印地语英语对话，并在代码切换设置下学习聊天摘要。</p><p><strong>Highlight</strong>：得益于预先训练的语言模型，当前的方法能够熟练地将原始聊天转化为简单的摘要实现。然而，他们仍然很难选择重要的部分，并且容易产生幻觉。在未来，这项任务需要探索强大的聊天建模策略和推理能力。</p><h3 id="email-threads-summarization">Email Threads Summarization</h3><p>电子邮件线程是一种异步多方通信，由多个参与者之间一致交换电子邮件消息组成，广泛应用于企业、学术和工作环境。与其他类型的对话相比，电子邮件有一些独特的特点。首先，它和元数据相关联，包括发送方、接收方、主体和签名。其次，电子邮件信息始终代表发件人的意图，包含行动项目，并可能使用引号来突出重要部分。第三，与面对面的口头对话不同，电子邮件中的回复不会立即发生。这种异步性质可能导致消息包含长句。为了应对电子邮件过载，电子邮件服务提供商寻求高效的摘要技术来改善用户体验。</p><h3 id="customer-service-summarization">Customer Service Summarization</h3><p>Customer Service 是顾客和代理人之间一对一的直接互动，经常发生在消费者行为之前和之后。</p><p>一方面， Customer Service 的参与者有很强的解决问题的意图和明确的动机，这使得 Customer Service 具有内在的逻辑性和围绕特定的主题。为此，一些工作探索了这个任务的主题建模。Liuet al.[2019a]采用粗到细的生成框架，首先生成一系列关键点(主题)来指示对话的逻辑，然后实现详细的总结。例如，一个关键点序列可以是提问→解决→用户认可→结束，这清楚地展示了对话的演变。Zouet al.[2021b] 为了缓解数据不足的问题，Zouet等人[2021a]提出了一个名为RankAE的无监督框架，其中首先根据中心性和多样性同时选择主题话语，并进一步使用去噪自动编码器生成最终的摘要。</p><p>另一方面， Customer Service 是一种面向任务的对话，它包含信息实体，涉及多个领域，并涉及两种不同类型的参与者。为了整合各种信息，Yuan和Yu[2019]提出了Scaffold Pointer Network，利用speaker角色、语义槽和对话域三种信息。以往的工作以第三人称的视角进行总结。由于 Customer Service 的参与者扮演着不同的角色，zhang等人[2021]提出了一个基于变分自动编码器的无监督框架，分别为客户和代理生成摘要。</p><p>Customer Service 旨在解决代理商提出的问题。因此，它自然具有强烈的动机，这使得对话在两个具有鲜明特征的参与者——客户和代理之间的互动之后，有了特定的演化方式。因此，对参与者角色、演进链和固有主题进行建模对于这项任务是很重要的。此外，还需要考虑一些细粒度的信息，如 slot 和 intent [Qinet al.， 2021]。</p><h3 id="medical-dialogue-summarization">Medical Dialogue Summarization</h3><p><strong>重点</strong>：医疗对话总结主要是帮助医生快速完成电子病历，医疗对话总结应该更加忠实而非创造性。因此，优选提取方法与简单提取方法相结合的方法。主题信息可以作为生成半结构化摘要的指导原则。此外，医学对话中的术语和否定应谨慎处理。</p><h3 id="other-types-of-dialogue-summarization">Other Types of Dialogue Summarization</h3><p>对话是由一个以上的人所产生的任何话语[Ford, 1991]。除了以上提到的对话类型，之前的工作还涉及播客[Zheng et al.， 2020]、在线讨论论坛[Tarnpradabet al.， 2021]、法律辩论[Duanet al.， 2019]和读者评论线程[Barkeret al.， 2016]总结任务。朝着更实用的方向，Tepperet al.[2018]提出了个性化聊天摘要任务，该任务通过话题分布和社交图连接隐式学习用户兴趣，并基于此提供个性化摘要。</p><h2 id="new-frontiers">New Frontiers</h2><h3 id="对话总结中的忠实度">对话总结中的忠实度</h3><p>尽管目前最先进的摘要系统已经取得了很大的进展，但它们存在事实不一致的问题，扭曲或捏造文章中的事实信息，也被称为幻觉 hallucinations [Huanget al.， 2021]。Chen and Yang [2020]指出错误的引用是因为对话摘要模型犯的一个主要错误，这意味着所生成的摘要包含不忠实于原对话信息(例如,将一个人的行为或位置与错误的 speaker 结合起来)，如图2所示。这种错误在很大程度上阻碍了对话摘要系统的应用。</p><p><img src="C:%5CUsers%5CHP%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210925232320623.png" srcset="/img/loading.gif" lazyload alt="image-20210925232320623" style="zoom:80%"></p><p>作者认为，这一问题主要是由对话的多重参与者和不同的参考因素造成的。通过明确地将人称代词信息[Leiet al.， 2021]和共指信息[Liuet al.， 2021b]纳入摘要模型。尽管如此，由于缺乏高质量的对话共参分辨率模型，导致了低质量的分辨率结果，进一步影响了摘要的质量。</p><p>为了缓解这一挑战，不仅可以增强具有对话特征的共指解析模型，还可以利用语境信息和语篇信息对共指进行隐式建模。</p><h3 id="multi-modal-dialogue-summarization">Multi-modal Dialogue Summarization</h3><p>对话往往发生在多模态的情况下，如会议的视听记录。除了语言信息外，非语言信息既可以补充已有信息，也可以提供新的信息，有效地丰富了纯语篇对话的表征。根据不同模式是否可以对齐，多模态信息的类型可分为同步和异步两类。</p><p>同步多模态对话主要是指会议，会议可能包含文本文本、韵律音频和视频。一方面，考虑对齐的音频和视频可以增强文本的表示。另一方面，音频和视频都可以提供新的见解，比如一个人进入房间加入会议或情感讨论。然而，面部特征和声纹特征已经成为了个人优越的隐私，这使得它们难以被获取和敏感。未来的工作可以考虑在联邦学习框架下的多模式会议总结[Liet al.， 2019b]。</p><p>异步多模态对话是指在不同时间发生的不同模态。随着通信技术的发展，应用程序在聊天对话中频繁使用语音、图片、表情等多模式信息。这些消息提供了丰富的信息，作为对话流的一部分。未来工作可以通过ASR系统获得的语音信息的文本信息，与表情相关的图片和情感提供的新实体，以产生有意义的总结。</p><h3 id="multi-domain-dialogue-summarization">Multi-domain Dialogue Summarization</h3><p>领域学习可以挖掘不同领域之间的共享信息，进一步帮助特定领域的任务，是一种适用于低资源场景的有效学习方法。由于摘要数据集的多样性，已经有一些研究对话摘要的多领域学习或领域适应的著作[Sanduet al.， 2010;Zhuet, 2020;Y et al.， 2021]。我们将这一方向分为宏观多领域学习和微观多领域学习两类。</p><p>宏观多领域学习旨在使用通用领域摘要数据集，如新闻和科学论文，以帮助对话摘要任务。这种学习方法工作的基础是，不管它们属于什么数据类型，它们的目标是选择原始文本的核心内容。然而，对话具有一些独特的特征，如更多的引用和参与者相关的特征。因此，直接使用这些通用数据集可能会降低它们的有效性。未来的作品可以首先注入一些对话的特定特征，比如用人称代词代替名字，或者将原始的通用领域文档转化为表层的turn-level文档。</p><p>微观多领域学习的目标是使用对话摘要数据集来帮助完成一个特定的对话摘要任务。例如，使用会议数据集来帮助电子邮件任务。如表1所示，近年来，人们提出了涵盖不同领域的多种对话摘要数据集。未来的工作可以采用元学习方法或依赖预先训练的语言模型来使用不同的数据集。</p><p>现在都用什么做？</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/NLP/">NLP</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/NLP/">NLP</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/10/11/%E3%80%90Multi-Hop%20KG%E3%80%91Multi-Hop%20Knowledge%20Graph%20Reasoning%20with%20Reward%20Shaping/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">【Multi-Hop KG】Multi-Hop Knowledge Graph Reasoning with Reward Shaping</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/09/23/BERT%EF%BC%9APre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding/"><span class="hidden-mobile">【BERT】Pre-training of Deep Bidirectional Transformers for Language Understanding</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>