<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="General Guide以下就是如何让你做得更好的攻略，它适用于前期所有的作业。

从最上面开始，如果你觉得结果不满意，第一件事是检查 training data 的 loss。如果发现 training data 的 loss 很大，显然它在训练集上面也没有训练好。接下来就要分析，在训练集上面没有学好的原因是什么。有两个种可能，可能是 model 的 bias，可能是 Optimization"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>李宏毅 深度学习基础 - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="李宏毅 深度学习基础"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-06-09 10:07" pubdate>2021年6月9日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 4.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 37 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">李宏毅 深度学习基础</h1><p class="note note-info">本文最后更新于：2021年8月10日</p><div class="markdown-body"><h2 id="General-Guide"><a href="#General-Guide" class="headerlink" title="General Guide"></a>General Guide</h2><p>以下就是如何让你做得更好的攻略，它适用于前期所有的作业。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609103623679.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>从最上面开始，如果你觉得结果不满意，第一件事是<strong>检查 training data 的 loss</strong>。如果发现 training data 的 loss 很大，显然它在训练集上面也没有训练好。接下来就要分析，在训练集上面没有学好的原因是什么。有两个种可能，可能是 model 的 bias，可能是 Optimization Issue。</p><h3 id="Model-bias"><a href="#Model-bias" class="headerlink" title="Model bias"></a>Model bias</h3><p>model bias 就是说，model 太过简单。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609103903538.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>举例，现在写了一个含有未知参数的 function，参数带入不同的数，就组成了一个 function set ，用它来表示 model。但是这个 function set 太小了，不存在可以让 loss 降低的 funciton。</p><blockquote><p>说明这个 model 比较没用，就像大海捞针，结果针不在海里…</p></blockquote><p>这个时候<strong>重新设计一个 model，给 model 更大的弹性</strong>。举例来说，可以<strong>增加输入的 features</strong> 。以上次课程的预测观影人数举栗，本来输入的 features 只有前一天的数据，要预测接下来的观看人数。用前一天的数据不够多，就用前28天、前56天的数据，那 model 的弹性就会增大。</p><p>也可以用 Deep Learning 增加更多的弹性。</p><p>所以如果 model 的弹性不够大，那可以增加更多 features，可以设一个更大的model。或者可以用 deep learning 来增加 model 的弹性。这是第一个可以的解法。</p><h3 id="Optimization-Issue"><a href="#Optimization-Issue" class="headerlink" title="Optimization Issue"></a>Optimization Issue</h3><p>training data 的 loss 大并不一定是 model bias 的问题。可能是 Optimization 做的不好。</p><p>比如卡在 local minima 的地方，没有办法找到一个真的可以让 loss 很低的参数，像下图这样。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609114342633.png" srcset="/img/loading.gif" lazyload style="zoom:70%"></p><p>蓝色部分是 model 可以表示的函式所形成的集合。你可以把 $θ$ 代入不同的数值形成不同的function，把所有的 function 通通集合在一起得到这个蓝色的 set 。这个蓝色的 set 里面确实包含了一些 function，这些 function 它的 loss 是低的，但<u>问题是 gradient descent 这个演算法，没法找出这个 loss 低的 function</u>。gradient descent 找到 $θ^<em>$ 然后就结束了，但这个 $θ^</em>$ 它的 loss 不够低。这一个 model 里面，存在着某一个 function ，它的 loss 是够低的，但是 gradient descent 没有给我们这一个 function 。</p><blockquote><p>就像大海捞针，针确实在海里，但是我们却没有办法把针捞起来。</p></blockquote><p>那么，training data 的 loss 不够低的时候，到底是 model bias 还是 optimization 的问题呢？</p><h4 id="Gaining-the-insights-from-comparison"><a href="#Gaining-the-insights-from-comparison" class="headerlink" title="Gaining the insights from comparison"></a>Gaining the insights from comparison</h4><p>一个有用的判断方法是，通过比较不同的模型判断你的 model 够不够大。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609143806330.png" srcset="/img/loading.gif" lazyload style="zoom:90%"></p><p>如图所示，可以看到测试数据，56层的 loss 比20层的 loss 高。但是这个不是 overfitting ，因为可以看到训练数据同样也是56层的 loss 比20层的 loss 高。这说明56层的网络 optimization 没有做好。那为什么是因为 optimization 而不是model bias，因为56层的网络一定比20层的网络弹性好。所以说出现这个问题，说明 optimization 做的不好。</p><p>如果现在经过一番努力，training data 的 loss 变小了，那接下来可以看 testing data 的loss 。如果 testing data 的 loss也小，那就结束了，没啥好做的了:P。</p><h3 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h3><p>什么是过拟合呢？</p><ul><li>如果 <strong>training data 的 loss 小，testing data 的 loss 大</strong>，那可能就遇到 overfitting 的问题。</li></ul><p>为什么会有这种情况？</p><blockquote><p>举个通俗的栗子：上学考试的时候，有的人采取题海战术，把每个题目都背下来。但是题目稍微一变，他就不会做了。因为他非常复杂的记住了每道题的做法，而没有抽象出通用的规则。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609165034992.png" srcset="/img/loading.gif" lazyload alt="image-20210609165034992"></p></blockquote><p>举个极端的栗子：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609163443384.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>这个函数表明，输入为x时，看训练集里有没有对应的 $\hat{y}$，有的话就输出，没有就随机输出一个值。那这个函数相当于啥也没干。虽然他啥也没做，但是在 training data 的 loss 可是0呢。但在 testing data 上 loss 就会很大，因为他什么都没有预测，就是随机输出…</p><p>再举普通的栗子：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609165456416.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>假设一个能力很强、弹性很大的 model，只给三个点作为 trainning data。</p><p>如果只用这三个数据，没有其他训练集作为限制，那这个 model 就会 freestyle，会产生奇奇怪怪的结果。这时，你再输入 testing data，loss 就会很大。</p><p><strong><em>那如何解决过拟合的问题呢？</em></strong></p><h4 id="1-增加训练集"><a href="#1-增加训练集" class="headerlink" title="1. 增加训练集"></a>1. 增加训练集</h4><p>也许这个方向往往是最有效的方向，是<strong>增加训练集</strong></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609170651739.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>如果训练集，蓝色的点变多了，那虽然 model 它的弹性可能很大，但是因为数据非常非常的多，它就可以限制住 model 不那么 freestyle。</p><p>增加数据集可以采用 <strong>data augmentation</strong> 的方式。例如可以对图像做水平翻转、截取等等。要根据你对资料的特性，对现在要处理的问题的理解，来选择合适的 data augmentation 的方式。</p><h4 id="2-给模型增加限制"><a href="#2-给模型增加限制" class="headerlink" title="2. 给模型增加限制"></a>2. 给模型增加限制</h4><p>另一个方法就是<strong>给模型增加一些限制</strong>，让它不要有那么大的弹性。</p><ol><li>给它<strong>比较少的参数</strong>。例如 DL 中，把一层的神经元数减少或者让 model 共用参数(like CNN)。</li><li>用<strong>比较少的features</strong></li><li><strong>Early stopping</strong></li><li><strong>Dropout</strong></li></ol><p>但是也不要给太多的限制，因为可能又回到了 model bias 的问题。</p><p>假设现在的模型一定设置成了 Linear Model，那它产生的 function 就一定是一条直线。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609172721431.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>给出的这三个点，没有办法产生一条直线，可以同时通过这三个点。这个时候，模型的限制就太大了，测试集不会有好结果。</p><p>但这个不是 overfitting，而是又回到了 <strong>model bias</strong> 的问题，因为模型的限制太大。</p><h3 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h3><p>把 Training 的资料分成两部分，一部分叫作 Training Set，一部分是 Validation Set。通过 Validation Set 来衡量模型的好坏。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609213036461.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h3 id="N-fold-Cross-Validation"><a href="#N-fold-Cross-Validation" class="headerlink" title="N-fold Cross Validation"></a>N-fold Cross Validation</h3><p>N-fold Cross Validation就是先<strong>把训练集切成N等份</strong>，在这个例子里面我们切成三等份，切完以后，拿其中<strong>一份当作 Validation Set</strong>，<strong>另外两份当 Training Set</strong>，然后这件事情要<strong>重复三次</strong></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609213743965.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>假设有三个模型需要判断，可以在这三种训练集的设定下，每个 model 都跑三次数据集，然后这个 model 的三次结果取平均。最终，三个模型的结果进行比较看谁的更好。</p><h3 id="Mismatch"><a href="#Mismatch" class="headerlink" title="Mismatch"></a>Mismatch</h3><p>Mismatch：训练集和测试集的分布不同。</p><p>like this…</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609154942184.png" srcset="/img/loading.gif" lazyload style="zoom:70%"></p><h2 id="Critical-Point"><a href="#Critical-Point" class="headerlink" title="Critical Point"></a>Critical Point</h2><h3 id="Training-Fails-because"><a href="#Training-Fails-because" class="headerlink" title="Training Fails because"></a>Training Fails because</h3><p>常常在做 Optimization 的时候，会发现<strong>随着的参数不断的update，training 的 loss 不再下降</strong>，或者有可能一开始 loss 就掉不下去。</p><p>掉不下去的原因是因为走到了一个 gradient 为0的地方，这个地方参数对 loss 的微分为0。此时，gradient descent 就没有办法再更新参数，这个时候training就会停止，loss 当然也就不会下降。</p><p>gradient 为0的点有两种情况：<strong>local minima</strong> 和 <strong>saddle point</strong>。如图，local minima 是局部的最低点。而 saddle point， 是 gradient 是零，但不是local minima，也不是 local maxima 的地方，像一个马鞍的形状，所以也翻译成鞍点。</p><ul><li>如果卡在<strong>local minima，那可能就没有路可以走了</strong></li><li>如果卡在<strong>saddle point，saddle point旁边还是有路可以走的，</strong>只要逃离saddle point，就有可能让 loss 更低</li></ul><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210609220523185.png" srcset="/img/loading.gif" lazyload style="zoom:70%"></p><p>那当 gradient 接近0时，如何判断到底是哪种类型？</p><h4 id="Warning-of-Math"><a href="#Warning-of-Math" class="headerlink" title="Warning of Math"></a>Warning of Math</h4><div class="note note-info"><p>（缺乏数学知识导致这一段有些难理解…）</p></div><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul><li>Critical points have zero gradients.</li><li>Critical points can be either saddle points or local minima.<ul><li>Can be determined by the Hessian matrix.</li><li>Local minima may be rare.</li><li>It is possible to escape saddle points along the direction of eigenvectors of the Hessian matrix</li></ul></li></ul><h2 id="Batch"><a href="#Batch" class="headerlink" title="Batch"></a>Batch</h2><p>在算微分时，并不是对所有的 Data 算出来的 L 做微分。而是把 data 分为一个个的 batch。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210610084142724.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>每一个 batch 的大小为 B，每次在更新参数的时候，<strong>是拿一个 B 的 data 出来，算个 Loss，算个 Gradient，更新参数</strong>。<strong>所有的 Batch 看过一遍，叫做一个 Epoch</strong>。通常会在每一个 Epoch 开始之前做一个 Shuffle，就是重新分 Batch，随机打乱。每一个 Epoch 的 Batch 都不一样，就是 Shuffle。</p><h3 id="Small-Batch-v-s-Large-Batch"><a href="#Small-Batch-v-s-Large-Batch" class="headerlink" title="Small Batch v.s. Large Batch"></a>Small Batch v.s. Large Batch</h3><p>为什么用 Batch 呢？</p><p>举个栗子：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210610085136616.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><ul><li><p>左边的 Case 就是没有用 Batch，即 Batch = N（Full Batch）</p><p>Batch = N 时，所有 N 笔 Examples 都看完以后，才能够计算 Loss，才能够计算 Gradient，参数才能够更新一次。</p></li><li><p>右边的 Case 就是 Batch = 1</p><p>Batch = 1 时，在每一个 Epoch 里面，参数都会更新 N 次，就是看一次更新一次，所以他的 loss 时曲曲折折的。</p></li></ul><p>那么，哪种方式好一些呢？</p><p>看起来，左边的没有用 Batch 的方式时间可能会比较长，因为要看完所有的数据才更新一次。但是优点是比较稳定。右边的时间可能比较短，但是不稳定。<strong>实际上，考虑并行运算的话,左边这个并不一定时间比较长</strong></p><h4 id="Larger-batch-size-does-not-require-longer-time-to-compute-gradient"><a href="#Larger-batch-size-does-not-require-longer-time-to-compute-gradient" class="headerlink" title="Larger batch size does not require longer time to compute gradient"></a>Larger batch size does not require longer time to compute gradient</h4><p>验证：比较大的 Batch Size 算 Loss，再进而算 Gradient，所需要的时间，不一定比小的 Batch Size 要花的时间长</p><p>LHY 做了一个实验来验证，发现 bs = 1 和 bs = 1000 所需时间差不多。因为GPU可以做并行计算。这些数据可以并行处理，所以 1000 笔数据所需时间不是 1 笔数据的 1000 倍。然而 GPU处理数据量也是有上限的，所以数据如果很大时，会发现时间还是会增大。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210610093102242.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><h4 id="Smaller-batch-requires-longer-time-for-one-epoch"><a href="#Smaller-batch-requires-longer-time-for-one-epoch" class="headerlink" title="Smaller batch requires longer time for one epoch"></a>Smaller batch requires longer time for one epoch</h4><p>更新一次 Batch 时，因为 GPU 的并行运算能力，bs = 1 和 bs = 1000 所需时间相差无几。</p><p>然而，跑完一整个 Epoch 的时间就相差很多。bs = 1 时，需要更新 60000 次 Batch 才能跑完一整个 Epoch。bs = 1000 时，更新 60 次 Batch 就能跑完。右图中可以明显看到时间上的差距，bs = 1 比 bs = 1000 慢很多。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210610093247673.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><p>所以，直觉上觉得，大的 Batch 所需时间长，但是如果考虑到 GPU 的并行计算，一个 Batch 大的 Epoch 花的时间反而是比较少的。</p><p>那这么说，大的 Batch Size 不就没有缺点了吗？</p><h4 id="“Noisy”-update-is-better-for-training"><a href="#“Noisy”-update-is-better-for-training" class="headerlink" title="“Noisy” update is better for training"></a>“Noisy” update is better for training</h4><p>LHY 的实验表明，小的 bs 反而性能更好。小的 bs 的 noisy 的 gradient 会帮助 train。大的 bs结果比较差，因为 Optimization 会有问题。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210610093320005.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>假设是 Full Batch，每次更新参数都是沿着这一个 loss function 进行梯度下降，当下降到 critical point 时，可能就会卡住没法进行更新。</p><p>如果是 Small Batch，因为每次更新都是挑一个 Batch 算它的 Loss，相当于是不同的 loss function 进行梯度下降，当遇到 saddle point 时可能会卡住，但是换另一个 loss function 更新时，也许就可以继续下去了。</p><p>所以这种 Noisy 的 Update 的方式，结果反而对 Training 是有帮助的。</p><h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><div class="table-container"><table><thead><tr><th></th><th style="text-align:center">Small</th><th style="text-align:center">Large</th></tr></thead><tbody><tr><td>Speed for one update (no parallel)</td><td style="text-align:center">Faster</td><td style="text-align:center">Slower</td></tr><tr><td>Speed for one update (with parallel)</td><td style="text-align:center">Same</td><td style="text-align:center">Same(not too large)</td></tr><tr><td>time for one epoch</td><td style="text-align:center">Slower</td><td style="text-align:center"><strong>Faster</strong></td></tr><tr><td>Gradient</td><td style="text-align:center">Noisy</td><td style="text-align:center">Stable</td></tr><tr><td>Optimization</td><td style="text-align:center"><strong>Better</strong></td><td style="text-align:center">Worse</td></tr><tr><td>Generalization</td><td style="text-align:center"><strong>Better</strong></td><td style="text-align:center">Worse</td></tr></tbody></table></div><p>bs 的大小各自都有优势。所以，<strong>Batch Size</strong>变成了一个需要去调整的 Hyperparameter。</p><p><strong>Smaller batch size help escape critical points.</strong></p><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>在物理世界里，一个球从高处斜坡滚落，如果遇到 saddle point，因为<strong>惯性</strong>，它不会被卡住，而是继续前进，如果动量够大，甚至会翻过这个坡。Momentum 就是这样的存在。</p><h4 id="Vanilla-一般的-Gradient-Descent"><a href="#Vanilla-一般的-Gradient-Descent" class="headerlink" title="Vanilla(一般的) Gradient Descent"></a>Vanilla(一般的) Gradient Descent</h4><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210610111814035.png" srcset="/img/loading.gif" lazyload style="zoom:100%"></p><p>一般的梯度下降，设置初始的参数 $\theta^0$，计算一下它的 gradient，然后往 gradient 的反向更新参数。</p><script type="math/tex;mode=display">\theta^1 = \theta^0 - \eta g^0</script><p>然后重复这个过程。</p><h4 id="Gradient-Descent-Momentum"><a href="#Gradient-Descent-Momentum" class="headerlink" title="Gradient Descent + Momentum"></a>Gradient Descent + Momentum</h4><p>为了突破 critical point，我们在 Gradient Descent 基础上加上 Momentum。</p><p>就是我们在更新参数时，<strong>不只是 Gradient 的反方向，再加上前一步更新的方向，二者加和去更新参数。</strong></p><blockquote><p>我的理解：</p><ul><li>Movement：当前前进方向，当前的 Momentum</li><li>Momentum：Movement of the last step</li></ul></blockquote><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210610112403096.png" srcset="/img/loading.gif" lazyload style="zoom:100%"></p><p>Gradient Descent + Momentum 的 Movement 过程是这样：</p><p>同样，设置初始的参数 $\theta^0$，然后再假设前一步的 update 量为0</p><script type="math/tex;mode=display">\theta^0 = 0
 \\
 m^0 = 0</script><p>在 $\theta^0$ 的地方，计算 Gradient 的方向 $g^0$。然后要计算 Movement，<u>因为是初始，所以前一步的 Momentum 是0，与 Gradient 的反方向结合</u>，Movement 方向就变成：</p><script type="math/tex;mode=display">m^1 = \lambda m^0 - \eta g^0 = - \eta g^0
\\
Move\quad to: \theta ^1 = \theta ^0 + m^1</script><p>参数现在到 $\theta^1$，计算 Gradient 的方向 $g^1$。然后要计算 Movement，<u>前一步的 update 方向是 $m1$，与 Gradient 的反方向结合</u>，Movement 的方向就变成：</p><script type="math/tex;mode=display">m^2 = \lambda m^1 - \eta g^1 
\\
Move\quad to: \theta ^2 = \theta ^1 + m^2</script><p>接下来也是反复进行同样的过程，在这个位置计算出 Gradient，然后 将前一步的方向与Gradient 反方向结合，作为下一步的移动方向。</p><p>每一步的移动 Movement 我们都用 m 来表示，</p><script type="math/tex;mode=display">m^0 = 0
\\
m^1 = \lambda m^0 - \eta g^0 = - \eta g^0
\\
m^2 = \lambda m^1 - \eta g^1 = - \lambda \eta g^0 - \eta g^1
\\
m^3 = \lambda m^2 - \eta g^2 = - \lambda (- \lambda \eta g^0 - \eta g^1) - \eta g^2
\\
...</script><p>当前的 Momentum 是 上一步的 Movement 加上这次梯度的反方向。另一种解读是，上面的式子类推，发现，<u>当加上 Momentum 时，Update 的方向，不是只考虑现在的 Gradient，而是考虑过去所有 Gradient 的总合。</u></p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>假设有两个参数，它们对 <strong>Loss 的斜率差别非常大</strong>，在 $w_1$ 这个方向上斜率变化很小，在 $w_2$ 这个方向上斜率变化很大。如果是<strong>固定的 learning rate</strong>，可能很难得到好的结果。所以说需要 adaptive 的 learning rate、 Adam 等等比较进阶的 optimization 的方法,才能够得到好的结果。</p><p>如果从另外一个方向想，<strong>直接把难做的 error surface 把它改掉</strong>，看能不能够改得好做一点。</p><p>首先，为什么会有两个参数相差比较大的情况呢？</p><p>假设一个简单的 linear model，$y = w_1<em>x_1 + w_2</em>x_2 + b$</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210612103105451.png" srcset="/img/loading.gif" lazyload alt=""></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210612103119376.png" srcset="/img/loading.gif" lazyload alt=""></div></div></div><p>对于 $\Delta w$，当 $x_1$ 输入很小时， $\Delta w$ 变化，$y$ 的变化就很小。当 $x_2$ 输入很大时， $\Delta w$ 即使变化可能很小，$y$ 的变化也会很大。那样 L 的变化就会很大。当 $w$ 改变一点点，那 error surface 就会有很大的变化。</p><p>既然在这个 linear model 里，当 input 的 feature，<strong>每一个 dimension 的值，它的 scale 差距很大</strong>的时候，就可能产生像这样的 error surface，就可能产生<strong>不同方向，斜率非常不同，坡度非常不同的 error surface</strong></p><p>所以有没有办法，可以给 feature 里面<strong>不同的 dimension，让它有同样的数值的范围</strong>，这样就可以有比较好的 error surface，让训练变得容易一些。</p><p>其实有很多不同的方法，这些不同的方法合起来统称为 <strong>Feature Normalization</strong></p><h3 id="Feature-Normalization"><a href="#Feature-Normalization" class="headerlink" title="Feature Normalization"></a>Feature Normalization</h3><p>举个栗子：假设 $x^1$ 到 $x^R$ ，是所有的训练资料的 feature vector。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210612165401860.png" srcset="/img/loading.gif" lazyload style="zoom:100%"></p><p>每一个 vector 里面，$x_1$ 里 $x_1^1$ 代表 $x_1$ 的第一个 element，以此类推。</p><p>把<strong>不同笔资料即不同 feature vector，同一个 dimension</strong> 里面的数值取出来，计算第 i 个 dimension 的 平均值 $m_i$，计算第 i 个 dimension 的标准差 $\sigma_i$，然后可以做一个 normalization，标准化，也叫 standardization</p><script type="math/tex;mode=display">\tilde{x}_{i}^{r} \leftarrow \frac{x_{i}^{r}-m_{i}}{\sigma_{i}}</script><p>得到新的数值 $\tilde{x}$ 以后，<strong>再将新的数值把它塞回去</strong></p><p>好处是？</p><ul><li>normalize 以后，这个 dimension 上面的数值就会平均是 0， variance就会是 1,所以<strong>这一排数值的分布就都会在 0 上下</strong></li><li>对每一个 dimension 都做一样的 normalization，就会发现所有 feature 不同 dimension 的数值都在 0 上下,那就可以<strong>制造一个比较好的 error surface</strong></li></ul><p>Feature Normalization 的方式对 training 有帮助。它可以让你在做 gradient descent 的时候，这个 gradient descent <strong>的 Loss 收敛更快一点,可以让 gradient descent 的训练更顺利一点</strong></p><h3 id="Considering-Deep-Learning"><a href="#Considering-Deep-Learning" class="headerlink" title="Considering Deep Learning"></a>Considering Deep Learning</h3><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>在 deep network 中</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210612170834778.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>我们对 input 做了 Feature Normalization，通过一层网络后输出，这个输出 $z_1, z_2…$ 也是下一层的输入，可能经过运算后数值分布又有了很大差异，那就也需要对类似的这些 feature 做 normalization。</p><p>以 $z_1,z_2,z_3$ 为例，算出 $\mu$ ，算出 $\sigma$</p><script type="math/tex;mode=display">\begin{aligned}
\boldsymbol{\mu} &=\frac{1}{3} \sum_{i=1}^{3} \mathbf{z}^{i} \\
\boldsymbol{\sigma} &=\sqrt{\frac{1}{3} \sum_{i=1}^{3}\left(\mathbf{z}^{i}-\boldsymbol{\mu}\right)^{2}}
\end{aligned}</script><p>接下来就把这边的每一个 $z$，都去减掉 $\mu$ 除以 $\sigma$，得到 $\tilde{z}^{i}$。输出作为新一轮的输入，以此类推。这就是这个网络。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210612231003466.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>出现一个问题，训练资料很多，不能把整个数据 load 进 GPU，所以每次只送入一个 Batch 的数据，这就是 <strong>Batch Normalization</strong>。</p><p>在做 <strong>BN</strong> 的时候，会设置两个向量 $\gamma$ 和 $\beta$：</p><script type="math/tex;mode=display">\hat{\mathbf{z}}^{i}=\gamma \odot \tilde{\mathbf{z}}^{i}+\boldsymbol{\beta}</script><p>为什么这么做？</p><p>如果做 normalization 以后，$\tilde{z}^{i}$ 它的平均就一定是 0。这可能会给 network 一些限制，<strong>也许这个限制会带来什麼负面的影响</strong>，所以我们把 $\gamma$ 和 $\beta$ 加回去。</p><div class="note note-info"><p>其实我没太明白为什么…</p></div><p>在训练时：</p><ul><li>$\gamma$ 的初始值，都设为 1 的 one vector</li><li>$\beta$ 的初始值，都设为 0 的 one vector</li></ul><p>所以让 network 在一开始训练的时候，每一个 dimension 的分布是比较接近的。也许训练到后来已经训练够长的一段时间，已经找到一个比较好的 error surface，走到一个比较好的地方以后，再把 $\gamma$ 和 $\beta$ 慢慢地加进去。加 Batch Normalization，往往对训练是有帮助的。</p><h4 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h4><p>Batch Normalization 在 training 的时候，会分 batch 进入。</p><p>但是在 testing 时，不可能等一批数据完整进入才做一次运算。而 $\gamma$ 和 $\beta$ 必须要等一个完整的 batch 才能计算。如何解决这个问题？</p><p>答：<strong>moving average</strong>.</p><p>在 training 的时候，如果在做 Batch Normalization，在 training 的时候，每取一个 batch ，就计算出来 $\mu^i$ 和 $\sigma ^i$ ，算 <strong>moving average</strong>：</p><script type="math/tex;mode=display">\overline{\boldsymbol{\mu}} \leftarrow p \overline{\boldsymbol{\mu}}+(1-p) \boldsymbol{\mu}^{t}
\quad
(p 在Pytorch里通常取 0.1)</script><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210612234903184.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>所以在 testing 的时候，没有 batch 这个东西。就直接拿 $\overline{\boldsymbol{\mu}}$ 跟 $\overline{\boldsymbol{\sigma}}$，也就是 $\overline{\boldsymbol{\mu}}$ 跟 $\overline{\boldsymbol{\sigma}}$ 在训练的时候得到的 moving average $\overline{\boldsymbol{\mu}}$ 跟 $\overline{\boldsymbol{\sigma}} $来取代这边的 $\mu$ 跟 $\sigma$。</p><p>这个就是 Batch Normalization 在 testing 的时候的运作方式。</p><h2 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h2><h2 id="激活函数？"><a href="#激活函数？" class="headerlink" title="激活函数？"></a>激活函数？</h2></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> <a class="hover-with-bg" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/07/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20NN/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">神经网络 NN</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/06/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/"><span class="hidden-mobile">李宏毅 机器学习介绍</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>