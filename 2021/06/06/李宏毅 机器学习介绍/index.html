<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="Machine Learning
机器学习就是让机器具备找一个函数的能力。本质就是寻找一个函数 function，来找到一个输入 input 与输出 output 之间的映射关系。可以是输入一段语音，输出这段语音对应的文字；也可以输入一张图，输出这张图的内容；也可以输入一场棋局，输出下一步应该走哪一格。
根据函数输出结果，机器学习有不同的类别：

回归任务Regression：
分类任务"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>李宏毅 机器学习介绍 - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="李宏毅 机器学习介绍"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-06-06 13:55" pubdate>2021年6月6日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 14 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">李宏毅 机器学习介绍</h1><p class="note note-info">本文最后更新于：2021年8月10日</p><div class="markdown-body"><h2 id="machine-learning">Machine Learning</h2><p><strong>机器学习就是让机器具备找一个函数的能力</strong>。本质就是寻找一个函数 function，来找到一个输入 input 与输出 output 之间的映射关系。可以是输入一段语音，输出这段语音对应的文字；也可以输入一张图，输出这张图的内容；也可以输入一场棋局，输出下一步应该走哪一格。</p><p>根据函数输出结果，机器学习有不同的类别：</p><ol type="1"><li><strong>回归任务Regression</strong>：</li><li><strong>分类任务Classification</strong>：</li><li><strong>结构化学习structured learning</strong>：</li></ol><p>举一个预测 youtube 次日观影次数的栗子。任务是：找到一个函数，输出第二天的观看次数。</p><p>ML 分为3步骤：</p><p><strong>1. 建立 model</strong>：Function with Unknown Parameters</p><p><strong>2. 定义损失函数</strong>：Define Loss from Training Data</p><p><strong>3. 优化参数</strong>：Optimization</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/MLstep.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h3 id="建立-model">1. 建立 model</h3><p>先设定一个带有未知参数的函数。具体选择建立什么样的函数需要一定的domain knowledge。在这里我们先建立模型： <span class="math display">\[ y = b + w * x_1 \]</span></p><ul><li><p><span class="math inline">\(y\)</span>：准备预测的次数</p></li><li><p><span class="math inline">\(x_1\)</span>：是前一天总共观看的次数，是已知的<strong>特征 feature</strong></p></li><li><p><span class="math inline">\(b\)</span> 跟 <span class="math inline">\(w\)</span> 是未知参数，要通过数据找出来。</p><p><span class="math inline">\(b\)</span>：<strong>偏置bias</strong></p><p><span class="math inline">\(w\)</span>：<strong>权重weight</strong></p></li></ul><p>这个带有 Unknown Parameter 的 Function 就叫做 <strong>Model</strong></p><h3 id="定义损失函数">2. 定义损失函数</h3><p>损失函数 <span class="math inline">\(L(b, w)\)</span>，<span class="math inline">\(L\)</span> 越小说明参数越好。</p><p>损失函数的计算过程：对每一个训练样本，计算其模型预测值（输出）与真实值（label）之间的<strong>误差</strong>（e），整个模型的损失函数就是全部训练样本误差的平均值。</p><p>不同的误差计算方式：MAE / MSE / Cross-entropy <span class="math display">\[ e = \left| y-\hat{y} \right| \quad MAE \]</span></p><p><span class="math display">\[ e = (y-\hat{y})^2 \quad MSE \]</span></p><p>Error Surface：误差曲面。绘制出参数和Loss之间的映射关系的等高线图</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/error surface.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h3 id="优化参数">3. 优化参数</h3><p>优化参数的目的是将参数优化到使Loss值最小。</p><p>优化方法：梯度下降 gradient descend</p><h4 id="一个参数-w">3.1 一个参数 <span class="math inline">\(w\)</span></h4><p>画出其对应的 error surface</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/w_errorsurface.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><p>先随便选个 <span class="math inline">\(w\)</span>，然后计算它的梯度。梯度，也就是斜率，当梯度为负时就应增大 <span class="math inline">\(w\)</span>，这样就可以减小 loss</p><blockquote><p>注意图片中的 error surface 是假的，因为如果以 MAE 或 MSE 来计算不会为负。</p><p>loss func 本身可以随便设置。在真实情况下 loss 确实可能非负。</p></blockquote><p>增大 x 的幅度（步长）由<strong>学习率 learning rate</strong> 和<strong>斜率</strong>决定。学习率是一个需要提前自己设定的超参数 hyperparameter（相比超参而言，<span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span> 这种参数是要通过数据学习的）</p><p>如此迭代多次更新 <span class="math inline">\(w\)</span>（停止条件：迭代次数达到阈值（超参）或微分趋于0）</p><p>梯度下降的问题在于只能找到局部最小值（但其实这是个假问题，将在后面课程中解释）</p><h4 id="两个参数-w-和-b">3.2 两个参数 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span></h4><p>与一个参数的计算过程类似。随机选取初始参数，计算器偏微分，使用梯度下降的方式同时更新两个参数，多次迭代直至停止。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606160045810.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606160100428.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h3 id="测试">测试</h3><p>通过训练阶段得到 <span class="math inline">\(w^∗ = 0.97 , b^∗ = 0.1 k\)</span> ，训练集上的损失函数 $L(w^<em>,b^</em>)=0.48k <span class="math inline">\(，测试集上的损失函数\)</span>L'=0.58k$</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606162931084.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>通过上图真实值和拟合值对比，可以看出数据有<u>明显的周期性</u>，因此修改自变量为前 <span class="math inline">\(j\)</span> 日的流量。</p><p>尝试拟合1日、7日、28日、56日的数据来预测下一日流量。可以发现随着自变量日期增加，在测试集上的结果不再变好。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606163451911.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><h2 id="线性模型到神经网络">线性模型到神经网络</h2><p>上面的那种模型都是线性模型linear model，但是线性模型太简单，不管怎么调参数都只是一条直线（下图蓝线），无法拟合复杂的真实情况（如下图红线）</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606163853191.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>这种模型本身的限制叫做 model bias（注意跟模型参数的那个bias不一样），它指的意思是说,没有办法模拟真实的状况</p><p>所以我们需要写一个更复杂的,更有弹性的,有未知参数的 Function。</p><p>如下图所示，这条红线可以用很多个这种蓝线来加总拟合。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606164438853.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>事实上，可以使用分段函数去逼近任何的连续的曲线，每一个分段函数的 curve 又可以用一大堆蓝色的 Function 组合起来</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606164946779.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>这种蓝线（hard sigmoid）可以用<strong>sigmoid</strong>来拟合</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606165045533.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>不同的参数（<span class="math inline">\(c\)</span> <span class="math inline">\(b\)</span> <span class="math inline">\(w\)</span>） → 不同的 sigmoid function → 组合逼近出不同的piecewise linear function → 近似得到不同的 continuous function</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606165732730.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><ul><li>改 <span class="math inline">\(w\)</span> ，就会改变<strong>斜率</strong>，就会改变斜坡的坡度</li><li>改 <span class="math inline">\(b\)</span>，就会把这一个 Sigmoid Function 左右移动</li><li>改 <span class="math inline">\(c\)</span> ，就会改变它的高度</li></ul><div class="note note-info"><p>为什么需要非线性变化？用于拟合 hard sigmoid 为什么需要不同参数？为了得到不同的 hard sigmoid</p></div><p>最后合出来的红线（真实情况）就可以写成这样的公式： <span class="math display">\[ y=b+\sum_{i} c_{i} \text { sigmoid }\left(b_{i}+w_{i} x_{1}\right) \]</span> <img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606170202531.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>这个公式是针对单独一个特征。我们刚才已经选取了更多特征作为输入。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210606195621970.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><h3 id="back-to-ml_step1设定model">back to ML_step1：设定model</h3><p>以 <span class="math inline">\(j\)</span> 是 1 2 3 的状况举例，就是<strong>只考虑三个 Feature</strong>。</p><ul><li>所以 <span class="math inline">\(j\)</span> 等于 1 2 3，输入就是 <span class="math inline">\(x_1\)</span> 代表前一天的观看人数，<span class="math inline">\(x_2\)</span> 两天前观看人数，<span class="math inline">\(x_3\)</span> 三天前的观看人数</li><li>每一个 <strong>i 就代表了一个蓝色的 Function</strong>，只是我们现在每一个蓝色的 Function，都用一个 Sigmoid Function 来逼近似它</li></ul><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607091908138.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>将三个式子简化为矩阵形式。得到 <span class="math inline">\(r=b+Wx\)</span></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607092914325.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p><span class="math inline">\(σ\)</span> 表示做 sigmoid</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607093111740.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607093717450.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>将这三个式子连起来</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607094028833.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>就得到了模型最后的式子</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607094453684.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>下面将重新定义一下参数，将未知参数排成一个向量（ <span class="math inline">\(W\)</span> 拿行和拿列是一样的）组成 <span class="math inline">\(\theta\)</span>（就是下文优化部分用的），还有两个b是不一样的。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607095355124.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><h3 id="back-to-ml_step2定义损失函数">back to ML_step2：定义损失函数</h3><p>有了新的这个 Model 以后，定义 Loss 的方法是一样的，只是符号改了一下，直接<strong>用 θ 来统设所有的参数</strong>，所以现在的 Loss Function 就变成 <span class="math inline">\(L(\theta)\)</span></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607095812607.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><h3 id="back-to-ml_step3优化参数">back to ML_step3：优化参数</h3><p>现在就是要<strong>找一组 θ,这个 θ 可以让 Loss 越小越好</strong>。</p><p>开始要随机选一个初始的数值 <span class="math inline">\(\theta_0\)</span>，然后对每一个未知的参数,都去计算它对 L 的微分，组成一个向量叫 gradient。算出 g 之后要 <strong>更新参数</strong>，通过迭代梯度下降更新参数。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607100406691.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>在实际运行中，不会一次运行所有数据，而会用<strong>batch</strong>：将整个数据分成很多batch，每次运行一个batch，update一次参数；将所有batch运行完一次，是为一个<strong>epoch</strong>。 batch size是需要设置的超参。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607102236585.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><h4 id="activation-function从-sigmoid-到-relu">Activation Function：从 Sigmoid 到 ReLU</h4><p>hard sigmoid 可以看作是两个ReLU 加起来，如图所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607102356512.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>所以 Sigmoid 也可以换成 ReLU：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607102434886.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>哪一个更好点？</p><p>实验结果显示 ReLU 的效果会好一些。</p><h4 id="深度神经网络">深度神经网络</h4><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607102524196.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>一个 sigmoid 或 ReLU 就是一个neuron，很多个 neuron 就组成了 neural network 神经网络。 neuron 层叫 layer，输入输出之外的层叫 hidden layer。有很多层就叫 Deep，这个技术就叫做 <strong>Deep Learning</strong>。</p><p>既然很多个 sigmoid 或 ReLU 就能拟合任意函数，那么展开一层极多的神经元不就行了吗，为什么选择把网络变深而不是变胖呢？这一问题留待以后解读。</p><p>在示例实验中，神经网络没有太深的原因是会<strong>过拟合</strong></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210607103014775.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/06/09/%E6%9D%8E%E5%AE%8F%E6%AF%85%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">李宏毅 深度学习基础</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/06/04/Numpy%20%E5%AD%A6%E4%B9%A0/"><span class="hidden-mobile">Numpy 学习</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>