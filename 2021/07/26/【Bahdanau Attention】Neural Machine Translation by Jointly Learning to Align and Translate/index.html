<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="【Bahdanau Attention】：Neural Machine Translation by Jointly Learning to Align and Translate
这是一篇2015年发表在 ICLR 上的论文，也是 NLP 中 Attention 机制的开山之作，Attention 机制是为了解决一般的 RNN Encoder-Decoder 对长句子表现不佳的问题而设计的。从"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>Bahdanau Attention：Neural Machine Translation by Jointly Learning to Align and Translate - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="Bahdanau Attention：Neural Machine Translation by Jointly Learning to Align and Translate"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-07-26 08:22" pubdate>2021年7月26日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.2k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 46 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">Bahdanau Attention：Neural Machine Translation by Jointly Learning to Align and Translate</h1><p class="note note-info">本文最后更新于：2021年8月10日</p><div class="markdown-body"><h2 id="bahdanau-attentionneural-machine-translation-by-jointly-learning-to-align-and-translate">【Bahdanau Attention】：Neural Machine Translation by Jointly Learning to Align and Translate</h2><p>这是一篇2015年发表在 ICLR 上的论文，也是 NLP 中 Attention 机制的开山之作，Attention 机制是为了解决一般的 RNN Encoder-Decoder 对长句子表现不佳的问题而设计的。从论文题目中可以看到，作者希望通过 Attention 机制将输入句子 input 和输出句子 output 进行“对齐”（SMT 中也有所谓的词对齐模型），当然，由于不同语言的句法语法结构千差万别，想将源句子与翻译句子严格的对齐是很困难的，所以这里的对齐实际上是<strong>软对齐（soft-alignment）</strong>，也就是不必将源句子显式分割，因而又被形象地称为<strong>注意力机制（Attention Mechanism）</strong>。</p><blockquote><p><strong>定义：对齐</strong></p><p>对齐是指将原文的片段与其对应的译文片段进行匹配。</p></blockquote><h3 id="background">Background</h3><p>传统的神经网络机器翻译模型属于 <em>编码器-解码器</em> 系列。编码器读取并编码源语句成一个固定长度的向量。然后解码器根据这个向量解码输出翻译。<strong>这种编码器-解码器方法的潜在问题是神经网络需要能将源语句的所有重要的信息压缩到一个固定长度向量中。</strong> 这可能使得神经网络难以处理长句子，编码器-解码器的性能会随着输入句子的长度增加而迅速下降，序列越长，信息丢失的就越厉害。</p><p>为了解决这个问题，这篇论文引入编码器-解码器模型的一种扩展，它<strong>联合学习对齐和翻译</strong>(对应了论文题目)。 提出的模型在翻译中每次生成单词时，<strong>它（软）搜索源语句中信息最相关的一组位置。 然后，模型基于与这些源位置相关联的上下文向量和先前产生的所有目标单词来预测新的目标单词。</strong>即，在翻译目标词的每一步，让 Decoder 自动抽取源句子中那些与目标词信息相关的部分，而忽略不相关的部分，<strong>这些部分的信息构成一个上下文向量（context vectors）<span class="math inline">\(c_i\)</span> 来取代传统 Encoder 中的语义表示向量 <span class="math inline">\(h_{T_x}\)</span>。</strong>也就是说，<strong>Encoder 将句子编码为一个向量序列（而不是一个向量），然后在预测翻译单词的每一步选择这些向量的子集作为注意力向量输入到Decoder中。</strong></p><blockquote><p>这是符合我们的直觉的，因为人类在翻译句子时，不会每时每刻都考虑整个句子的含义，而是在翻译特定片段时，会重点注意那个片段附近的上下文，而不会注意离我们正在翻译的片段较远的那些片段。</p></blockquote><h3 id="model-rnn-encoder-decoder">Model: RNN Encoder-Decoder</h3><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730143319577.png" srcset="/img/loading.gif" lazyload></p><p>一个流程更清晰的图：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730153601466.png" srcset="/img/loading.gif" lazyload></p><h4 id="encoder-bidirectional-rnn-for-annotating-sequence">Encoder: Bidirectional RNN for Annotating Sequence</h4><p>基本的 RNN Encoder 是将源句子压缩成一个固定维度的向量，那么期间每个词的隐层状态只压缩了前面的词。Bahdanau 等人使用双向 RNN（bidirecitonal RNN，简记为 BiRNN），即一个词的隐层状态不仅压缩了其前面的词的信息，还压缩了后面的词</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210726190413854.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>一个 BiRNN 包含一个前向 (forward) 和一个后向 (backward) RNN。前向 RNN 按照词序列 $ ( .x_{1}, x_{2}, , x_{T})$ 的顺序依次压缩源句子，并得到一个隐层状态 <span class="math inline">\(\left(\overrightarrow{h_{1}}, \overrightarrow{h_{2}}, \ldots, \overrightarrow{h_{T}}\right)\)</span> , 类似地，后向 RNN 按照 <span class="math inline">\(\left(x_{T}, x_{T-1}, \ldots, x_{1}\right)\)</span> 的顺序依次压缩源句子, 得到 <span class="math inline">\(\left(\overleftarrow{h_{1}}, \overleftarrow{h_{2}}, \ldots, \overleftarrow{h_{T}}\right)\)</span> 。最后对于词 <span class="math inline">\(x_{i}\)</span>, 它的隐层状态通过连接两个 RNN 的结果得到 , 即 <span class="math inline">\(h_{i}=\left[\overrightarrow{h_{i}^{T}} ; \overleftarrow{h_{i}^{T}}\right]^{T}\)</span>, 可以看到，<span class="math inline">\(h_{i}\)</span> 压缩了 前向 和 后向 的表示，并且更加关注于词 <span class="math inline">\(x_{i}\)</span> 周围的词，使得 RNN 能更好地表达当前的输入</p><h4 id="decoder-learning-to-align-and-translate">Decoder: Learning to Align and Translate</h4><p>在预测目标词时，生成对应单词的条件概率定义为： <span class="math display">\[ p\left(y_{i} \mid y_{1}, \cdots, y_{i-1}, \mathbf{x}\right)=g\left(y_{i-1}, z_{i}, c_{i}\right) \]</span> <span class="math inline">\(g\)</span> 一般是多层非线性神经网络（<strong>softmax+maxout</strong>），其中 <span class="math inline">\(z_i\)</span> 为 RNN 在 <span class="math inline">\(i\)</span> 时刻的隐层状态： <span class="math display">\[ z_i = f(z_{i-1}, y_{i-1}, c_i) \]</span> 可见，与简单的 Encoder-Decoder 结构不同的是，源语言端上下文向量表示由原来的 <span class="math inline">\(c\)</span> 变成了 <span class="math inline">\(c_i\)</span>，即针对每一个目标词 <span class="math inline">\(y_i\)</span>，都有一个特定的 <span class="math inline">\(c_i\)</span> 与之对应（也就是说如果固定 <span class="math inline">\(c_i=\overrightarrow{h_{T}}\)</span>，那么模型就是一个普通的 RNN Encoder-Decoder）</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210726192812405.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><blockquote><p>这个图不是很清晰，看 model 的图更容易理解。</p></blockquote><p>其中，<span class="math inline">\(h_j\)</span> 是 Encoder 源句子第 j 个词的隐状态，<span class="math inline">\(z_{i-1}\)</span> 是 Decoder 第 i 个 target word 的隐状态</p><p>上下文向量 <span class="math inline">\(c_{i}\)</span> 依赖于 Encoder 得到的 <span class="math inline">\(annotations \left(h_{1}, \ldots, h_{T}\right)\)</span> , 其中每个 <span class="math inline">\(h_{j}\)</span> 压缩了源语言端的词并且 &quot;关注于&quot; <span class="math inline">\(x_{j}\)</span> 周围的词。上下文向量 <span class="math inline">\(c_{i}\)</span> 可以通过各个 <span class="math inline">\(annotations\)</span> 的加权平均得到： <span class="math display">\[ c_{i}=\sum_{j=1}^{T_{x}} \alpha_{i j} h_{j} \]</span> 其中, 从 <span class="math inline">\(h_{j}\)</span> 到 <span class="math inline">\(c_{i}\)</span> 的权重 <span class="math inline">\(\alpha_{i j}\)</span> 为:</p><p><span class="math display">\[ \alpha_{i j} =\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T} \exp \left(e_{ik} \right)} \]</span></p><p><span class="math display">\[ e_{i j} = a\left(z_{i-1}, h_{j}\right) =v_{a}^{T} \tanh \left(W_{a} z_{i-1}+U_{a} h_{j}\right) \]</span></p><p>Decoder 中的 a 其实就是<strong>对齐模型/对齐函数(alignment score, Luong称为对齐函数)</strong>，一般定义为一个前馈神经网络而不是SMT中的隐含变量（Latent variable，如HMM、LDA），这使得<strong>损失函数的梯度可以反向传播</strong>，可以与 Encoder-Decoder 网络共同训练，而作者为了减少计算量，将其定义为了单层前馈神经网络（因为对于每个翻译对需要计算 <span class="math inline">\(T_x \times T_y\)</span> 个权重出来），这样，由于与 <span class="math inline">\(i\)</span> 无关，因此可以预先计算出来。</p><p>不同于传统的对齐模型：源句子端每个词明确对齐到目标句子端一个或多个词（hard alignment），而该方法计算得到得是一种 soft alignment，可以融入整个 NMT 框架，通过反向传播算法求梯度以及更新参数</p><p>上面的三个式子实际上就是 Attention机制 的全部精髓。<font color="red"><span class="math inline">\(e_{ij}\)</span> 用来衡量第 <span class="math inline">\(j\)</span> 个源句子词与目标句子第 <span class="math inline">\(i\)</span> 个词的匹配程度，其实就是在预测目标句子第 <span class="math inline">\(i\)</span> 个单词时，Attention机制 赋予源句子中第 <span class="math inline">\(j\)</span> 个单词的<strong>注意力大小</strong>，而注意力大小由源句子中的单词语义表示 <span class="math inline">\(h_j\)</span> 和 Decoder 的上一状态 <span class="math inline">\(z_{i-1}\)</span> 共同决定。然后，将源句子中所有词对第 <span class="math inline">\(i\)</span> 个预测词的注意力做一个<strong>softmax归一化</strong>处理得到和为1的权重 <span class="math inline">\(\alpha_{ij}\)</span>，将其用于对源句子的词表示向量进行加权求和，就得到了<strong>上下文注意力向量</strong> <span class="math inline">\(c_i\)</span>，其反映了源句子中对于当前预测单词而言最为重要的那部分信息。</font></p><h3 id="experiments">Experiments</h3><h4 id="定量分析">定量分析</h4><p>作者比较了原始的 RNN Encoder-Decoder 模型（RNNenc）和由 Attention 机制改进的模型（RNNsearch），让这两个模型分别在句子最大长度为 30 和 50 的训练集上训练，在测试集上翻译结果的 BLEU 分数与句子长度的关系如下图所示</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210726193814272.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>可以看到，RNNsearch 的机器翻译结果明显好于 RNNenc。RNNSearch-50 对长句子已经相当稳健，几乎不受长句子的影响，而对于 RNNsearch-30 来说，当句子长度超过 30 时，分数还是会出现大幅下滑</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210726230605129.png" srcset="/img/loading.gif" lazyload></p><p>可以看到，虽然Moses仍旧高于神经翻译模型，但 RNNsearch-50 已经和 Moses 很接近了，其中训练时间更久的 RNNsearch-50* 在无生词的情况下性能已经超越了Moses，而对比于 RNNenc，Attention机制 的引入使得传统的 Encoder-Decoder 模型的分数提高了8个点左右，说明 Attention机制 是相当有效的。</p><h4 id="定性分析">定性分析</h4><p>为了从直观上感受Attention机制，作者随机抽取了测试集中的几个句子，将他们的注意力权重采用热图的方式展示了出来：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210726200551755.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>通过上图我们可以清晰地看到<strong>在生成目标词时，源句中的哪些词被认为更重要</strong>。不难看出，输入句子（英语）与输出句子（法语）的词之间的关系在很大程度上是单调的，即<strong>权重矩阵近似于单位矩阵</strong>，而对于一些输入与输出语言存在的一些<strong>不对称的语法现象</strong>（如图1中英语和法语的形容词与名词的顺序不同），<strong>Attention机制 能够自动跳过一些单词，调节语法顺序，使得输入输出得到正确的软对齐。</strong></p><p>比起SMT中的硬对齐，软对齐更加灵活，不生硬，同时能<strong>自然地处理不同长度的输入和输出</strong>，而不是以一种反直觉的方式把一些词映射成NULL。这也是为什么 Attention机制 能够很好地处理长句子问题，传统的 RNNenc 在翻译长句子时，在翻译的后半段往往会漏掉许多细节，偏离句子的原意，而 RNNsearch 在翻译长句子的后半段时，不会遗漏那些重要的信息，因为这些信息是即时计算的，而不像 RNNenc 那样固定不变。</p><div class="note note-info"><p><strong>关于 soft attention 和 hard attention</strong></p><p>两者的区别在于<strong>Hard Attention 关注一个很小的区域，而soft Attention 关注的相对要发散。hard 只注重于当下时刻，聚焦于一个point，而soft则更加general。</strong></p><p>for example：</p><blockquote><p>我是小明 --&gt; I am XiaoMing</p></blockquote><p>对于 Hard Attention而言，在第1时刻翻译时，只关注“我”这个词，我们翻译得到“I”，在第2时刻翻译时，关注“是”这个词，翻译结果为“am”，以此直到 t 时刻结束。 它是采用one-hot编码的方式对位置进行标记，比如第1时刻，编号信息就是[1,0,0...]， 第二时刻，编码信息就是 [0, 1, 0, ...]， 以此类推。</p><p>这样会带来一个缺点：<strong>无法采用常规优化方法来进行优化</strong></p><p>而对于soft attention 而言，在第一时刻翻译时， “我是小明” 都对 “I” 做出了贡献，只不过贡献有大小之分，也就是说，虽然“我”这个词很重要，但是我们也不能放过其他词所带来的信息。</p><p><strong>finally：</strong></p><p>概括来说：</p><ol type="1"><li><p>Soft Attention Model（与 Luong 的 Global Attention 基本相同）</p><p>所谓 Soft，意思是在求注意力分配概率分布的时候，对于输入句子X中任意一个单词都给出个概率，是个概率分布。</p><ul><li><em>优点</em>：模型平滑且可微。</li><li><em>缺点</em>：当源输入很大时很昂贵。</li></ul></li><li><p>Hard Attention Model</p><p>既然 Soft 是给每个单词都赋予一个单词对齐概率，那么如果不这样做，直接从输入句子里面找到某个特定的单词，然后把目标句子单词和这个单词对齐，而其它输入句子中的单词硬性地认为对齐概率为0，这就是 Hard Attention Model 的思想。</p><ul><li><em>优点</em>：推理时计算量较少。</li><li><em>缺点</em>：模型是不可微的，需要更复杂的技术，如方差减少或强化学习来训练。( <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">Luong, et al., 2015</a> )</li></ul><p>Hard AM 在图像里证明有用，但是估计在文本里面用处不大，因为这种单词一一对齐明显要求太高，如果对不齐对后续处理负面影响很大，所以在NLP的文献里看不到用 Hard AM，估计大家都试过了，效果不好。</p></li></ol><p><strong>soft attention输出注意力分布的概率值，hard attention 输出 onehot 向量。</strong>比较二者而言，很显然，soft attention有很大的优势，因此，对于 NLP 领域而言，目前大多数的研究都基于 soft Attention 进行扩展。</p></div><h3 id="summary">Summary</h3><p>作者提出了 Attention机制 来解决一般的 RNN Encoder-Decoder 对长句子表现不好的问题。一般的 RNN Encoder-Decoder 将源语句的所有重要的信息压缩到一个固定长度的上下文向量 <span class="math inline">\(c\)</span> 中，当输入序列越长，因为压缩导致的信息丢失就越多。Attention机制 让输入和输出语句进行一种软对齐，在生成每个目标词时，（软）搜索源句子中所有词中最相关的一组位置(也就是计算注意力分数找最相关的)，该模型基于 与这些源位置相关联的上下文向量 <span class="math inline">\(c_i\)</span> 和 所有先前生成的目标词 来预测目标词。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/NLP/">NLP</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/NLP/">NLP</a> <a class="hover-with-bg" href="/tags/Attention/">Attention</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/07/27/%E3%80%90Luong%20Attention%E3%80%91%20Effective%20Approaches%20to%20Attention-based%20Neural%20Machine%20Translation/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Luong Attention：Effective Approaches to Attention-based Neural Machine Translation</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/07/11/Seq2Seq%E5%92%8CAttention/"><span class="hidden-mobile">Seq2Seq和Attention机制</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>