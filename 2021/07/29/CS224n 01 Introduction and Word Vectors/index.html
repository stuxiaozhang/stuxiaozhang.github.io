<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="CS224n 01 Introduction and Word Vectors
NLP——Natural Language Processing，其中Natural Language指的就是human language。Human language想一想真的很奇妙，仅仅用纸张上的抽象文字就能指代实际的物理事物以及其他抽象概念，而想让机器理解人类的语言是一项很复杂的任务，而且有很多有趣又实用的研究"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>CS224n 01 Introduction and Word Vectors - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="CS224n 01 Introduction and Word Vectors"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-07-29 22:34" pubdate>2021年7月29日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 5.6k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 49 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">CS224n 01 Introduction and Word Vectors</h1><p class="note note-info">本文最后更新于：2021年8月31日</p><div class="markdown-body"><h1 id="cs224n-01-introduction-and-word-vectors">CS224n 01 Introduction and Word Vectors</h1><p>NLP——Natural Language Processing，其中Natural Language指的就是human language。Human language想一想真的很奇妙，仅仅用纸张上的抽象文字就能指代实际的物理事物以及其他抽象概念，而想让机器理解人类的语言是一项很复杂的任务，而且有很多有趣又实用的研究方向如 Machine Translation，Semantic Analysis，Question Answering 等等。</p><p>其中一个基本的问题就是我们如何表示一个单词的含义(meaning of a word)。</p><h2 id="wordnet">WordNet</h2><p>著名的WordNet，它被称为是NLP中的瑞士军刀，下图展示了通过调取wordnet工具包查询一个词的相关信息：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210811124440299.png" srcset="/img/loading.gif" lazyload></p><p>WordNet 的构建花费了很多人多年时间，是对 NLP 领域伟大的贡献。但是我们现在实际上很少使用它了，因为它有这么几个缺陷：</p><ul><li>缺乏词汇之间的差别的刻画</li><li>不能计算精确的相似度</li><li>词汇永远不会完整，且难以更新</li></ul><h2 id="one-hot-vector">one-hot vector</h2><p><strong>Representing words as discrete symbols</strong></p><p>在传统的自然语言处理中，我们把词语看作离散的符号。单词可以通过<strong>独热向量(one-hot vectors，只有一个1，其余均为0的稀疏向量) 。向量维度=词汇量</strong>。 <span class="math display">\[ \begin{aligned} motel &amp;=\left[\begin{array}{llllllllllllll} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right] \\ hotel &amp;=\left[\begin{array}{llllllllllll} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right] \end{aligned} \]</span> <strong>Problem with words as discrete symbols</strong></p><ul><li><p>词汇太多，用one-hot表示<strong>太稀疏</strong>，向量维度过大</p></li><li><p>所有向量是正交的。由于任意两个不同词的独热向量之间的余弦相似度为0，所以独热向量不能编码词之间的相似性。对于独热向量，<strong>没有关于相似性概念</strong>。</p></li></ul><h2 id="word-embedding">Word Embedding</h2><p>之前的 one-hot vector 是一种 sparse vector，我们想要构建的是 dense vector，即大多数元素不为零且维度较小的向量，并且希望在相似的 context 下的 word vector 也较为相似。</p><p><strong>Representing words by their context</strong></p><ul><li><u>Distributional semantics</u>：<strong>一个单词的意思是由经常出现在它附近的单词给出的</strong><ul><li><em>“You shall know a word by the company it keeps”</em> (J. R. Firth 1957: 11)</li><li>现代统计NLP最成功的理念之一</li></ul></li><li>当一个单词 <span class="math inline">\(w\)</span> 出现在文本中时，它的上下文是出现在其附近的一组单词(在一个固定大小的窗口中)。</li><li>使用 <span class="math inline">\(w\)</span> 的许多上下文来构建 <span class="math inline">\(w\)</span> 的表示</li></ul><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210729225215683.png" srcset="/img/loading.gif" lazyload></p><p>我们为每个单词构建一个 <strong>密集</strong> 的向量，使其与出现在相似上下文中的单词向量相似。</p><p>词向量 <strong><em>word vectors</em></strong> 有时被称为词嵌入 <strong><em>word embeddings</em></strong> 或词表示 <strong><em>word representations</em></strong>。它们是分布式表示 <strong><em>distributed representation</em></strong>。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210729231215648.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h3 id="什么是-word-embedding">什么是 Word Embedding？</h3><p>要理解这个概念，先理解什么是 Emdedding？Embedding 在数学上表示一个 maping, f: X -&gt; Y， 也就是一个 function，其中该函数是 injective（就是我们所说的单射函数，每个 Y 只有唯一的 X 对应，反之亦然）和 structure-preserving (结构保存，比如在 X 所属的空间上X1 &lt; X2,那么映射后在 Y 所属空间上同理 Y1 &lt; Y2)。 <strong>那么对于 word embedding，就是将单词 word 映射到另外一个空间，其中这个映射具有 injective 和 structure-preserving 的特点。</strong> 通俗的翻译可以认为是单词嵌入，就是把 X 所属空间的单词映射为到 Y 空间的多维向量，那么该多维向量相当于嵌入到 Y 所属空间中，一个萝卜一个坑。word embedding，就是找到一个映射或者函数，生成在一个新的空间上的表达，该表达就是 word representation。</p><h3 id="有哪些类型的-word-embeddings">有哪些类型的 Word Embeddings？</h3><p>目前主要分为两类：</p><ul><li>Frequency based Embedding<ul><li>Count Vector</li><li>TF-IDF Vector</li><li>Co-Occurrence Vector 共现向量</li></ul></li><li>Prediction based Embedding(Word2vec)<ul><li>COBW（Continuous Bag of words）根据上下文来预测一个单词</li><li>Skip-gram 根据一个单词来预测上下文</li></ul></li></ul><p><strong>Frequency based Embedding 就是基于词频统计的映射方式，主要有以下三种：</strong></p><ul><li><p>Count Vector 这种就是最简单，最基本的词频统计算法：比如我们有N个文本（document），我们统计出所有文本中不同单词的数量，结果组成一个矩阵。那么每一列就是一个向量，表示这个单词在不同的文档中出现的次数。</p></li><li><p>TF-IDF Vector TF-IDF 方法基于前者的算法进行了一些改进，它的计算公式如下： <span class="math display">\[ {\mathrm {tfidf_{ {i,j} } } } ={\mathrm {tf_{ {i,j} } } } \times {\mathrm {idf_{ {i} } } } \]</span> 其中，<span class="math inline">\(tf_{i,j}\)</span>（term-frequence）指的是第 <span class="math inline">\(i\)</span> 个单词在第 <span class="math inline">\(j\)</span> 个文档中出现的频次；而 <span class="math inline">\(idf_i\)</span> (inverse document frequency)的计算公式如下： <span class="math display">\[ {\mathrm idf_{i} = \log(N/n)} \]</span> 其中，N 表示文档的总个数，n 表示包含该单词的文档的数量。这个公式是什么意思呢？其实就是一个权重，设想一下如果一个单词在各个文档里都出现过，那么 <span class="math inline">\(N/n=1\)</span>，所以 <span class="math inline">\(idf_i=0\)</span>。这就意味着这个单词并不重要。这个东西其实很简单，就是在 term-frequency 的基础上加了一个权重，从而显著降低一些不重要/无意义的单词的 frequency，比如 a,an,the 等。</p></li><li><p>Co-Occurrence Vector 共现向量 这个比较有意思，中文直译过来就是协同出现向量。在解释这个概念之前，我们先定义两个变量：</p><ul><li><p>Co-occurrence 协同出现指的是两个单词 <span class="math inline">\(w1\)</span> 和 <span class="math inline">\(w_2\)</span> 在一个 Context Window 范围内共同出现的次数</p></li><li><p>Context Window</p><p>指的是某个单词 <span class="math inline">\(w\)</span> 的上下文范围的大小，也就是前后多少个单词以内的才算是上下文？比如一个Context Window Size = 2的示意图如下：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210831141820473.png" srcset="/img/loading.gif" lazyload></p><p>比如我们有如下的语料库：<em>He is not lazy. He is intelligent. He is smart.</em>​</p><p>我们假设Context Window=2，那么我们就可以得到如下的co-occurrence matrix</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210831141913892.png" srcset="/img/loading.gif" lazyload></p><p>这个方法比之前两个都要进步一点，为什么呢？ <strong>因为它不再认为单词是独立的，而考虑了这个单词所在附近的上下文，这是一个很大的突破。</strong> 如果两个单词经常出现在同一个上下文中，那么很可能他们有相同的含义。比如vodka和brandy可能经常出现在wine的上下文里，那么在这两个单词相对于wine的co-occurrence就应该是相近的，于是我们就可以认为这两个单词的含义是相近的。</p></li></ul></li></ul><h2 id="word2vec">Word2Vec</h2><p><strong><em>Word2vec</em></strong> (Mikolov et al. 2013)是一个学习单词向量的 <strong>框架</strong></p><p><strong>IDEA</strong>：</p><ul><li>我们有大量的文本 (corpus means 'body' in Latin. 复数为corpora)</li><li>固定词汇表中的每个单词都由一个向量表示</li><li>文本中的每个位置 <span class="math inline">\(t\)</span>，其中有一个中心词 <span class="math inline">\(c\)</span> 和上下文(“外部”)单词 <span class="math inline">\(o\)</span></li><li>使用 <span class="math inline">\(c\)</span> 和 <span class="math inline">\(o\)</span> 的 <strong>词向量的相似性</strong> 来计算给定 <span class="math inline">\(c\)</span> 的 <span class="math inline">\(o\)</span> 的 <strong>概率</strong> (反之亦然)</li><li><strong>不断调整词向量</strong> 来最大化这个概率</li></ul><p>下图为窗口大小 <span class="math inline">\(j=2\)</span> 时的 <span class="math inline">\(P(w_{t+j}|w_t)\)</span> 计算过程，center word 分别为 <span class="math inline">\(into\)</span> 和 <span class="math inline">\(banking\)</span></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210729232018361.png" srcset="/img/loading.gif" lazyload></p><h3 id="word2vec-model">word2vec model</h3><p>2013 年，Google 团队发表了 word2vec 工具。word2vec 词向量可以较好地表达不同词之间的相似度和类比关系。word2vec 是一个软件包,实际上包含：</p><ul><li><strong>两个算法模型</strong>：<strong>continuous bag-of-words（CBOW）</strong>和 <strong>skip-gram</strong>。CBOW 是根据中心词周围的上下文单词来预测该词的词向量。skip-gram 则相反，是根据中心词预测周围上下文的词的概率分布。</li><li><strong>两个训练方法</strong>：<strong>负采样（negative sampling）</strong>和<strong>层序 softmax（hierarchical softmax）</strong>。Negative sampling 通过抽取负样本来定义目标，hierarchical softmax 通过使用一个有效的树结构来计算所有词的概率来定义目标。</li></ul><p>对于在语义上有意义的表示，它们的训练依赖于条件概率，条件概率可以被看作是使用语料库中一些词来预测另一些单词。由于是不带标签的数据，因此跳字模型和连续词袋模型都是<strong>自监督模型</strong>。</p><h4 id="skip-gram-跳字模型">skip-gram 跳字模型</h4><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210811211150440.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>在<strong>跳字模型中，我们用一个词来预测它在文本序列周围的词。</strong>例如，给定文本序列 &quot;the&quot;,&quot;man&quot;,&quot;loves&quot;,&quot;his&quot;,&quot;son&quot;。设背景窗口大小为 2， 跳字模型所关心的是，给定 &quot;loves&quot;，生成它邻近词 &quot;the&quot;,&quot;man&quot;.&quot;his&quot;,&quot;son&quot; 的概率（在这个例子中，&quot;loves&quot; 叫中心词，&quot;the&quot;,&quot;man&quot;,&quot;his&quot;,&quot;son&quot; 叫背景词），即 <span class="math display">\[ P(\textrm{&quot;the&quot;},\textrm{&quot;man&quot;},\textrm{&quot;his&quot;},\textrm{&quot;son&quot;}\mid\textrm{&quot;loves&quot;}). \]</span> 假设在给定中心词的情况下，上下文词的生成是相互独立的，那么上式可以改写成 <span class="math display">\[ P(\textrm{&quot;the&quot;}\mid\textrm{&quot;loves&quot;})\cdot P(\textrm{&quot;man&quot;}\mid\textrm{&quot;loves&quot;})\cdot P(\textrm{&quot;his&quot;}\mid\textrm{&quot;loves&quot;})\cdot P(\textrm{&quot;son&quot;}\mid\textrm{&quot;loves&quot;}). \]</span> 对于每个位置 <span class="math inline">\(t=1,…,T\)</span> ，在大小为 <span class="math inline">\(m\)</span> 的固定窗口内预测上下文单词，给定中心词 <span class="math inline">\(w_j\)</span></p><p><span class="math display">\[ \text { Likelihood }=L(\theta)=\prod_{t=1}^{T} \prod_{-m \leq j \leq m} P\left(w_{t+j} \mid w_{t} ; \theta\right) \]</span></p><ul><li>其中，<span class="math inline">\(θ\)</span> 为所有需要优化的变量</li></ul><p><strong>跳字模型参数是词表中每个词的中心词向量和上下文词向量。</strong>在训练中，我们通过最大化似然函数(即最大似然估计)来学习模型参数。这相当于最小化以下损失函数。损失函数 <span class="math inline">\(J(\theta)\)</span> 是(平均)负对数似然函数： <span class="math display">\[ J(\theta)=-\frac{1}{T} \log L(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop j \neq 0} \log P\left(w_{t+j} | w_{t} ; \theta\right) \]</span> 其中 <span class="math inline">\(log\)</span> 形式是方便将连乘转化为求和，负号是希望将 极大化似然率 转化为 极小化损失函数 的等价问题。</p><blockquote><p>在连乘之前使用 <span class="math inline">\(log\)</span> 转化为求和非常有效，特别是在做优化时 <span class="math display">\[ \log \prod_{i} x_{i}=\sum_{i} \log x_{i} \]</span></p></blockquote><p>我们的目的是 <strong>最小化目标函数 ⇔ 最大化预测精度</strong>，那么如何计算 <span class="math inline">\(P(w_{t+j}|w_t;θ)\)</span> ？</p><p><strong>答</strong>：对于每个单词都是用两个向量</p><ul><li><span class="math inline">\(\boldsymbol v\)</span> 代表中心词的词向量</li><li><span class="math inline">\(\boldsymbol u\)</span> 代表上下文词的词向量</li></ul><p>换言之，对于词典中一个索引为 <span class="math inline">\(i\)</span> 的词，它本身有两个向量 <span class="math inline">\(\boldsymbol v_i\)</span> 和 <span class="math inline">\(\boldsymbol u_i\)</span> 进行表示，在计算的过程中，根据其所处的角色不同，选择不同的词向量。<strong>词典中所有词的这两种向量正是跳字模型所需要学习的参数</strong>。为了将模型参数植入损失函数，我们需要使用模型参数表达损失函数中的中心词生成上下文单词的概率。假设中心词的概率是相互独立的。给定<u>中心词 <span class="math inline">\(w_c\)</span></u> 在词典中的索引为 <span class="math inline">\(c\)</span>，<u>上下文单词 <span class="math inline">\(w_o\)</span></u> 在词典中的索引为 <span class="math inline">\(o\)</span>，损失函数中，中心词生成上下文单词的概率可以使用 softmax 函数进行定义： <span class="math display">\[ P(w_o|w_c)=\frac{\exp(\boldsymbol{u}_o^T\boldsymbol {v}_c)}{\sum_{i\in V}\exp(\boldsymbol{u}_i^T\boldsymbol{v}_c)} \]</span></p><p>公式中，向量 <span class="math inline">\(\boldsymbol u_o\)</span> 和向量 <span class="math inline">\(\boldsymbol v_c\)</span> 进行点乘。<strong>用点积比较 <span class="math inline">\(w_o\)</span> 和 <span class="math inline">\(w_c\)</span> 的相似性</strong>，<span class="math inline">\(u^{T} v=u \cdot v=\sum_{i=1}^{n} u_{i} v_{i}\)</span>，向量之间越相似，点积越大，从而归一化后得到的概率值也越大。模型的训练正是为了使得具有相似上下文的单词，具有相似的向量。分母是对整个词汇表进行标准化，从而给出概率分布。</p><p>当序列长度 <span class="math inline">\(T\)</span> 较大时，我们通常随机采样一个较小的子序列来计算损失函数并使用 SGD 优化该损失函数。通过求导，我们可以计算出上式生成概率的对数关于中心词向量 <span class="math inline">\(\boldsymbol {v}_c\)</span> 的梯度为： <span class="math display">\[ \begin{aligned} \frac{\partial \log P(w_o\mid w_c)}{\partial \boldsymbol{v}_c}&amp;=\frac{\partial}{\partial \boldsymbol{v}_c}\log\frac{\exp(\boldsymbol{u}_o^T\boldsymbol{v}_c)}{\sum_{i=1}^{|V|}\exp(\boldsymbol{u}_i^T\boldsymbol{v}_c)}\\&amp;=\underbrace {\frac{\partial}{\partial \boldsymbol{v}_c} \log \exp(\boldsymbol{u}_o^T\boldsymbol{v}_c)}_1-\underbrace{\frac{\partial}{\partial \boldsymbol{v}_c}\log \sum_{i=1}^{|V|}\exp(\boldsymbol{u}_i^T{\boldsymbol{v}_c})}_2 \end{aligned} \]</span> 第一部分推导 <span class="math display">\[ \frac { \partial} {\partial \boldsymbol{v}_c} \color{red}{\log \exp (\boldsymbol{u}_o^T \boldsymbol{v}_c) } = \frac { \partial} {\partial \boldsymbol{v}_c} \color{red}{\boldsymbol{u}_o^T \boldsymbol{v}_c} = \mathbf{\boldsymbol{u}_o} \]</span> 第二部分推导 <span class="math display">\[ \begin{aligned} \frac { \partial} {\partial \boldsymbol{v}_c} \log \sum_{i=1}^{|V|} \exp(\boldsymbol{u}_i^T \boldsymbol{v}_c) &amp; = \frac{1}{\sum_{i=1}^{|V|} \exp(\boldsymbol{u}_i^T \boldsymbol{v}_c)} \cdot \color{red}{ \frac { \partial} {\partial \boldsymbol{v}_c} \sum_{x=1}^{|V|} \exp(\boldsymbol{u}_x^T \boldsymbol{v}_c)} \\ &amp; = \frac{1}{....} \cdot \sum_{x=1}^{|V|} \color{red} {\frac { \partial} {\partial \boldsymbol{v}_c} \exp(\boldsymbol{u}_x^T \boldsymbol{v}_c)} \\ &amp; = \frac{1}{....} \cdot \sum_{x=1}^{|V|} \exp (\boldsymbol{u}_x^T \boldsymbol{v}_c) \color{red} {\frac { \partial} {\partial \boldsymbol{v}_c} \boldsymbol{u}_x^T \boldsymbol{v}_c} \\ &amp; = \frac{1}{\sum_{i=1}^{|V|} \exp(\boldsymbol{u}_i^T \boldsymbol{v}_c)} \sum_{x=1}^{|V|} \exp (\boldsymbol{u}_x^T \boldsymbol{v}_c) \color{red} {\boldsymbol{u}_x} \\ &amp; = \sum_{x=1}^{|V|} \color{red} { \frac{\exp (\boldsymbol{u}_x^T \boldsymbol{v}_c)} {\sum_{i=1}^{|V|} \exp(\boldsymbol{u}_i^T \boldsymbol{v}_c)}} \boldsymbol{u}_x \\ &amp; = \sum_{x=1}^{|V|} \color{red} {P(w_x \mid w_c) }\boldsymbol{u}_x \end{aligned} \]</span></p><blockquote><p>公式中，这里的 log 应该是 以 e为底（即 ln），所以 (log x)’ = <span class="math inline">\(\frac 1 x\)</span></p><p>偏导数可以移进求和中(其中，<span class="math inline">\(y_i\)</span> 是有关 <span class="math inline">\(x\)</span> 的函数)： <span class="math display">\[ \frac{\partial}{\partial x}\sum_iy_i = \sum_i\frac{\partial}{\partial x}y_i \]</span></p></blockquote><p>综上所述 <span class="math display">\[ \frac{\partial \log P(w_o\mid w_c)}{\partial \boldsymbol{v}_c}=\boldsymbol{u}_o-\sum_{j\in V}P(w_j\mid w_c)\boldsymbol {u}_j \]</span> 通过上面计算得到梯度后，我们可以使用随机梯度下降来不断迭代模型参数 <span class="math inline">\(\boldsymbol {v}_c\)</span>。另一个模型参数 <span class="math inline">\(\boldsymbol {u}_o\)</span> 的迭代方式同理可得： <span class="math display">\[ \begin{align} \frac{\partial \log P(w_o\mid w_c)}{\partial \boldsymbol{u}_o} &amp;=\frac{\partial}{\partial \boldsymbol{u}_o}\log\frac{\exp(\boldsymbol{u}_o^T\boldsymbol{v}_c)}{\sum_{i=1}^{|V|}\exp(\boldsymbol{u}_i^T\boldsymbol{v}_c)}\\ &amp;=\frac{\partial}{\partial \boldsymbol{u}_o}\left(\log \exp(\boldsymbol{u}_o^T\boldsymbol{v}_c)-\log\sum_{i=1}^{|V|}\exp(\boldsymbol{u}_i^T{\boldsymbol{v}_c})\right)\\ &amp;=\frac{\partial}{\partial \boldsymbol{u}_o} (\boldsymbol{u}_o^T\boldsymbol{v}_c)- \frac{\partial}{\partial \boldsymbol{u}_o} \log \sum_{i=1}^{|V|} \exp \left(\boldsymbol{u}_{i}^{T} \boldsymbol{v}_{c}\right) \\ &amp;=\boldsymbol{v}_c- \frac{\partial}{\partial \boldsymbol{u}_{o}} \log \sum_{i=1}^{|V|} \exp \left(\boldsymbol{u}_{i}^{T} \boldsymbol{v}_{c}\right) \\ &amp;=\boldsymbol{v}_c-\frac{\sum_{i=1}^{|V|} \frac{\partial}{\partial \boldsymbol{u}_{o}}\exp(\boldsymbol{u}_{i}^{T} \boldsymbol{v}_{c})}{\sum_{i=1}^{|V|} \exp \left(\boldsymbol{u}_{i}^{T} \boldsymbol{v}_{c}\right)}\\ &amp;=\boldsymbol{v}_c - \frac{\exp(\boldsymbol{u}_o^T\boldsymbol{v}_c)\boldsymbol{v}_c}{\sum_{i=1}^{|V|} \exp \left(\boldsymbol{u}_{i}^{T} \boldsymbol{v}_{c}\right)}\\ &amp;=\boldsymbol{v}_c - P(w_o|w_c)\boldsymbol{v}_c\\ &amp;=(1-P(w_o|w_c))\boldsymbol{v}_c \end{align} \]</span> 最终，对于词典中任一索引为 <span class="math inline">\(i\)</span> 的词，我们均得到该词作为中心词和上下文词的两组词向量 <span class="math inline">\(\boldsymbol {v}_i\)</span> 和 <span class="math inline">\(\boldsymbol {u}_i\)</span></p><h4 id="cbow-连续词袋模型">CBOW 连续词袋模型</h4><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210811170513754.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>连续词袋模型（Continuous Bag of Words）与跳字模型类似。与跳字模型最大的不同是，连续词袋模型是用一个中心词在文本序列<strong>周围的词 来预测中心词</strong>。例如，给定文本 &quot;the&quot;,&quot;man&quot;,&quot;hit&quot;,&quot;the&quot;,&quot;ball&quot;，连续词袋模型所关心的是，邻近词 &quot;the&quot;,&quot;man&quot;,&quot;the&quot;,&quot;ball&quot; 一起生成中心词 &quot;hit&quot; 的概率</p><p>连续词袋模型需要最大化由背景词生成任一中心词的概率： <span class="math display">\[ \prod_{t=1}^TP(w^{(t)}\mid w^{(t-m)},...,w^{(t-1)},w^{(t+1)},...,w^{(t+m)}) \]</span> 上式得最大似然估计与最小化以下损失函数等价 <span class="math display">\[ -\sum_{t=1}^T\log P(w^{(t)}\mid w^{(t-m)},...,w^{(t-1)},w^{(t+1)},...,w^{(t+m)}) \]</span> 我们可以用 <span class="math inline">\(\boldsymbol {v}\)</span> 和 <span class="math inline">\(\boldsymbol {u}\)</span> 分别代表上下文词和中心词的向量（注意符号和跳字模型刚好相反）。给定中心词 <span class="math inline">\(w_c\)</span> 在词典中的索引为 <span class="math inline">\(c\)</span>，背景词 <span class="math inline">\(w_{o_1},...,w_{o_{2m}}\)</span> 在词典中的索引为 <span class="math inline">\(o_1,...,o_{2m}\)</span></p><p>需要对上述的上下文向量求平均值：<font color="red">(why?)</font> <span class="math display">\[ \widehat{\boldsymbol{v}}=\frac{\boldsymbol{v}_{o_1}+...+\boldsymbol{v}_{o_{2m}}}{2m} \]</span> 损失函数中的上下文词生成中心词的概率可以使用 softmax 函数定义为 <span class="math display">\[ P(w_c\mid w_{o_1},...,w_{o_{2m}})=\frac{\exp[\boldsymbol{u}_c^T(\boldsymbol{v}_{o_1}+...+\boldsymbol{v}_{o_{2m}})/(2m)]}{\sum_{j\in V}\exp[\boldsymbol{u}_j^T(\boldsymbol{v}_{o_1}+...+\boldsymbol{v}_{o_{2m}})/(2m)]} \]</span> 同样，当序列长度 T 较大时，我们通常随机采样一个较小的子序列来计算损失函数，并使用随机梯度下降优化该损失函数，通过微分，我们可以计算出上式生成概率的对数关于任一背景词向量 <span class="math inline">\(\boldsymbol {v}_{o_i}(i=1,...,2m)\)</span> 的梯度为： <span class="math display">\[ \frac{\partial \log P(w_c\mid w_{o_1},...,w_{o_{2m}})}{\partial \boldsymbol{v}_{o_i}}=\frac{1}{2m}(\boldsymbol {u}_c-\sum_{j\in V}\frac{\exp(\boldsymbol u_j^T\boldsymbol v_c)}{\sum_{i\in V}\exp(\boldsymbol u_i^T\boldsymbol v_c)}\boldsymbol u_j) \]</span> 而上式与下式等价： <span class="math display">\[ \frac{\partial \log P(w_c\mid w_{o_1},...,w_{o_{2m}})}{\partial \boldsymbol{v}_{o_i}}=\frac{1}{2m}(\boldsymbol {u}_c-\sum_{j\in V}P(w_j\mid w_c)\boldsymbol u_j) \]</span></p><h3 id="近似训练法">近似训练法</h3><p>可以看到，无论是跳字模型还是连续词袋模型，每一步梯度计算的开销与词典 V 的大小呈正相关。显然，当词典较大时，这种训练方法的计算开销会很大。所以使用上述训练方法在实际中是由难度的。我们可以使用近似的方法来计算这些梯度，从而减小计算开销。常用的近似训练法包括<strong>负采样</strong>和<strong>层序 softmax</strong></p><h4 id="负采样-negative-sampling">负采样 negative sampling</h4><p>以跳字模型为例讨论负采样。词典 <span class="math inline">\(V\)</span> 的大小之所以会在目标函数中出现，是因为中心词 <span class="math inline">\(w_c\)</span> 生成上下文词 <span class="math inline">\(w_o\)</span> 的概率 <span class="math inline">\(P(w_o|w_c)\)</span> 使用了 softmax，而 softmax 考虑到了上下文词可能是词典中任一词，并体现在了 softmax 的分母上。</p><p>不妨换个角度，假设中心词 <span class="math inline">\(w_c\)</span> 生成上下文词 <span class="math inline">\(w_o\)</span> 由以下两个互相独立的联合事件组成来近似：</p><ol type="1"><li>中心词 <span class="math inline">\(w_c\)</span> 和上下文词 <span class="math inline">\(w_o\)</span> 同时出现在该训练数据窗口</li><li>中心词 <span class="math inline">\(w_c\)</span> 和噪声词不同时出现在该训练数据窗口<ul><li>中心词 <span class="math inline">\(w_c\)</span> 和第 1 个噪声词 <span class="math inline">\(w_1\)</span> 不同时出现在训练数据窗口（噪声词 <span class="math inline">\(w_k\)</span> 按噪声词分布 <span class="math inline">\(P(w)\)</span> 随机生成）</li><li>...</li><li>中心词 <span class="math inline">\(w_c\)</span> 和第 K 个噪声词 <span class="math inline">\(w_k\)</span> 不同时出现在训练数据窗口（噪声词 <span class="math inline">\(w_k\)</span> 按噪声词分布 <span class="math inline">\(P(w)\)</span> 随机生成）</li></ul></li></ol><p>我们可以使用 <span class="math inline">\(\sigma (x)=\frac {1}{1+\exp (-x)}\)</span> 函数来表达中心词 <span class="math inline">\(w_c\)</span> 和上下文词 <span class="math inline">\(w_o\)</span> 同时出现在训练数据窗口的概率：</p><p><span class="math display">\[ P(D=1\mid w_o,w_c)=\sigma(\boldsymbol{u}_o^T,\boldsymbol{v}_c) \]</span> 那么，中心词 <span class="math inline">\(w_c\)</span> 生成上下文词 <span class="math inline">\(w_o\)</span> 的对数概率可以近似为： <span class="math display">\[ \log P(w_o\mid w_c)=\log[P(D=1\mid w_o,w_c)\prod_{k=1,w_k\sim P(w)}^KP(D=0\mid w_k,w_c)] \]</span> 假设噪声词 <span class="math inline">\(w_k\)</span> 在词典中的索引为 <span class="math inline">\(i_k\)</span>，上式可改写为： <span class="math display">\[ \log P(w_o\mid w_c)=\log\frac{1}{1+\exp(-\boldsymbol u_o^T\boldsymbol{v}_c)}+\sum_{k=1,w_k\sim P(w)}^K\log[1-\frac{1}{1+\exp(-\boldsymbol u_{i_k}^T\boldsymbol{v}_c)}] \]</span> 因此，有关中心词 <span class="math inline">\(w_c\)</span> 生成上下文词 <span class="math inline">\(w_o\)</span> 的损失函数是： <span class="math display">\[ -\log P(w_o\mid w_c)=-\log\frac{1}{1+\exp(-\boldsymbol u_o^T\boldsymbol{v}_c)}-\sum_{k=1,w_k\sim P(w)}^K\log\frac{1}{1+\exp(\boldsymbol u_{i_k}^T\boldsymbol{v}_c)} \]</span> 现在，训练中每一步的梯度计算开销不再与词典大小相关，而与 <span class="math inline">\(K\)</span> 线性相关。当 <span class="math inline">\(K\)</span> 取较小的常数时，负采样的每一步梯度计算开销也较小</p><p>同理，也可以对连续词袋模型进行负采样。有关上下文词 <span class="math inline">\(w^{(t-m)},...,w^{(t-1)},w^{(t+1)},...,w^{(t+m)}\)</span> 生成中心词 <span class="math inline">\(w_c\)</span> 的损失函数 <span class="math display">\[ -\log P(w^{(t)}\mid w^{(t-m)},...,w^{(t-1)},w^{(t+1)},...,w^{(t+m)}) \]</span> 在负采样中可以近似为 <span class="math display">\[ -\log\frac{1}{1+\exp[-\boldsymbol{u}_c^T(\boldsymbol{v}_{o_1}+...+\boldsymbol{v}_{o_{2m}})/(2m)]}-\sum_{k=1,w_k\sim P(w)}^K\log\frac{1}{1+\exp[\boldsymbol{u}_{i_k}^T(\boldsymbol{v}_{o_1}+...+\boldsymbol{v}_{o_{2m}})/(2m)]} \]</span></p><h4 id="层序-softmax">层序 softmax</h4><p>层序 softmax 利用了二叉树(哈夫曼树)。树的每个叶子节点代表着词典 <span class="math inline">\(V\)</span> 中的每个词。每个词 <span class="math inline">\(w_i\)</span> 对应的词向量为 <span class="math inline">\(v_i\)</span>。我们以下图为例，来描述层序 softmax 的工作机制</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210812222709840.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><p>设 <span class="math inline">\(L(w)\)</span> 为从二叉树根节点到代表词 <span class="math inline">\(w\)</span> 的叶子节点的路径上的节点数，并设 $n(w,i) $ 为该路径上第 i 个节点，该节点的向量为 <span class="math inline">\(u_{n(w,j)}\)</span>。以上图为例，<span class="math inline">\(L(w_3)=4\)</span>。那么，跳字模型和连续词袋模型所需要计算的任意词 <span class="math inline">\(w_i\)</span> 生成词 <span class="math inline">\(w\)</span> 的概率为：</p><p><span class="math display">\[ P(w\mid w_i)=\prod_{j=1}^{L(w)-1}\sigma([n(w,j+1)=left\_child(n(w,j))]·\boldsymbol{u}_{n(w,j)}^T\boldsymbol{v}_i) \]</span> 其中，如果 <span class="math inline">\(x\)</span> 为真，<span class="math inline">\([x]=1\)</span>；反之 <span class="math inline">\([x]=-1\)</span></p><p>由于 <span class="math inline">\(\sigma (x)+\sigma (-x)=1\)</span>，<span class="math inline">\(w_i\)</span> 生成词典中任何词的概率之和为 1： <span class="math display">\[ \sum_{w=1}^VP(w\mid w_i)=1 \]</span> 举个具体的例子，计算 <span class="math inline">\(w_i\)</span> 生成 <span class="math inline">\(w_3\)</span> 的概率，由于在二叉树中由根到 <span class="math inline">\(w_3\)</span> 的路径需要向左、向右、再向左地遍历，所以得到 <span class="math display">\[ P(w_3\mid w_i)=\sigma(\boldsymbol{u}_{n(w_3,1)}^T\boldsymbol{v}_i)·\sigma(-\boldsymbol{u}_{n(w_3,2)}^T\boldsymbol{v}_i)·\sigma(\boldsymbol{u}_{n(w_3,3)}^T\boldsymbol{v}_i)P(w_3\mid w_i)=\sigma(\boldsymbol{u}_{n(w_3,1)}^T\boldsymbol{v}_i)·\sigma(-\boldsymbol{u}_{n(w_3,2)}^T\boldsymbol{v}_i)·\sigma(\boldsymbol{u}_{n(w_3,3)}^T\boldsymbol{v}_i) \]</span> 由此，我们就可以使用随机梯度下降在跳字模型和连续词袋模型中不断迭代计算词典中所有词向量 <span class="math inline">\(v\)</span> 和非叶子节点的向量 <span class="math inline">\(u\)</span>。每次迭代的计算开销由 <span class="math inline">\(O(|V|)\)</span> 降为二叉树的高度 <span class="math inline">\(O(log⁡|V|)\)</span></p><p>层序 softmax 的二叉树是如何建立的？</p><ul><li>这里的二叉树 Huffman 树，权重是语料库中 word 出现的频率。通过词频大小来建立哈夫曼树。</li></ul><div class="note note-primary"><p>这块还是不太懂....留个坑吧..</p></div></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/CS224n/">CS224n</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/NLP/">NLP</a> <a class="hover-with-bg" href="/tags/Word2vec/">Word2vec</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/08/01/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">知识图谱构建技术综述</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/07/27/Luong%20Attention%EF%BC%9AEffective%20Approaches%20to%20Attention-based%20Neural%20Machine%20Translation/"><span class="hidden-mobile">Luong Attention：Effective Approaches to Attention-based Neural Machine Translation</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>