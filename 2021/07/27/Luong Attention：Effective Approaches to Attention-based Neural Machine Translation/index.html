<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="【Luong Attention】：Effective Approaches to Attention-based Neural Machine Translation
2014年 Bahdanau 首次提出 Attention机制 后，在短短一年的时间内就在 NMT 中得到了非常广泛的应用。Luong 等人 [Luong et al.2015a] 提出了在 NMT 中引入 Attention "><meta name="author" content="小张同学"><meta name="keywords" content=""><title>Luong Attention：Effective Approaches to Attention-based Neural Machine Translation - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="Luong Attention：Effective Approaches to Attention-based Neural Machine Translation"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-07-27 07:38" pubdate>2021年7月27日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 31 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">Luong Attention：Effective Approaches to Attention-based Neural Machine Translation</h1><p class="note note-info">本文最后更新于：2021年8月10日</p><div class="markdown-body"><h2 id="luong-attentioneffective-approaches-to-attention-based-neural-machine-translation">【Luong Attention】：Effective Approaches to Attention-based Neural Machine Translation</h2><p>2014年 Bahdanau 首次提出 Attention机制 后，在短短一年的时间内就在 NMT 中得到了非常广泛的应用。Luong 等人 [Luong et al.2015a] 提出了在 NMT 中引入 Attention 机制两种简单且有效的实现方式：Global Attention 和 Local Attention。<strong>Global Attention 关注源句子的所有单词，而 Local Attention 只关注每个目标词对应的源句子的一个子集。</strong>Global Attention 和 Bahdanau 提出的 Attention机制 相似，但结构上更加简单，而 Local Attention 可以看作是 Hard Attention 和 Soft Attention 的结合，它在计算复杂度上比 Soft Attention 要小，并且比起 Hard Attention，Local Attention 是几乎处处可微的，因此易于实现和训练。另外，作者还提出了三种不同的<strong>对齐函数（Alignment Functions）：Dot、General、Concat</strong> 来计算 Attention。</p><h3 id="attention-based-models">Attention-based Models</h3><p>在作者的模型中，<strong>编解码器均采用层叠 LSTMs</strong>，最上层的状态作为编码的隐层表示，而 Bahdanau 的模型是单层双向编码器 和 单层单向解码器，具体的细节请看==这篇==【Attention】: Neural Machine Translation by Jointly Learning to Align and Translate。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210727142430977.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>作者使用的基于注意力的编码器解码器如上图所示。关于注意力机制，作者提出了两种：Global Attention 和 Local Attention。首先，这两种注意力机制的不同之处在于<strong>输出上下文向量 <span class="math inline">\(c_t\)</span> 的计算方法不同</strong>，其他的比如 <span class="math inline">\(h_t\)</span> 和 <span class="math inline">\(y_t\)</span> 的计算都是相同的： <span class="math display">\[ \tilde{\boldsymbol{h}}_{t}=\tanh \left(\boldsymbol{W}_{c}\left[\boldsymbol{c}_{t} ; \boldsymbol{h}_{t}\right]\right) \\ p\left(y_{t} \mid y_{&lt;t}, x\right)=\operatorname{softmax}\left(\boldsymbol{W}_{s} \tilde{\boldsymbol{h}}_{t}\right) \]</span></p><blockquote><p>针对 Bahdanau ‘s Attention 的公式：</p><p><span class="math inline">\(h_t\)</span> 就是 Decoder 上一个隐状态 <span class="math inline">\(z_{i-1}\)</span>, <span class="math inline">\(\tilde h_t\)</span> 就是 Decoder 当前隐状态 <span class="math inline">\(z_i\)</span></p></blockquote><p>在这里，作者引入了一个新的注意力层来生成单词，接下来介绍两种注意力机制如何计算 <span class="math inline">\(c_i\)</span>.</p><h3 id="global-attention">Global Attention</h3><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210727091818341.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p><strong>Global Attention 的思想是在计算上下文向量 <span class="math inline">\(c_i\)</span> 时考虑编码器所有的隐藏状态。在每个时间步 <span class="math inline">\(t\)</span>，模型根据当前目标状态 <span class="math inline">\(h_t\)</span> 和所有源状态 <span class="math inline">\(\bar h_s\)</span> 推断对齐权重向量 <span class="math inline">\(a_t\)</span>。然后，根据 <span class="math inline">\(a_t\)</span>，将全局上下文向量 <span class="math inline">\(c_t\)</span> 计算为所有源状态的加权平均值。</strong></p><p>首先根据 目标词状态 <span class="math inline">\(h_t\)</span> 和 源句子所有词状态 <span class="math inline">\(\bar h_s\)</span> 计算对齐向量 <span class="math inline">\(a_t\)</span>: <span class="math display">\[ \begin{aligned} \boldsymbol{a}_{t}(s) &amp;=\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \\ &amp;=\frac{\exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)\right)}{\sum_{s^{\prime}} \exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s^{\prime}}\right)\right)} \end{aligned} \]</span> <strong>score 其实就是计算当前隐层输出与源隐层输出的相似的程度</strong>，作者给出了 score 的三种 <strong>content-based function</strong>： <span class="math display">\[ \operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)=\left\{\begin{array}{l} \boldsymbol{h}_{t}^{\top} \overline{\boldsymbol{h}}_{s}, \quad \text{ dot } \\ \boldsymbol{h}_{t}^{\top} \boldsymbol{W}_{\boldsymbol{a}} \overline{\boldsymbol{h}}_{s}, \quad \text { general } \\ \boldsymbol{v}_{a}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{a}}\left[\boldsymbol{h}_{t} ; \overline{\boldsymbol{h}}_{s}\right]\right), \quad \text { concat } \end{array}\right. \]</span></p><blockquote><p>另外，为了对比，作者还给出了一个比较早期的，只和位置相关的 <strong>location-based function</strong>： <span class="math display">\[ \boldsymbol{a}_{t}=\operatorname{softmax}\left(\boldsymbol{W}_{\boldsymbol{a}} \boldsymbol{h}_{t}\right), \quad \text { location } \]</span></p></blockquote><div class="note note-primary"><p>这里我觉得因为 score 函数包括了 <span class="math inline">\(\bar h_s\)</span> ，所以叫基于内容的(content-based). 早期给出的基于位置的(location-based)就只针对 target word，而不包括源句子，所以后面实验可以看到效果不好。</p></div><p>得到对齐向量后，用其对 source hidden states 加权平均即可得到上下文向量 <span class="math inline">\(c_t\)</span>。</p><blockquote><p>这里很多博客说的都是加权求和，但是原论文里写的是加权平均，如果以后有时间看一下代码实现就知道了！</p><p>“Given the alignment vector as weights, the context vector <span class="math inline">\(c_t\)</span> is computed as the <u>weighted average over all the source hidden states</u>”</p></blockquote><p>一个流程更清晰的 model 如下图所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/640%20(2).jpg" srcset="/img/loading.gif" lazyload></p><h3 id="local-attention">Local Attention</h3><p>Global Attention 有缺陷：<strong>当输入的句子特别长的时候（比如输入是段落或文档），Global Attention 的计算量将会变得很大</strong>。因为我们要求源句子中所有的词都参与每一时刻的计算，所以作者才提出了Local Attention，即只注意源句子的一个小子集，而不是所有单词。Local Attention 的思想也来源于前人的工作，有关 Soft Attention 和 Hard Attention 之间的折中权衡。 <strong>不像软对齐那样注意所有的输入而导致计算量过大，也不像硬对齐那样只选择一个输入而导致过程不可微，需要加入复杂的技巧（variance reduction、reinforcement learning）来训练模型。</strong>因此 Local Attention 既可微，能训练，同时计算量小。</p><p><strong>Local Attention 的思想是只关注每个目标词的源位置的子集。该模型首先预测当前目标词的对齐位置。然后，以源位置 <span class="math inline">\(p_t\)</span> 为中心的窗口用于计算上下文向量 <span class="math inline">\(c_t\)</span>，即窗口中源隐藏状态的加权平均值。从窗口中的当前目标状态 <span class="math inline">\(h_t\)</span> 和源状态 <span class="math inline">\(\bar h_s\)</span> 推断出权重 <span class="math inline">\(a_t\)</span>。</strong></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210727091743663.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>首先，在时间 <span class="math inline">\(t\)</span> 为每个目标词生成一个 Aligned position <span class="math inline">\(p_t\)</span>，然后计算上下文向量 <span class="math inline">\(c_i\)</span> 为窗口 <span class="math inline">\([p_t - D, p_t + D]\)</span> 内 源隐藏状态的加权平均值。<span class="math inline">\(D\)</span> 是根据经验选的窗口半径，是一个超参数。这样，对齐向量是一个定长的向量，长度便为 <span class="math inline">\(2D+1\)</span>.</p><p>如何确定 <img src="https://www.zhihu.com/equation?tex=p_t" srcset="/img/loading.gif" lazyload alt="[公式]">的值，Local Attention 给出了<strong>两种确定注意力中心的方法：</strong></p><h4 id="monotonic-alignment-local-m单调对齐">Monotonic alignment (local-m)：单调对齐</h4><p>作者假设输入和输出在很大程度是一一对应的，也就是假设输入和输出是<strong>单调对齐</strong>（Monotonic）的，简单的设置 <span class="math inline">\(p_t = t\)</span>. 对齐向量 <span class="math inline">\(a_t\)</span> 同 Global Attention 的一致。 <span class="math display">\[ \begin{aligned} \boldsymbol{a}_{t}(s) &amp;=\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \\ &amp;=\frac{\exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)\right)}{\sum_{s^{\prime}} \exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s^{\prime}}\right)\right)} \end{aligned} \]</span></p><h4 id="predictive-alignment-local-p预测中心对齐">Predictive alignment (local-p)：预测中心对齐</h4><p>不像上面那样假设输入输入单调对齐，而是预测对齐中心。即针对每个目标端输出，预测他在源语言端的对齐位置： <span class="math display">\[ p_{t}=S \cdot \operatorname{sigmoid}\left(\boldsymbol{v}_{p}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{p}} \boldsymbol{h}_{t}\right)\right) \]</span> 其中，<span class="math inline">\(\boldsymbol{W}_{\boldsymbol{p}}\)</span> 和 <span class="math inline">\(v_p\)</span> 都是用来预测位置的参数。<span class="math inline">\(S\)</span> 是源句长度，这样就有 <span class="math inline">\(p_t \in [0, S]\)</span>。另外，这里还有一个trick，作者对对齐向量的计算做了修改，在 <span class="math inline">\(p_t\)</span> 周围引入了一个服从 <span class="math inline">\(N(p_t, \frac D 2)\)</span> 的高斯分布来对齐权重，从直觉上考虑，<strong>距离目标位置越近的词理当起到更大的作用</strong>，因此对齐向量为 <span class="math display">\[ \boldsymbol{a}_{t}(s)=\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \exp \left(-\frac{\left(s-p_{t}\right)^{2}}{2 \sigma^{2}}\right) \]</span> 其中 <span class="math inline">\(\sigma\)</span> 也是超参数，作者凭经验选择 <span class="math inline">\(\sigma = \frac D 2\)</span>。同时也可以看到，在没有引入高斯分布之前，位置 <span class="math inline">\(p_t\)</span> 并没有直接与网络相连，虽然计算 <span class="math inline">\(p_t\)</span> 的过程可以微分，但是<strong>作为窗口中心这个操作是不可微的</strong>，因此也需要某种额外的方式（用来 <span class="math inline">\(p_t\)</span> 导出 <span class="math inline">\(a_t\)</span>)将其与网络关联起来，使得参数可以通过 <strong>backprop</strong> 训练。</p><div class="note note-info"><p>可以想下，没有 Gassian 的引入，是学习不了计算 <span class="math inline">\(p_t\)</span> 中的 <span class="math inline">\(\boldsymbol{W}_{\boldsymbol{p}}\)</span> 和 <span class="math inline">\(v_p\)</span> 的，因为它不是直接通过某个函数和网络联系起来，而是计算出 <span class="math inline">\(p_t\)</span> 的值，然后把它当做窗口的中心。虽然计算 <span class="math inline">\(p_t\)</span> 的操作可以微分，但是当做窗口中心这个操作是没办法微分的，所以没办法BP。在 Hard Attention 中，直接取 score 最大的作为 Attention，同理，这个<strong>取max的操作无法微分</strong>，所以 Hard Attention 需要使用其他 technique 来进行优化 BP。</p></div><div class="note note-info"><p><strong>关于 soft attention 和 hard attention</strong></p><ol type="1"><li><p>Soft Attention Model（与 Luong 的 Global Attention 基本相同）</p><p>所谓 Soft，意思是在求注意力分配概率分布的时候，对于输入句子X中任意一个单词都给出个概率，是个概率分布。</p><ul><li><em>优点</em>：模型平滑且可微。</li><li><em>缺点</em>：当源输入很大时很昂贵。</li></ul></li><li><p>Hard Attention Model</p><p>既然 Soft 是给每个单词都赋予一个单词对齐概率，那么如果不这样做，直接从输入句子里面找到某个特定的单词，然后把目标句子单词和这个单词对齐，而其它输入句子中的单词硬性地认为对齐概率为0，这就是 Hard Attention Model 的思想。</p><ul><li><em>优点</em>：推理时计算量较少。</li><li><em>缺点</em>：模型是不可微的，需要更复杂的技术，如方差减少或强化学习来训练。(本文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">Luong, et al., 2015</a> )</li></ul><p>Hard AM 在图像里证明有用，但是估计在文本里面用处不大，因为这种单词一一对齐明显要求太高，如果对不齐对后续处理负面影响很大，所以在 NLP 的文献里看不到用 Hard AM，估计大家都试过了，效果不好。</p></li><li><p><strong>介于 Soft 和 Hard 之间</strong></p><p>Soft AM 要求输入句子每个单词都要赋予单词对齐概率，Hard AM 要求从输入句子中精确地找到一个单词来和输出单词对齐，那么可以放松 Hard 的条件，先找到 Hard AM 在输入句子中单词对齐的那个单词大致位置，然后以这个单词作为轴心，向左向右拓展出一个大小为D的窗口，在这个 2D+1 窗口内的单词内进行类似 Soft AM 的对齐概率计算即可。</p></li></ol><p><strong>Global Attention 和 Local Attention 各有优劣，实际中 Global 的用的更多一点，因为：</strong></p><ul><li><strong>Local Attention 当 encoder 不长时，计算量并没有减少</strong></li><li><strong>位置向量 <span class="math inline">\(p_t\)</span> 的预测并不非常准确，直接影响到 Local Attention 的准确率</strong></li></ul></div><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730205202489.png" srcset="/img/loading.gif" lazyload></p><h3 id="input-feeding-approach">Input-feeding approach</h3><p>在之前提到的几种Attention机制中，每个Attention决策都是独立做出的，在传统的统计机器翻译中，我们通常会保存一个 coverage set，也就是记录到目前为止源语言端已经被翻译过的词。同样，在 Attention-based NMT 中，<strong>前面计算得到的对齐信息也应该用来辅助当前的对齐决策过程</strong>。因此作者在每一步将之前的 Attention 隐层输出 <span class="math inline">\(\tilde h_t\)</span> 与输入拼接后再喂到编码器中，这样就可以使得模型能够注意到之前生成过的单词，同时这样的结构，<strong>无论在垂直方向还是水平方向，都有足够的深度</strong>，因此可以期待这样的深度网络可以有更好的表现。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210727171334525.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><blockquote><p>decoder 部分第二层第 t 步的状态会作为第一层第 t+1 步的输入</p></blockquote><h3 id="luong-attention-与-bahdanau-attention-区别">Luong Attention 与 Bahdanau Attention 区别</h3><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730205552231.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>Luong Attention 与 Bahdanau 的 Soft Attention 相似，但是有一定的区别：</p><ol type="1"><li><p>Luong Attention 中，Encoder 和 Decoder 使用的是 多层 LSTM 网络。使用的是 LSTM 最顶层的hidden states</p><p>而 Bahdanau Attention 中，Encoder 使用的是 双向RNN(BiGRU)，Encoder 端的隐层状态使用前向与反向 RNN 的隐层向量 concatenate 合成，Decoder 使用 单向 RNN。(上图有点不严谨..)</p></li><li><p>注意力的计算方式不同。 通过模型和计算公式可以看出 Luong Attention 计算路径从 <span class="math inline">\(\boldsymbol{h}_{t} \rightarrow \boldsymbol{a}_{t} \rightarrow \boldsymbol{c}_{t} \rightarrow \tilde{\boldsymbol{h}}_{t}\)</span>. 作者直接由<strong>当前隐状态 <span class="math inline">\(h_t\)</span> 和每一个输入隐状态 <span class="math inline">\(\bar h_s\)</span> </strong>计算得出权重。</p><p>而 Bahdanau Attention 的计算路径从 <span class="math inline">\(\boldsymbol{h}_{t-1} \rightarrow \boldsymbol{a}_{t} \rightarrow \boldsymbol{c}_{t} \rightarrow \boldsymbol{h}_{t}\)</span>， 再将其送入深度输出网络和 Maxout 层得到预测结果。即利用<strong>过去的隐状态 <span class="math inline">\(h_{t-1}\)</span> 和每一个输入隐状态 <span class="math inline">\(\bar h_s\)</span> </strong>来预测当前的隐状态</p><blockquote><p>作者提到 Luong Attention 计算路径(computation path)更简单。</p><p>Luong attention 机制中的 decoder 在每一步使用当前步（而非前一步）的 hidden state 来计算注意力，从逻辑上更自然，但需要使用一层额外的 RNN decoder 来计算输出。</p></blockquote></li><li><p>Decoder 的输入输出不同. Luong Attention 中decoder 部分建立了一层额外的网络结构，以上下文向量 <span class="math inline">\(c_t\)</span> 与原 Decoder 第 t 步的 hidden state <span class="math inline">\(h_t\)</span> 拼接作为输入，得到第 t 步的 hidden state <span class="math inline">\(\tilde h_t\)</span> 并输出 <span class="math inline">\(\hat y_t\)</span>.</p><p>而 Bahdanau Attention , Decoder 在第 t 步时，输入是由上下文向量 <span class="math inline">\(c_t\)</span> 与前一步 hidden state <span class="math inline">\(h_{t-1}\)</span> 拼接(concatenate )得出的，得到第 t 步的 hidden state <span class="math inline">\(h_t\)</span> 并直接输出 <span class="math inline">\(\hat y_{t+1}\)</span></p></li><li><p>Luong 的 Attention 只用了一种对齐打分函数 <span class="math inline">\(concat\)</span>。而 Global Attention 给出了三种对齐方式 <span class="math inline">\(dot, general, concat\)</span>。（并且在后面验证，general 的效果更好一些）</p></li><li><p>最后，本方法使用一种 input-feeding 的方式来记录过去已经计算过的 对齐信息（即将前面得到的 attentional hidden state <span class="math inline">\(\widetilde{h}_{i-1}\)</span> 加入到当前 <span class="math inline">\(\widetilde{h}_{i}\)</span> 的计算过程中).</p><p>在机器翻译领域，都需要使用一个coverage set（覆盖集）去记录那些source words已经被翻译。在 Bahdanau 等人的工作中, 在计算 <span class="math inline">\(\boldsymbol{h}_{t}\)</span> 时引入了前一时刻的隐层状态, 虽然也类似实现了 “coverage” 的效果, 但是并没有分析这样的网络连接是否有效。Luong 等人认为该模型更 general, 可以用到各种堆叠的 RNN 结构中 (stacking recurrent architectures)</p></li></ol><h3 id="experiments">Experiments</h3><p>从训练曲线可以看到加入Local Attention和Feed Input之后优化速度和优化结果都要好过原始模型，同时训练结果也更加<strong>稳健</strong>。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210727175347854.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210727175258520.png" srcset="/img/loading.gif" lazyload alt="image-20210727175258520" style="zoom:80%"></p><p>由于计算资源的限制，作者没有给出所有的 Attention机制 和对齐函数的所有可能的组合，但也可以分析出很多有价值的结论。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210727175323502.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>从上图可以看出，location-based对齐函数比起其他对齐函数而言效果不好，<strong>当引入unk replacement之后带来的BLEU分数提升不如其他的对齐函数高</strong>，而对于content-based对齐函数，concat 表现得不好，因此作者没有列出，一个可能的原因是作者过度简化了 <span class="math inline">\(\boldsymbol{W}_{\boldsymbol{a}}\)</span>。有趣的一点是，<strong>dot更适合global attention而general更适合local attention</strong>，但综合比较来看，local-p总体效果是最好的，<strong>local-p(general)</strong>的所有指标都是最佳的。</p><h3 id="对齐质量"><strong>对齐质量</strong></h3><p>Bahdanau 在提出 Attention 时通过对样例可视化的方法表明了Attention对齐的有效性，而这里作者用<strong>AER指标（Alignment Error Rate）</strong>来衡量模型的对齐效果，不同模型的给出的AER指标如下。AER当然是越小越好，可以看到Local Attention的AER指标更低。值得注意的是，ensemble竟不比单纯的local-m(general)要好，而在BLEU中ensemble分数要高很多，这一定程度上说明了<strong>AER指标和翻译分数的相关性不是很强</strong>。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210727175310187.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h3 id="summary">Summary</h3><p>本文提出了 Global Attention 和 Local Attention。<strong>Global Attention 关注源句子的所有单词，而 Local Attention 只关注每个目标词对应的源句子的一个子集。</strong>Global Attention 和 Bahdanau 提出的 Attention机制 相似，但做了简化（有关对齐函数）。另外，作者还提出了三种不同的<strong>对齐函数（Alignment Functions）：Dot、General、Concat</strong> 来计算 Attention。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/NLP/">NLP</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/NLP/">NLP</a> <a class="hover-with-bg" href="/tags/Attention/">Attention</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/07/29/CS224n%2001%20Introduction%20and%20Word%20Vectors/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">CS224n 01 Introduction and Word Vectors</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/07/26/Bahdanau%20Attention%EF%BC%9ANeural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate/"><span class="hidden-mobile">Bahdanau Attention：Neural Machine Translation by Jointly Learning to Align and Translate</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>