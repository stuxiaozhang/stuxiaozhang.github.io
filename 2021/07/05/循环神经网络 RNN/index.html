<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="循环神经网络 RNN循环神经网络（recurrent neural network）或RNN是一类用于处理序列数据的神经网络。序列数据用原始的神经网络难以建模，基于此，RNN 引入了 hidden state， 可对序列数据提取特征，接着再转换为输出。


注：图中的圆圈表示向量，箭头表示对向量做变换。

这就是最经典的RNN结构，它的输入是 $x_1, x_2, …x_n$，输出为 $y1, y"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>循环神经网络 RNN - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="循环神经网络 RNN"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-07-05 21:51" pubdate>2021年7月5日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.8k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 57 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">循环神经网络 RNN</h1><p class="note note-info">本文最后更新于：2021年7月30日</p><div class="markdown-body"><h2 id="循环神经网络-RNN"><a href="#循环神经网络-RNN" class="headerlink" title="循环神经网络 RNN"></a>循环神经网络 RNN</h2><p>循环神经网络（recurrent neural network）或RNN是一类用于处理<strong>序列数据</strong>的神经网络。序列数据用原始的神经网络难以建模，基于此，RNN 引入了 hidden state， 可对序列数据提取特征，接着再转换为输出。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210724085122874.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><blockquote><p>注：图中的圆圈表示向量，箭头表示对向量做变换。</p></blockquote><p>这就是最经典的RNN结构，它的输入是 $x_1, x_2, …x_n$，输出为 $y1, y2, …yn$，也就是说，<strong>输入和输出序列必须要是等长的</strong>。再详细展开RNN。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210706192557245.png" srcset="/img/loading.gif" lazyload style="zoom:90%"></p><p>$x$ 是一个向量，它表示<strong>输入层</strong>的值。$s$ 是一个向量，它表示<strong>隐藏层</strong>的值。$U$ 是输入层到隐藏层的<strong>权重矩阵</strong>。$o$ 也是一个向量，它表示<strong>输出层</strong>的值。$V$ 是隐藏层到输出层的<strong>权重矩阵</strong>。</p><p><strong>循环神经网络</strong>的<strong>隐藏层</strong>的值 $s$ 不仅仅取决于当前这次的输入 $x$，还取决于上一次<strong>隐藏层</strong>的值 $s$。<strong>权重矩阵</strong> $W$ 就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。</p><p>所以，RNN这个网络在 $t$ 时刻接收到输入 $x<em>{t}$ 之后, 隐藏层的值是 $s</em>{t}$, 输出值是 $o<em>{t}$ . 关键一点是, $s</em>{t}$ 的值不仅仅取决于 $x<em>{t}$，还取决于 $s</em>{t-1}$ 。可以用下面的公式来表示RNN的计算方法:</p><script type="math/tex;mode=display">\begin{array}{l}
S_{t}=f\left(U \cdot X_{t}+W \cdot S_{t-1}\right) \\ 
O_{t}=g\left(V \cdot S_{t}\right) \\
\end{array}</script><p>这里的 $f()$ 函数表示激活函数，对于 CNN 来说，激活函数一般选取的都是 ReLU，但是 RNN 一般选用 tanh。$S<em>t$ 的值不仅仅取决于 $X_t$，还取决于 $S</em>{t-1}$。可以将这个过程理解为，你现在大四，你的知识是由大四学到的知识（当前输入 $X<em>t$）和大三以及大三以前学到的知识（记忆 $S</em>{t-1}$）的结合。</p><p><strong>RNN 的结构细节：</strong></p><ol><li>可以把 $S_t$ 当作隐状态，捕捉了之前时间点上的信息。就像你去考研一样，考的时候记住了你能记住的所有信息</li><li>可惜的是 $S_t$ 并不能捕捉之前所有时间点的信息，或者说在网络传播的过程中会 “忘掉” 一部分。就像你考研也记不住所有的英语单词一样</li><li>和卷积神经网络一样，RNN 中的每个节点都共享了一组参数 $(U,V,W)$，这样就能极大降低计算量</li></ol><p>由于 <strong>RNN 输入和输出序列必须是等长的</strong>，因为这个限制的存在，经典RNN的适用范围比较小，但也有一些问题适合用经典的RNN结构建模，如：</p><ul><li><p>计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。</p></li><li><p>输入为字符，输出为下一个字符的概率。这就是著名的 <a target="_blank" rel="noopener" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Char RNN</a></p></li></ul><h3 id="经典的RNN结构（N-vs-N）"><a href="#经典的RNN结构（N-vs-N）" class="headerlink" title="经典的RNN结构（N vs N）"></a>经典的RNN结构（N vs N）</h3><h4 id="N-VS-1"><a href="#N-VS-1" class="headerlink" title="N VS 1"></a>N VS 1</h4><p>有的时候，要处理的问题是，输入是一个序列，输出是一个单独的值(而不是序列)，此时通常在最后的一个 序列上进行输出变换。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210724090448797.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>这种 N VS 1 结构通常用来处理序列分类问题，如：</p><ul><li>输入一段文字判别它所属的类别</li><li>输入一个句子判断其情感倾向</li><li>输入一段视频并判断它的类别等等。</li></ul><h4 id="1-VS-N"><a href="#1-VS-N" class="headerlink" title="1 VS N"></a>1 VS N</h4><p>有时要处理的问题输入是一个单独的值，输出是一个序列。此时，有两种主要建模方式：</p><p>方式一：可只在其中的某一个序列进行计算，比如序列第一个进行输入计算：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210724090842023.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>方式二：把输入信息X作为每个阶段的输入：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210724090926460.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>这种 1 VS N 的结构可以处理的问题有：</p><ul><li>从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子</li><li>从类别生成语音或音乐等</li></ul><h4 id="N-vs-M"><a href="#N-vs-M" class="headerlink" title="N vs M"></a>N vs M</h4><p>RNN最重要的一个变种：N vs M。这种结构又叫 Encoder-Decoder 模型，也可以称之为Seq2Seq模型。原始的 N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。</p><p>因此，<strong>Encoder-Decoder结构先将输入数据编码成一个语义向量c，然后用另一个RNN网络对其进行解码</strong>。</p><p><img src="C:%5CUsers%5CHP%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210724091327947.png" srcset="/img/loading.gif" lazyload alt="image-20210724091327947"></p><p>这种 N VS M 的结构可以处理的问题有：</p><ul><li>机器翻译，输入一种语言文本序列，输出另外一种语言的文本序列</li><li>文本摘要，输入文本序列，输出这段文本序列摘要</li><li>阅读理解，输入文章，输出问题答案</li><li>语音识别，输入语音序列信息，输出文字序列</li></ul><p>详细的 Seq2Seq 介绍可以看这篇文章 ==Seq2Seq和Attention==。</p><h3 id="CNN和RNN的区别"><a href="#CNN和RNN的区别" class="headerlink" title="CNN和RNN的区别"></a>CNN和RNN的区别</h3><div class="table-container"><table><thead><tr><th>类别</th><th>特点描述</th></tr></thead><tbody><tr><td>相同点</td><td>1、传统神经网络的扩展。<br>2、前向计算产生结果，反向计算模型更新。<br>3、每层神经网络横向可以多个神经元共存,纵向可以有多层神经网络连接。</td></tr><tr><td>不同点</td><td>1、CNN空间扩展，神经元与特征卷积；RNN时间扩展，神经元与多个时间输出计算<br>2、RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出</td></tr></tbody></table></div><h3 id="前向传播和反向传播BPTT"><a href="#前向传播和反向传播BPTT" class="headerlink" title="前向传播和反向传播BPTT"></a>前向传播和反向传播BPTT</h3><p>以下图为例：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210706201554377.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>假设时间序列只有三段, $S_{0}$ 为给定值, 神经元没有激活函数, 则RNN最简单的前向传播过程如下:</p><script type="math/tex;mode=display">S_{1}=W_{x} X_{1}+W_{s} S_{0}+b_{1} O_{1}=W_{o} S_{1}+b_{2} \\
 S_{2}=W_{x} X_{2}+W_{s} S_{1}+b_{1} O_{2}=W_{o} S_{2}+b_{2} \\
 S_{3}=W_{x} X_{3}+W_{s} S_{2}+b_{1} O_{3}=W_{o} S_{3}+b_{2}</script><p>假设在 $\mathrm{t}=3$ 时刻，损失函数为 $L<em>{3}=\frac{1}{2}\left(Y</em>{3}-O<em>{3}\right)^{2}$。则对于一次训练任务的损失函数为 $L=\sum</em>{t=0}^{T} L_{t}$, 即每一时刻损失值的累加.</p><p>BPTT(back-propagation through time)算法是常用的训练RNN的方法，其本质还是BP算法，只不过RNN处理时间序列数据，所以要基于时间反向传播，故叫随时间反向传播。BPTT的中心思想和BP算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。使用随机梯度下降法训练RNN其实就是对 $W<em>{x},W</em>{s},W<em>{o}$ 以及 $b</em>{1},b_{2}$ 求偏导, 并不断调整它们以使L尽可能达到最小的过程。</p><p>现在假设时间序列只有三段, $\mathrm{t} 1, \mathrm{t} 2, \mathrm{t} 3$ 。只对 $\mathrm{t} 3$ 时刻的 $W<em>{x},W</em>{s},W_{o}$ 求偏导(其他时刻类似):</p><script type="math/tex;mode=display">\frac{\partial L_{3}}{\partial W_{o}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial W_{o}} \\
 \frac{\partial L_{3}}{\partial W_{x}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial W_{x}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial W_{x}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial S_{1}} \frac{\partial S_{1}}{\partial W_{x}} \\
 \frac{\partial L_{3}}{\partial W_{s}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial S_{1}} \frac{\partial S_{1}}{\partial W_{s}}</script><p>可以看出对于 $W<em>{0}$ 求偏导并没有长期依赖, 但是对于 $W</em>{x}, W<em>{s}$ 求偏导, 会随着时间序列产生长期依赖。因为 $S</em>{t}$ 随着时间序列向前传播, 而 $S<em>{t}$ 又是 $W</em>{x}, W<em>{s}$ 的函数。<br>根据上述求偏导的过程可以得出任意时刻对 $W</em>{x}, W_{s}$ 求偏导的公式:</p><script type="math/tex;mode=display">\frac{\partial L_{t}}{\partial W_{x}}=\sum_{k=0}^{t} \frac{\partial L_{t}}{\partial O_{t}} \frac{\partial O_{t}}{\partial S_{t}}\left(\prod_{j=k+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}}\right) \frac{\partial S_{k}}{\partial W_{x}} \\

\frac{\partial L_{t}}{\partial W_{s}}=\sum_{k=0}^{t} \frac{\partial L_{t}}{\partial O_{t}} \frac{\partial O_{t}}{\partial S_{t}}\left(\prod_{j=k+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}}\right) \frac{\partial S_{k}}{\partial W_{s}}</script><p>如果加上激活函数, $S<em>{j}=\tanh \left(W</em>{x} X<em>{j}+W</em>{s} S<em>{j-1}+b</em>{1}\right) $,<br>则</p><script type="math/tex;mode=display">\prod_{j=k+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}}=\prod_{j=k+1}^{t} \tanh ^{\prime} \cdot W_{s}</script><h3 id="RNN-梯度消失和梯度爆炸"><a href="#RNN-梯度消失和梯度爆炸" class="headerlink" title="RNN 梯度消失和梯度爆炸"></a>RNN 梯度消失和梯度爆炸</h3><p>激活函数tanh和它的导数图像如下。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/tanh.jpg" srcset="/img/loading.gif" lazyload style="zoom:30%"></p><p>由上图可以看出 $\tanh ^{\prime} \leq 1$, 对于训练过程大部分情况下 $tanh$ 的导数是小于1的，因为很少情 况下会出现 $W<em>{x} X</em>{j}+W<em>{s} S</em>{j-1}+b<em>{1}=0$ , 如果 $W</em>{s}$ 也是一个大于0小于1的值, 则当t很大时 $\prod<em>{j=k+1}^{t} \tanh ^{\prime} \cdot W</em>{s}$ 就会趋近于0(和 $0.01^{50} $ 趋近与0是一个道理)。同理当 $W<em>{s}$ 很大时 $\prod</em>{j=k+1}^{t} \tanh ^{\prime} \cdot W_{s}$ 就会趋近于无穷, 这就是RNN中梯度消失和爆炸的原因。</p><p>于是，为了解决这个现象，出现了LSTM.</p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层，如下图所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM1.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>Long Short Term网络，一般就叫做 LSTM。是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM2.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>注：上图图标具体含义如下所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM3.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>上图中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p><h3 id="LSTM-核心思想"><a href="#LSTM-核心思想" class="headerlink" title="LSTM 核心思想"></a>LSTM 核心思想</h3><p>LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。示意图如下所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM4.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。示意图如下：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM5.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>LSTM 拥有三个门，分别是忘记门，输入门和输出门，来保护和控制细胞状态。</p><h4 id="忘记门"><a href="#忘记门" class="headerlink" title="忘记门"></a>忘记门</h4><p>作用对象：细胞状态 cell state</p><p>作用：表示是否保留当前节点的历史状态</p><p>操作步骤：该门会读取 $h<em>{t-1}$ 和 $x_t$，输出一个在 0 到 1 之间的数值给每个在细胞状态$C</em>{t-1}$中的数字。1 表示“完全保留”，0 表示“完全舍弃”。示意图如下：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM6.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h4 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h4><p>作用对象：细胞状态 cell state</p><p>作用：表示是否允许当前隐层节点的输出值传递到下一层</p><p>操作步骤：</p><ul><li><p>步骤一：sigmoid 层称 “输入门层” 决定什么值我们将要更新。</p></li><li><p>步骤二：tanh 层创建一个新的候选值向量 $\tilde{C}_t$ 加入到状态中。其示意图如下：</p></li></ul><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM7.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><ul><li>步骤三：将 $c<em>{t-1}$ 更新为 $c</em>{t}$。将旧状态与 $f_t$ 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 $i_t * \tilde{C}_t$ 得到新的候选值，根据我们决定更新每个状态的程度进行变化。其示意图如下：</li></ul><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM8.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h4 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h4><p>作用对象：隐层 $h_t$ hidden state</p><p>作用：表示是否允许当前隐层节点的输出值传递到下一层</p><p>操作步骤：</p><ul><li><p>步骤一：通过 sigmoid 层来确定细胞状态的哪个部分将输出。</p></li><li><p>步骤二：把细胞状态通过 tanh 进行处理，并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p></li></ul><p>其示意图如下所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM9.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>无论是“ 遗忘门 ”，“ 输入门 ”，“ 输出门 ”，他们都被一个 $0$ ~ $1$ 之间的值所决定，这个值与一组数据做乘积，来显示这个数据的重要性。 而这个值都是由 $h_{t-1}$ 和 $x_t$ 经过 sigmoid 函数而得出来的，他们在不同的位置，衡量的却都是 “重要性”，更好理解的说法是：他们都衡量当前这个数据往后传递的必要性（重要性）：</p><ul><li>在遗忘门中，它决定了 $C_{t-1}$ 被遗忘的程度</li><li>在输入门中，它决定了 $\tilde C_{i}$ 的记忆程度，多少加入到记忆中来更新记忆</li><li>在输出门中，它决定了 这个更新的记忆输入到下一个隐藏层的重要程度。</li></ul><p>公式总结：</p><script type="math/tex;mode=display">\begin{aligned}
\left(\begin{array}{l}
i \\
f \\
o \\
g
\end{array}\right) &=\left(\begin{array}{c}
\sigma \\
\sigma \\
\sigma \\
\tanh
\end{array}\right) W\left(\begin{array}{c}
h_{t-1} \\
x_{t}
\end{array}\right) \\
c_{t} &=f \odot c_{t-1}+i \odot g \\
h_{t} &=o \odot \tanh \left(c_{t}\right)
\end{aligned}</script><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210723162852993.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210710222902681.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>GRU(Gate Recurrent Unit) 是 RNN 的另一类演化变种，与LSTM非常相似。GRU结构中<strong>去除了单元状态，而使用隐藏状态来传输信息。</strong>它只有两个门结构，分别是更新门和重置门。相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。</p><h3 id="重置门"><a href="#重置门" class="headerlink" title="重置门"></a>重置门</h3><p>重置门用于决定丢弃先前信息的程度。</p><h3 id="更新门"><a href="#更新门" class="headerlink" title="更新门"></a>更新门</h3><p>更新门的作用类似于LSTM中的遗忘门和输入门，它能决定要丢弃哪些信息和要添加哪些新信息。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM12.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>可以看到，这里的遗忘 $z<em>t$ 和选择 $1-z_t$是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 $1-z_t$ ，我们就会使用包含当前输入的 $\tilde{h}</em>{t}$ 中所对应的权重进行弥补 $z_t$ 。以保持一种”恒定“状态。</p><h2 id="BiRNN"><a href="#BiRNN" class="headerlink" title="BiRNN"></a>BiRNN</h2><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730213648990.png" srcset="/img/loading.gif" lazyload alt="image-20210730213648990"></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730213824267.png" srcset="/img/loading.gif" lazyload alt="image-20210730213824267" style="zoom:67%"></p><blockquote><p>计算 t1 到 tT 时刻所有的前向传播结果，上图中的黑实线，黄实线</p><p>再计算反向传播（输入反转）的黑虚线，黄虚线，从 tT 到 t1</p><p>再从黄点计算输出结果到红点</p><p>(好像是这样?)</p></blockquote></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> <a class="hover-with-bg" href="/tags/RNN/">RNN</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/07/11/Seq2Seq%E5%92%8CAttention/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Seq2Seq和Attention机制</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/07/03/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN/"><span class="hidden-mobile">卷积神经网络 CNN</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>