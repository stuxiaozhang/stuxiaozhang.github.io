<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="循环神经网络 RNN
循环神经网络（recurrent neural network）或RNN是一类用于处理序列数据的神经网络。序列数据用原始的神经网络难以建模，基于此，RNN 引入了 hidden state， 可对序列数据提取特征，接着再转换为输出。


注：图中的圆圈表示向量，箭头表示对向量做变换。

这就是最经典的RNN结构，它的输入是 \(x_1, x_2, ...x_n\)"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>循环神经网络 RNN - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="循环神经网络 RNN"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-07-05 21:51" pubdate>2021年7月5日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 31 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">循环神经网络 RNN</h1><p class="note note-info">本文最后更新于：2021年8月16日</p><div class="markdown-body"><h1 id="循环神经网络-rnn">循环神经网络 RNN</h1><p>循环神经网络（recurrent neural network）或RNN是一类用于处理<strong>序列数据</strong>的神经网络。序列数据用原始的神经网络难以建模，基于此，RNN 引入了 hidden state， 可对序列数据提取特征，接着再转换为输出。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210724085122874.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><blockquote><p>注：图中的圆圈表示向量，箭头表示对向量做变换。</p></blockquote><p>这就是最经典的RNN结构，它的输入是 <span class="math inline">\(x_1, x_2, ...x_n\)</span>，输出为 <span class="math inline">\(y1, y2, ...yn\)</span>，也就是说，<strong>输入和输出序列必须要是等长的</strong>。再详细展开RNN。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210706192557245.png" srcset="/img/loading.gif" lazyload style="zoom:90%"></p><p><span class="math inline">\(x\)</span> 是一个向量，它表示<strong>输入层</strong>的值。<span class="math inline">\(s\)</span> 是一个向量，它表示<strong>隐藏层</strong>的值。<span class="math inline">\(U\)</span> 是输入层到隐藏层的<strong>权重矩阵</strong>。<span class="math inline">\(o\)</span> 也是一个向量，它表示<strong>输出层</strong>的值。<span class="math inline">\(V\)</span> 是隐藏层到输出层的<strong>权重矩阵</strong>。</p><p><strong>循环神经网络</strong>的<strong>隐藏层</strong>的值 <span class="math inline">\(s\)</span> 不仅仅取决于当前这次的输入 <span class="math inline">\(x\)</span>，还取决于上一次<strong>隐藏层</strong>的值 <span class="math inline">\(s\)</span>。<strong>权重矩阵</strong> <span class="math inline">\(W\)</span> 就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。</p><p>所以，RNN这个网络在 <span class="math inline">\(t\)</span> 时刻接收到输入 <span class="math inline">\(x_{t}\)</span> 之后, 隐藏层的值是 <span class="math inline">\(s_{t}\)</span>, 输出值是 <span class="math inline">\(o_{t}\)</span> . 关键一点是, <span class="math inline">\(s_{t}\)</span> 的值不仅仅取决于 <span class="math inline">\(x_{t}\)</span>，还取决于 <span class="math inline">\(s_{t-1}\)</span> 。可以用下面的公式来表示RNN的计算方法: <span class="math display">\[ \begin{array}{l} S_{t}=f\left(U \cdot X_{t}+W \cdot S_{t-1}\right) \\ O_{t}=g\left(V \cdot S_{t}\right) \\ \end{array} \]</span> 这里的 <span class="math inline">\(f()\)</span> 函数表示激活函数，对于 CNN 来说，激活函数一般选取的都是 ReLU，但是 RNN 一般选用 tanh。<span class="math inline">\(S_t\)</span> 的值不仅仅取决于 <span class="math inline">\(X_t\)</span>，还取决于 <span class="math inline">\(S_{t-1}\)</span>。可以将这个过程理解为，你现在大四，你的知识是由大四学到的知识（当前输入 <span class="math inline">\(X_t\)</span>）和大三以及大三以前学到的知识（记忆 <span class="math inline">\(S_{t-1}\)</span>）的结合。</p><p><strong>RNN 的结构细节：</strong></p><ol type="1"><li>可以把 <span class="math inline">\(S_t\)</span> 当作隐状态，捕捉了之前时间点上的信息。就像你去考研一样，考的时候记住了你能记住的所有信息</li><li>可惜的是 <span class="math inline">\(S_t\)</span> 并不能捕捉之前所有时间点的信息，或者说在网络传播的过程中会 &quot;忘掉&quot; 一部分。就像你考研也记不住所有的英语单词一样</li><li>和卷积神经网络一样，RNN 中的每个节点都共享了一组参数 <span class="math inline">\((U,V,W)\)</span>，这样就能极大降低计算量</li></ol><p>由于 <strong>RNN 输入和输出序列必须是等长的</strong>，因为这个限制的存在，经典RNN的适用范围比较小，但也有一些问题适合用经典的RNN结构建模，如：</p><ul><li><p>计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。</p></li><li><p>输入为字符，输出为下一个字符的概率。这就是著名的 <a target="_blank" rel="noopener" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Char RNN</a></p></li></ul><h2 id="经典的rnn结构n-vs-n">经典的RNN结构（N vs N）</h2><h3 id="n-vs-1">N VS 1</h3><p>有的时候，要处理的问题是，输入是一个序列，输出是一个单独的值(而不是序列)，此时通常在最后的一个 序列上进行输出变换。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210724090448797.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>这种 N VS 1 结构通常用来处理序列分类问题，如：</p><ul><li>输入一段文字判别它所属的类别</li><li>输入一个句子判断其情感倾向</li><li>输入一段视频并判断它的类别等等。</li></ul><h3 id="vs-n">1 VS N</h3><p>有时要处理的问题输入是一个单独的值，输出是一个序列。此时，有两种主要建模方式：</p><p>方式一：可只在其中的某一个序列进行计算，比如序列第一个进行输入计算：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210724090842023.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>方式二：把输入信息X作为每个阶段的输入：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210724090926460.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>这种 1 VS N 的结构可以处理的问题有：</p><ul><li>从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子</li><li>从类别生成语音或音乐等</li></ul><h3 id="n-vs-m">N vs M</h3><p>RNN最重要的一个变种：N vs M。这种结构又叫 Encoder-Decoder 模型，也可以称之为Seq2Seq模型。原始的 N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。</p><p>因此，<strong>Encoder-Decoder结构先将输入数据编码成一个语义向量c，然后用另一个RNN网络对其进行解码</strong>。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210724091327947.png" srcset="/img/loading.gif" lazyload></p><p>这种 N VS M 的结构可以处理的问题有：</p><ul><li>机器翻译，输入一种语言文本序列，输出另外一种语言的文本序列</li><li>文本摘要，输入文本序列，输出这段文本序列摘要</li><li>阅读理解，输入文章，输出问题答案</li><li>语音识别，输入语音序列信息，输出文字序列</li></ul><p>详细的 Seq2Seq 介绍可以看这篇文章 ==Seq2Seq和Attention==。</p><h2 id="cnn和rnn的区别">CNN和RNN的区别</h2><table><colgroup><col style="width:9%"><col style="width:90%"></colgroup><thead><tr class="header"><th>类别</th><th>特点描述</th></tr></thead><tbody><tr class="odd"><td>相同点</td><td>1、传统神经网络的扩展。<br>2、前向计算产生结果，反向计算模型更新。<br>3、每层神经网络横向可以多个神经元共存,纵向可以有多层神经网络连接。</td></tr><tr class="even"><td>不同点</td><td>1、CNN空间扩展，神经元与特征卷积；RNN时间扩展，神经元与多个时间输出计算<br>2、RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出</td></tr></tbody></table><h2 id="前向传播和反向传播bptt">前向传播和反向传播BPTT</h2><p>以下图为例：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210706201554377.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>假设时间序列只有三段, <span class="math inline">\(S_{0}\)</span> 为给定值, 神经元没有激活函数, 则RNN最简单的前向传播过程如下: <span class="math display">\[ S_{1}=W_{x} X_{1}+W_{s} S_{0}+b_{1} O_{1}=W_{o} S_{1}+b_{2} \\ S_{2}=W_{x} X_{2}+W_{s} S_{1}+b_{1} O_{2}=W_{o} S_{2}+b_{2} \\ S_{3}=W_{x} X_{3}+W_{s} S_{2}+b_{1} O_{3}=W_{o} S_{3}+b_{2} \]</span> 假设在 <span class="math inline">\(\mathrm{t}=3\)</span> 时刻，损失函数为 <span class="math inline">\(L_{3}=\frac{1}{2}\left(Y_{3}-O_{3}\right)^{2}\)</span>。则对于一次训练任务的损失函数为 <span class="math inline">\(L=\sum_{t=0}^{T} L_{t}\)</span>, 即每一时刻损失值的累加.</p><p>BPTT(back-propagation through time)算法是常用的训练RNN的方法，其本质还是BP算法，只不过RNN处理时间序列数据，所以要基于时间反向传播，故叫随时间反向传播。BPTT的中心思想和BP算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。使用随机梯度下降法训练RNN其实就是对 <span class="math inline">\(W_{x},W_{s},W_{o}\)</span> 以及 <span class="math inline">\(b_{1},b_{2}\)</span> 求偏导, 并不断调整它们以使L尽可能达到最小的过程。</p><p>现在假设时间序列只有三段, <span class="math inline">\(\mathrm{t} 1, \mathrm{t} 2, \mathrm{t} 3\)</span> 。只对 <span class="math inline">\(\mathrm{t} 3\)</span> 时刻的 <span class="math inline">\(W_{x},W_{s},W_{o}\)</span> 求偏导(其他时刻类似): <span class="math display">\[ \frac{\partial L_{3}}{\partial W_{o}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial W_{o}} \\ \frac{\partial L_{3}}{\partial W_{x}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial W_{x}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial W_{x}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial S_{1}} \frac{\partial S_{1}}{\partial W_{x}} \\ \frac{\partial L_{3}}{\partial W_{s}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial S_{1}} \frac{\partial S_{1}}{\partial W_{s}} \]</span> 可以看出对于 <span class="math inline">\(W_{0}\)</span> 求偏导并没有长期依赖, 但是对于 <span class="math inline">\(W_{x}, W_{s}\)</span> 求偏导, 会随着时间序列产生长期依赖。因为 <span class="math inline">\(S_{t}\)</span> 随着时间序列向前传播, 而 <span class="math inline">\(S_{t}\)</span> 又是 <span class="math inline">\(W_{x}, W_{s}\)</span> 的函数。 根据上述求偏导的过程可以得出任意时刻对 <span class="math inline">\(W_{x}, W_{s}\)</span> 求偏导的公式: $$ =<em>{k=0}^{t} (</em>{j=k+1}^{t} ) \</p><p>=<em>{k=0}^{t} (</em>{j=k+1}^{t} ) <span class="math display">\[ 如果加上激活函数, $S_{j}=\tanh \left(W_{x} X_{j}+W_{s} S_{j-1}+b_{1}\right)$, 则 \]</span> <em>{j=k+1}^{t} =</em>{j=k+1}^{t} ^{} W_{s} $$</p><h2 id="rnn-梯度消失和梯度爆炸">RNN 梯度消失和梯度爆炸</h2><p>激活函数tanh和它的导数图像如下。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/tanh.jpg" srcset="/img/loading.gif" lazyload style="zoom:30%"></p><p>由上图可以看出 <span class="math inline">\(\tanh ^{\prime} \leq 1\)</span>, 对于训练过程大部分情况下 <span class="math inline">\(tanh\)</span> 的导数是小于1的，因为很少情况下会出现 <span class="math inline">\(W_{x} X_{j}+W_{s} S_{j-1}+b_{1}=0\)</span> , 如果 <span class="math inline">\(W_{s}\)</span> 也是一个大于0小于1的值, 则当t很大时 <span class="math inline">\(\prod_{j=k+1}^{t} \tanh ^{\prime} \cdot W_{s}\)</span> 就会趋近于0(和 $0.01^{50} $ 趋近与0是一个道理)。同理当 <span class="math inline">\(W_{s}\)</span> 很大时 <span class="math inline">\(\prod_{j=k+1}^{t} \tanh ^{\prime} \cdot W_{s}\)</span> 就会趋近于无穷, 这就是RNN中梯度消失和爆炸的原因。</p><p>于是，为了解决这个现象，出现了LSTM.</p><h1 id="lstm">LSTM</h1><p>长时间的短期记忆网络（Long Short-Term Memory Networks），很多地方翻译为长短期记忆网络，给人一种歧义，以为是网络一会儿能记很长的内容，一会儿能记很短的内容，但其实正确的翻译应该是长时间的短期记忆网络。它的本质就是能够记住很长时期内的信息.</p><p>所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层，如下图所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM1.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>LSTM 是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM2.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>注：上图图标具体含义如下所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM3.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>上图中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，如向量的乘法、加法，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接（比如一个是 <span class="math inline">\(h_{t−1}\)</span>，另一个是 <span class="math inline">\(x_t\)</span>，那么合并后的输出就是 <span class="math inline">\([h_{t−1},x_t]\)</span>）。分开的线表示内容被复制，然后分发到不同的位置。</p><h2 id="lstm-核心思想">LSTM 核心思想</h2><p>LSTM 的关键就是 cell 状态，即贯穿图顶部的水平线。cell 状态的传输就像一条传送带，向量从整个 cell 中穿过，只是做了少量的线性操作，这种结构能很轻松地实现信息从整个 cell 中穿过而不做改变（这样就可以实现长时期地记忆保留）</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM4.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>LSTM 也有能力向 cell 状态中添加或删除信息，这是由称为<strong>门（gates）</strong>的结构仔细控制的。<strong>门</strong>可以选择性的让信息通过，它们由 sigmoid 神经网络层和 pointwise 逐点相乘实现：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM5.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>LSTM 拥有三个门，分别是忘记门，输入门和输出门，来控制信息。</p><h3 id="遗忘门">遗忘门</h3><p>LSTM 的第一步是<strong>决定要从 cell 状态中丢弃什么信息</strong>，这个决定是由一个叫做 <code>forget gate layer</code> 的 sigmoid 神经层来实现的。它的输入是 <span class="math inline">\(h_{t−1}\)</span> 和 <span class="math inline">\(x_t\)</span>，输出是一个数值都在 0~1 之间的向量（向量长度和 <span class="math inline">\(C_{t−1}\)</span> 一样），表示让 <span class="math inline">\(C_{t−1}\)</span> 的各部分信息通过的比重，0 表示不让任何信息通过，1 表示让所有信息通过</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM6.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h3 id="输入门">输入门</h3><p>下一步是<strong>决定要让多少新的信息加入到 cell 状态中</strong>。实现这个需要包括两个步骤：首先，一个叫做 <code>input gate layer</code> 的 sigmoid 层决定哪些信息需要更新。另一个 tanh 层创建一个新的 candidate 向量 <span class="math inline">\(\tilde{C}_t\)</span> 。最后，我们把这两个部分联合起来对 cell 状态进行更新.</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM7.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>下一步是将 <span class="math inline">\(C_{t-1}\)</span> 更新为 <span class="math inline">\(C_{t}\)</span>。将旧状态与 <span class="math inline">\(f_t\)</span> 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 <span class="math inline">\(i_t * \tilde{C}_t\)</span> 得到新的候选值，根据我们决定更新每个状态的程度进行变化。其示意图如下：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM8.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h3 id="输出门">输出门</h3><p>最后，需要决定输出什么值了。这个输出主要是依赖于 cell 状态 <span class="math inline">\(C_t\)</span>，但是是经过筛选的版本。作用对象是隐层 <span class="math inline">\(h_t\)</span> hidden state。</p><p>操作步骤：</p><ul><li><p>步骤一：通过 sigmoid 层来确定细胞状态的哪个部分将输出。</p></li><li><p>步骤二：把细胞状态 <span class="math inline">\(C_t\)</span> 通过 tanh 进行处理（把数值归一化到 - 1 和 1 之间），并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p></li></ul><p>其示意图如下所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM9.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>无论是“ 遗忘门 ”，“ 输入门 ”，“ 输出门 ”，他们都被一个 <span class="math inline">\(0\)</span> ~ <span class="math inline">\(1\)</span> 之间的值所决定，这个值与一组数据做乘积，来显示这个数据的重要性。 而这个值都是由 <span class="math inline">\(h_{t-1}\)</span> 和 <span class="math inline">\(x_t\)</span> 经过 sigmoid 函数而得出来的，他们在不同的位置，衡量的却都是 “重要性”，更好理解的说法是：他们都衡量当前这个数据往后传递的必要性（重要性）：</p><ul><li>在遗忘门中，它决定了 <span class="math inline">\(C_{t-1}\)</span> 被遗忘的程度</li><li>在输入门中，它决定了 <span class="math inline">\(\tilde C_{i}\)</span> 的记忆程度，多少加入到记忆中来更新记忆</li><li>在输出门中，它决定了 这个更新的记忆输入到下一个隐藏层的重要程度。</li></ul><p>公式总结： <span class="math display">\[ \begin{aligned} \left(\begin{array}{l} i \\ f \\ o \\ g \end{array}\right) &amp;=\left(\begin{array}{c} \sigma \\ \sigma \\ \sigma \\ \tanh \end{array}\right) W\left(\begin{array}{c} h_{t-1} \\ x_{t} \end{array}\right) \\ c_{t} &amp;=f \odot c_{t-1}+i \odot g \\ h_{t} &amp;=o \odot \tanh \left(c_{t}\right) \end{aligned} \]</span> <img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210723162852993.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><h1 id="gru">GRU</h1><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210710222902681.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>GRU(Gate Recurrent Unit) 是 RNN 的另一类演化变种，与LSTM非常相似。GRU结构中<strong>去除了单元状态，而使用隐藏状态来传输信息。</strong>它只有两个门结构，分别是更新门和重置门。相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。</p><h3 id="重置门">重置门</h3><p>重置门用于决定丢弃先前信息的程度。</p><h3 id="更新门">更新门</h3><p>更新门的作用类似于LSTM中的遗忘门和输入门，它能决定要丢弃哪些信息和要添加哪些新信息。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/LSTM12.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>可以看到，这里的遗忘 <span class="math inline">\(z_t\)</span> 和选择 <span class="math inline">\(1-z_t\)</span>是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 <span class="math inline">\(1-z_t\)</span> ，我们就会使用包含当前输入的 <span class="math inline">\(\tilde{h}_{t}\)</span> 中所对应的权重进行弥补 <span class="math inline">\(z_t\)</span> 。以保持一种”恒定“状态。</p><h2 id="birnn">BiRNN</h2><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730213648990.png" srcset="/img/loading.gif" lazyload></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730213824267.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>可以将 BiRNN 看成是两层神经网络，第一层从左边作为系列的起始输入，在文本处理上可以理解成从句子的开头开始输入，而第二层则是从右边作为系列的起始输入，在文本处理上可以理解成从句子的最后一个词语作为输入，反向做与第一层一样的处理处理。最后对得到的两个结果进行处理。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> <a class="hover-with-bg" href="/tags/RNN/">RNN</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/07/11/Seq2Seq%E5%92%8CAttention/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Seq2Seq和Attention机制</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/07/03/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN/"><span class="hidden-mobile">卷积神经网络 CNN</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>