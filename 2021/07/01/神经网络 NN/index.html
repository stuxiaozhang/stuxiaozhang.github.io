<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="深度学习_神经网络 NN1. M-P 神经元M‑P 神经元（名字源于两位发明者，一个用来模拟生物行为的数学模型）：接收n个输入(通常是来自其他神经元)，并给各个输入赋予权重计算加权和，然后和自身特有的阈值进行比较（作减法），最后经过激活函数（模拟“抑制”和“激活”）处理得到输出（通常是给下一个神经元）

y=f\left(\sum_{i=1}^{n} w_{i} x_{i}-\theta\righ"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>神经网络 NN - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="神经网络 NN"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-07-01 09:44" pubdate>2021年7月1日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2.8k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 26 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">神经网络 NN</h1><p class="note note-info">本文最后更新于：2021年7月7日</p><div class="markdown-body"><h2 id="深度学习-神经网络-NN"><a href="#深度学习-神经网络-NN" class="headerlink" title="深度学习_神经网络 NN"></a>深度学习_神经网络 NN</h2><h3 id="1-M-P-神经元"><a href="#1-M-P-神经元" class="headerlink" title="1. M-P 神经元"></a>1. M-P 神经元</h3><p>M‑P 神经元（名字源于两位发明者，一个用来模拟生物行为的数学模型）：接收<u>n个输入</u>(通常是来自其他神经元)，并<u>给各个输入赋予权重计算加权和</u>，然后<u>和自身特有的阈值进行比较（作减法）</u>，最后<u>经过激活函数（模拟“抑制”和“激活”）处理得到输出</u>（通常是给下一个神经元）</p><script type="math/tex;mode=display">y=f\left(\sum_{i=1}^{n} w_{i} x_{i}-\theta\right)=f\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)</script><p>单个 M‑P 神经元：感知机（ sign 作激活函数）、对数几率回归（ sigmoid 作激活函数）</p><p>多个 M‑P 神经元：神经网络</p><h3 id="2-感知机"><a href="#2-感知机" class="headerlink" title="2. 感知机"></a>2. 感知机</h3><h4 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h4><p>假设有一可以被线性分类的样本集 $\left{\left(x<em>{i}, y</em>{i}\right)\right}<em>{i=1}^{N}$ , 其中 $x</em>{i} \in \mathbb{R}^{p}, y<em>{i}{-1,+1}</em>{\circ}$ 输入 $x\in X$ 表示实例的特征向量，对应于输入空间（特征空间）的点； 输出 $y\in$ 表示实例的类别。 由输入空间到输出空间的如下函数：</p><script type="math/tex;mode=display">f(x)=\operatorname{sign}(w⋅x+b)</script><p>称为感知机。其中，$w$ 和 $b$ 为感知机模型参数，$w\in \R^n$ 叫作权值（weight ）或权值向量（weight vector ) , $b\in\R$ 叫作偏置（bias), $w • x$ 表示 $w$ 和 $x$ 的内积。$\operatorname{sign}$ 是符号函数， 即</p><script type="math/tex;mode=display">\operatorname{sign}(x)=\left\{\begin{array}{l}
+1, x \geq 0 \\
-1, x<0
\end{array}\right.</script><p>感知机预测，就是通过学习得到的感知机模型，对于新的输入实例给出其对应的输出类别。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210703150903113.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><h4 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h4><p>数据集一定要线性可分。感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数 $w,b$，需要确定一个学习策略，即定义损失函数井将损失函数极小化。</p><p>选择损失函数的两种原则：</p><ul><li>误分类点的总数</li><li>误分类点到超平面 $S$ 的总距离（感知机是通过这种方法）</li></ul><p>感知机的思想是<strong>错误驱动</strong>。其模型是 $f(x)=\operatorname{sign}(w \cdot x+b)$，$f(x)$ 输出该样本点的类别，定义集合 $M$ 为误分类点的集合。</p><p>对于误分类的数据 $(x<em>{i},y</em>{i})$ 来说，，它的 $y_i$ 一定与 $w\cdot x_i+b$ 异号，即：</p><script type="math/tex;mode=display">-y_{i}\left(w \cdot x_{i}+b\right)>0</script><p>损失函数的一个自然选择是误分类点的个数, 即 $L(w)=\sum<em>{i=1}^{N} I\left{-y</em>{i}\left(w\cdot x_{i}+b\right)&gt;0\right}$ , 但是这样的损失函数是不可导的，不易优化。</p><blockquote><p>因为指示函数输出非0即1. 非连续的, 无法求导.</p></blockquote><p>因此采用另一种损失函数，<strong>即误分类点到超平面的总距离</strong>。</p><p>在 $\mathbb{R}^{p}$ 空间中任一点 $x_{0}$ 到超平面的距离为：</p><script type="math/tex;mode=display">\frac{|w\cdot x_{0}+b|}{\|w\|}</script><script type="math/tex;mode=display">可以参考初中知识，平面中点到直线的距离:  d=\frac{|A x+B y+C|}{\sqrt{A^{2}+B^{2}}}</script><p>所以误分类点 $x_i$ 到超平面 $S$ 的距离为</p><script type="math/tex;mode=display">-\frac{1}{\|w\|} y_{i}\left(w \cdot x_{i}+b\right)</script><p>其中, $|| w||$ 是 $w$ 的 $L_{2}$ 范式。我们假设误分类点的集合为 $M$ ，所以误分类点到超平面的总距离为，</p><script type="math/tex;mode=display">-\frac{1}{\|w\|} \sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)</script><p>不考虑 $\frac{1}{|w|}$，就得到感知机的损失函数：</p><script type="math/tex;mode=display">L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot  x_{i}+b\right)</script><p>这个损失函数就是感知机学习的经验风险函数。如果没有误分类点 ，损失函数值是0。而且， 误分类点越少，误分类点离超平面越近，损失函数值就越小。</p><h4 id="感知机学习算法"><a href="#感知机学习算法" class="headerlink" title="感知机学习算法"></a>感知机学习算法</h4><p>假设误分类点集合 $M$ 是固定的，那么计算损失函数 $L(w,b)$ 的梯度：</p><script type="math/tex;mode=display">\frac{\partial L(w, b)}{\partial w}=-\sum_{x_{i} \in M} y_{i} x_{i} \\
\frac{\partial L(w, b)}{\partial b}=-\sum_{x_{i} \in M} y_{i}</script><p>感知机的学习算法使用随机梯度下降法 (SGD)，其学习的步骤如下：</p><p>输入: 训练数据集 $T=\left{\left(x<em>{1}, y</em>{1}\right),\left(x<em>{2}, y</em>{2}\right), \cdots,\left(x<em>{N}, y</em>{N}\right)\right}$ ，其中 $x<em>{i} \in \mathcal{X}=\mathbf{R}^{n}, y</em>{i} \in \mathcal{Y}={-1,+1}, i=1,2, \cdots, N $；学习率 $\eta(0&lt;\eta \leqslant 1)$</p><p>输出: $w, b$；感知机模型 $f(x)=\operatorname{sign}(w \cdot x+b)$</p><ol><li><p>选取初值 $w<em>{0}, b</em>{0}$;</p></li><li><p>在训练集中选取数据 $\left(x<em>{i}, y</em>{i}\right) $</p></li><li><p>如果 $y<em>{i}\left(w^{T} x</em>{i}+b\right) \leq 0$, 则更新参数</p><script type="math/tex;mode=display">w \leftarrow w+\eta y_{i} x_{i} \\
b \leftarrow b+\eta y_{i}</script></li><li><p>④转至②，直到训练集中没有误分类点。</p></li></ol><p>这种学习算法直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时, 则调整 $w, b$ 的值, 使分离超平面向该误分类点的一侧移动, 以减少该误分类点与超平面间的距离, 直至超平面越过该误分类点使其被正确分类。</p><div class="note note-primary"><p><em>希望小张推算一下P40的例2.1。补上</em></p></div><p>由于<u>像感知机这种单个神经元分类能力有限，只能分类线性可分的数据集</u>，对于线性不可分的数据集则无能为力，<u>但是多个神经元构成的神经网络能够分类线性不可分的数据集（西瓜书上异或问题的那个例子），且有理论证明（通用近似定理）</u>：只需一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂度的连续函数。因此，神经网络既能做回归，也能做分类，而且不需要复杂的特征工程。</p><h4 id="非线性问题的三种解决方法"><a href="#非线性问题的三种解决方法" class="headerlink" title="非线性问题的三种解决方法"></a>非线性问题的三种解决方法</h4><p><strong>1.（明转）Non-Transformation:</strong> 将非线性问题转化为线性问题求解，高维比低维更易线性可分。</p><p>将二维转为三维。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210703115751657.png" srcset="/img/loading.gif" lazyload style="zoom:40%"></p><p><strong>2.（暗转）Kernel Method</strong>：隐藏了一个非线性变化，$K(x,x′)=&lt;ϕ(x),ϕ(x′)&gt;$</p><p>(具体可以看<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41485273/article/details/111752155">白板推导系列笔记（七）-核方法</a>) <font color="red">留坑，学完改链接。</font></p><p><strong>3.（自转）神经网络</strong>：</p><p>XOR(异或运算): 相同为0，不同为1</p><p>OR(或运算): 有一个就是1</p><p>AND(与运算): 都为1</p><p>NOT(非): 非0才为1</p><script type="math/tex;mode=display">x_{1} \oplus x_{2}= \left(\neg x_{1} \wedge x_{2}\right) \vee \left(x_{1} \wedge \neg x_{2}\right) \\
复合函数 \to 复合表达式</script><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210703134632242.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><div class="note note-info"><p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。</p><p>什么是特征工程？</p><p>特征工程本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210702201059588.png" srcset="/img/loading.gif" lazyload style="zoom:100%"></p></div><h3 id="3-神经网络"><a href="#3-神经网络" class="headerlink" title="3. 神经网络"></a>3. 神经网络</h3><p>前馈神经网络：又称多层感知机(MLP)。每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接。（隐层阈值 $\gamma_h $，输出层阈值 $\theta_j$ ）</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210702202444468.png" srcset="/img/loading.gif" lazyload style="zoom:40%"></p><p>将神经网络（记为 NN​）看作一个特征加工函数</p><script type="math/tex;mode=display">\boldsymbol{x} \in \mathbb{R}^{d} \rightarrow \mathrm{NN}(\boldsymbol{x}) \rightarrow \boldsymbol{y}=\boldsymbol{x}^{*} \in \mathbb{R}^{l}</script><ul><li><p>(单输出）回归：后面接一个 $\mathbb{R}^{l} \rightarrow \mathbb{R}$ ($l$ 维到1维)的神经元，例如：没有激活函数的神经元</p><script type="math/tex;mode=display">y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}^{*}+b</script></li><li><p>分类：后面接一个 $\mathbb{R}^{l} \rightarrow[0,1]$ 的神经元，例如：激活函数为sigmoid函数的神经元</p><script type="math/tex;mode=display">y=\frac{1}{1+e^{-\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}^{*}+b\right)}}</script></li></ul><p>在模型训练过程中，神经网络(NN)自动学习提取有用的特征。神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”（connection weight，权重..）以及每个功能神经元的阈值（偏置bias..）；换言之，神经网络学到的东西，蕴含在连接权和阈值中。</p><p>假设多层前馈网络中的激活函数全为sigmoid函数，且当前要完成的任务为一个（多输出）回归任务，因此损失函数可以采用均方误差（分类任务则用交叉嫡）。对于某个训 练样本 $\left(\boldsymbol{x}<em>{k}, \boldsymbol{y}</em>{k}\right)$ , 其中 $\boldsymbol{y}<em>{k}=\left(y</em>{1}^{k}, y<em>{2}^{k}, \ldots, y</em>{l}^{k}\right)$ , 假定其前馈网络的输出为 $\hat{\boldsymbol{y}}<em>{k}=<br>\left(\hat{y}</em>{1}^{k}, \hat{y}<em>{2}^{k}, \ldots, \hat{y}</em>{l}^{k}\right)$ , 则该单个样本的均方误差（损失）为</p><script type="math/tex;mode=display">E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(\hat{y}_{j}^{k}-y_{j}^{k}\right)^{2}</script><blockquote><p>$\frac{1}{2}$ 是为了求导加的，常数对求导无影响。</p></blockquote><h4 id="反向传播算法-BackPropagation"><a href="#反向传播算法-BackPropagation" class="headerlink" title="反向传播算法 BackPropagation"></a>反向传播算法 BackPropagation</h4><p>BP算法：基于随机梯度下降的参数更新算法。以目标的负梯度方向对参数进行调整。</p><script type="math/tex;mode=display">\begin{array}{r}
w \leftarrow w+\Delta w \\
\Delta w=-\eta \nabla_{w} E
\end{array}</script><p>其中只需推导出 $\nabla_{w} E$ 这个损失函数 $E$ 关于参数 $w$ 的一阶偏导数(梯度)即可(链式求导)。</p><blockquote><p>由于 NN(x) 通常是极其复杂的非凸函数，不具备像凸函数这种良好的数学性质，因此随机梯度下降不能保证一定能走到全局最小值点，更多情况下走到的都是局部极小值点(local minima)。</p></blockquote><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210702224902509.png" srcset="/img/loading.gif" lazyload style="zoom:40%"></p><p>以输入层第 $i$ 个神经元与隐层第 $h$ 个神经元之间的连接权 $w_{hj}$ 为例推导：</p><script type="math/tex;mode=display">E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(\hat{y}_{j}^{k}-y_{j}^{k}\right)^{2}, \quad \Delta v_{i h}=-\eta \frac{\partial E_{k}}{\partial v_{i h}}</script><script type="math/tex;mode=display">\frac{\partial E_{k}}{\partial v_{i h}}=\sum_{j=1}^{l} \frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \cdot \frac{\partial \beta_{j}}{\partial b_{h}} \cdot \frac{\partial b_{h}}{\partial \alpha_{h}} \cdot \frac{\partial \alpha_{h}}{\partial v_{i h}}</script><div class="note note-primary"><script type="math/tex;mode=display">E_k =\hat{y}_{1}^k + \hat{y}_{2}^k +...+\hat{y}_{l}^k \\\hat{y}_{1}^k = f_1(v_{ih}),\hat{y}_{2}^k = f_2(v_{ih})... \\\frac{\partial E_{k}}{\partial v_{i h}} = \frac{\partial f_{1}}{\partial v_{i h}} +  \frac{\partial f_{2}}{\partial v_{i h}} + ... +  \frac{\partial f_{l}}{\partial v_{i h}} =  \sum_{j=1}^{l} \frac{\partial f_{j}}{\partial v_{i h}}</script><p>对sigmoid求导为</p><script type="math/tex;mode=display">f'(x) = f(x)(1-f(x))</script><p>所以 $b_h$ 对 $a_h$ 求导为</p><script type="math/tex;mode=display">b_h  = f(a_h - \gamma_h) = sigmoid(a_h - \gamma_h)\\\frac{\partial b_{h}}{\partial \alpha_{h}}=f'(b_h) = f'(a_h- \gamma_h) = f(a_h- \gamma_h) \cdot (1-f(a_h- \gamma_h))=b_h(1-b_h)</script><p>$b_h$ 对 $a_h$ 求导为</p><script type="math/tex;mode=display">\hat{y}_{j}^k  = f(\beta_j - \theta_j) = sigmoid(\beta_j - \theta_j)\\\frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}}=f'(\hat{y}_{j}^k ) = f'(\beta_j - \theta_j) = f(\beta_j - \theta_j) \cdot (1-f(\beta_j - \theta_j))=\hat{y}_{j}^k (1-\hat{y}_{j}^k )</script><p>令 $g_j$ 为</p><script type="math/tex;mode=display">\begin{aligned}g_{j} &=-\frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \\&=-\left(\hat{y}_{j}^{k}-y_{j}^{k}\right) f^{\prime}\left(\beta_{j}-\theta_{j}\right) \\&=\hat{y}_{j}^{k}\left(1-\hat{y}_{j}^{k}\right)\left(y_{j}^{k}-\hat{y}_{j}^{k}\right)\end{aligned}</script></div><p>综上得：</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial E_{k}}{\partial v_{i h}} &=\sum_{j=1}^{l} \frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \cdot \frac{\partial \beta_{j}}{\partial b_{h}} \cdot \frac{\partial b_{h}}{\partial \alpha_{h}} \cdot \frac{\partial \alpha_{h}}{\partial v_{i h}} \\
&=\sum_{j=1}^{l} \frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \cdot \frac{\partial \beta_{j}}{\partial b_{h}} \cdot \frac{\partial b_{h}}{\partial \alpha_{h}} \cdot x_{i} \\
&=\sum_{j=1}^{l} \frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \cdot \frac{\partial \beta_{j}}{\partial b_{h}} \cdot f^{\prime}\left(\alpha_{h}-\gamma_{h}\right) \cdot x_{i} \\
&=\sum_{j=1}^{l} \frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \cdot w_{h j} \cdot f^{\prime}\left(\alpha_{h}-\gamma_{h}\right) \cdot x_{i} \\
&=\sum_{j=1}^{l}\left(-g_{j}\right) \cdot w_{h j} \cdot f^{\prime}\left(\alpha_{h}-\gamma_{h}\right) \cdot x_{i} \\
&=-f^{\prime}\left(\alpha_{h}-\gamma_{h}\right) \cdot \sum_{j=1}^{l} g_{j} \cdot w_{h j} \cdot x_{i} \\
&=-b_{h}\left(1-b_{h}\right) \cdot \sum_{j=1}^{l} g_{j} \cdot w_{h j} \cdot x_{i} \\
&=-e_{h} \cdot x_{i}
\end{aligned}</script><p>所以</p><script type="math/tex;mode=display">\Delta v_{i h}=-\eta \frac{\partial E_{k}}{\partial v_{i h}}=\eta e_{h} x_{i}</script><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><font color="red">能力不足，时间紧迫，日后有待补充...</font><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li>周志华《机器学习》(西瓜书)</li><li>谢文睿《机器学习公式详解》(南瓜书)</li><li>李航《统计学习方法》</li><li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Tt411s7fK">白板推导系列(二十三)-前馈神经网络（Feedforward Neural Network）</a></li></ol></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> <a class="hover-with-bg" href="/tags/MLP/">MLP</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/07/03/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">卷积神经网络 CNN</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/06/09/%E6%9D%8E%E5%AE%8F%E6%AF%85%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><span class="hidden-mobile">李宏毅 深度学习基础</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>