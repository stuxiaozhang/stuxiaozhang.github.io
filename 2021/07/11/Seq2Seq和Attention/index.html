<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="Seq2Seq和Attention机制
            2021.07.30：本文是有关 Attention机制 的一个小总结。
          
RNN有关RNN的介绍请看==循环神经网络 RNN==。
Encoder-DecoderEncoder-Decoder 算是一个通用的框架(结构)，在这个框架下可以使用不同的算法来解决不同的任务。例如在 Image Caption 的应用中"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>Seq2Seq和Attention机制 - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="Seq2Seq和Attention机制"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-07-11 21:22" pubdate>2021年7月11日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 4.3k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 34 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">Seq2Seq和Attention机制</h1><p class="note note-info">本文最后更新于：2021年7月31日</p><div class="markdown-body"><h2 id="Seq2Seq和Attention机制"><a href="#Seq2Seq和Attention机制" class="headerlink" title="Seq2Seq和Attention机制"></a>Seq2Seq和Attention机制</h2><div class="note note-info"><p><strong><em>2021.07.30</em></strong>：本文是有关 Attention机制 的一个小总结。</p></div><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>有关RNN的介绍请看==循环神经网络 RNN==。</p><h3 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h3><p>Encoder-Decoder 算是一个通用的框架(结构)，在这个框架下可以使用不同的算法来解决不同的任务。例如在 Image Caption 的应用中 Encoder-Decoder 就是 CNN-RNN 的编码 - 解码框架；在神经网络机器翻译中 Encoder-Decoder 往往就是 LSTM-LSTM 的编码 - 解码框架，在机器翻译中也被叫做 <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sequence to Sequence learning</a> 。</p><p>Encoder-Decoder 这个框架很好的诠释了机器学习的核心思路：将现实问题转化为数学问题，通过求解数学问题，从而解决现实问题。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210711215759034.png" srcset="/img/loading.gif" lazyload style="zoom:90%"></p><p>关于 Encoder-Decoder，有 2 点需要说明：</p><ol><li>不论输入和输出的长度是什么，中间的「向量 C」 长度都是固定的</li><li>根据不同任务可以选择不同的编码器和解码器（可以是一个 RNN ，但通常是 LSTM 或者 GRU ）</li></ol><p>只要是符合上面的框架，都可以统称为 Encoder-Decoder 框架(结构)。</p><h3 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><p>Seq2seq 是 sequence to sequence 的缩写。Seq2seq 旨在将一个序列转换到另一个序列。Seq2Seq 其实就是 <strong>Encoder-Decoder </strong>结构的网络，它的输入是一个序列，输出也是一个序列。在 <strong>Seq2Seq</strong> 模型中，编码器 Encoder 把所有的输入序列都编码成一个统一的语义向量 Context，然后再由解码器 Decoder 解码。</p><blockquote><p>所谓编码，就是将输入的序列编码成一个固定长度的向量；解码，就是将之前生成的固定向量再解码成输出序列。这里的输入序列和输出序列正是机器翻译的结果和输出。</p></blockquote><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210729232411426.png" srcset="/img/loading.gif" lazyload alt="image-20210729232411426" style="zoom:50%"></p><p>为了方便阐述，在选取 Encoder 和 Decoder 时都假设其为 RNN。在 RNN 中，当前时刻隐藏状态 $h<em>t$ 是由上一时刻的隐藏状态 $h</em>{t−1}$ 和当前时刻的输入 $x_t$ 决定的，如公式所示：</p><script type="math/tex;mode=display">h_t=f(h_{t−1},x_t)</script><p>在 <strong>编码阶段</strong>，获得各个时刻的隐藏层状态后，通过把这些隐藏层的状态进行汇总，可以生成最后的语义编码向量 $C$ ，如公式所示，其中 $q$ 表示某种非线性神经网络，此处表示多层 RNN 。</p><script type="math/tex;mode=display">C=q\left(h_{1}, h_{2}, \cdots, h_{T_{x}}\right)</script><p>在一些应用中，也可以直接将最后的隐藏层编码状态作为最终的语义编码 $C$，即满足：</p><script type="math/tex;mode=display">C=q\left(h_{1}, h_{2}, \cdots, h_{T_{x}}\right) = h_{T_{x}}</script><p>在 <strong>解码阶段</strong>，需要根据给定的语义向量 $C$ 和之前已经生成的输出序列 $y<em>1,y_2,⋯,y</em>{t−1}$ 来预测下一个输出的单词 $y_t$，即满足公式：</p><script type="math/tex;mode=display">y_{t}=\arg \max P\left(y_{t}\right)=\prod_{t=1}^{T} p\left(y_{t} \mid y_{1}, y_{2}, \cdots, y_{t-1}, C\right)</script><p>由于我们此处使用的 Decoder 是 RNN ，所以当前状态的输出只与上一状态和当前的输入相关，所以可以将公式简写成如下形式：</p><script type="math/tex;mode=display">y_t=g(y_{t−1},s_{t-1},C)</script><p>其中，$s<em>{t−1}$ 表示 Decoder 中 RNN 神经元的隐藏层状态，$y</em>{t−1}$ 表示前一时刻的输出，$C$ 代表的是编码后的语义向量，而 $g(⋅)$ 则是一个非线性的多层神经网络，可以输出 $y_t$ 的概率，一般情况下是由多层 RNN 和 softmax 层组成。</p><p><strong>Seq2seq 的局限性</strong></p><p>Seq2Seq 的局限性在于：<strong>Encoder 和 Decoder 之间只通过一个固定长度的语义向量 C 来唯一联系。</strong>也就是说，Encoder 必须要将输入的整个序列的信息都压缩进一个固定长度的向量中，存在两个弊端：一是语义向量 C 可能无法完全表示整个序列的信息；二是先输入到网络的内容携带的信息会被后输入的信息覆盖掉，输入的序列越长，该现象就越严重。这两个弊端使得 Decoder 在解码时一开始就无法获得输入序列足够多的信息，因此导致解码的精确度不够准确。</p><p>为了弥补 Seq2Seq 的局限性，提出了 Attention 机制。</p><h3 id="Attention-机制"><a href="#Attention-机制" class="headerlink" title="Attention 机制"></a>Attention 机制</h3><p>带有 Attention 机制的 Encoder-Decoder 模型则是要从序列中学习到每一个元素的重要程度，然后按重要程度将元素合并。因此，注意力机制是编码器和解码器之间的接口，<strong>它向解码器提供来自每个编码器隐藏状态的信息</strong>。通过这个设置，<strong>模型能够选择性地关注输入序列的有用部分，从而学习它们之间的“对齐”。</strong>这有助于模型有效地处理长输入语句。这就表明，在 Encoder 将输入的序列元素进行编码时，得到的不在是一个固定的语义编码 C ，而是存在多个语义编码，且不同的语义编码由不同的序列元素以不同的权重参数组合而成。</p><blockquote><p><strong>定义：对齐</strong></p><p>对齐是指将原文的片段与其对应的译文片段进行匹配。</p></blockquote><p>以机器翻译举个例子。当使用 Seq2Seq 时，模型从头到尾开始阅读文本，阅读完成，它开始翻译。但是如果句子很长，他很可能已经忘记了前面读过的内容。当引入 Attention机制 时，模型从头到尾阅读文本，同时，他会记下关键词，在翻译时会利用刚写下的关键词。</p><p>==注意力机制就是这样，每翻译一个词时，注意力会集中在不同的位置。==</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730092045401.png" srcset="/img/loading.gif" lazyload alt="image-20210730092045401" style="zoom:50%"></p><p>在 Attention 机制下，语义编码 C 就不在是输入序列 $X$ 的直接编码了，而是各个元素按其重要程度加权求和得到的，即：</p><script type="math/tex;mode=display">C_{i}=\sum_{j=0}^{T_{x}} a_{i j} f\left(x_{j}\right)</script><p>其中, 参数 $i$ 表示时刻，$j$ 表示序列中的第 $j$ 个元素, $T<em>{x}$ 表示序列的长度, $f(\cdot)$ 表示对元素 $x</em>{j}$ 的编码。$a<em>{i j}$ 可以看作是一个概率, 反映了元素 $h</em>{j}$ 对 $C_{i}$ 的重要性，可以使用 $\operatorname{softmax}$ 来表示：</p><script type="math/tex;mode=display">a_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T_{x}} \exp \left(e_{i k}\right)}</script><p>这里 $e<em>{ij}$ 正是反映了待编码的元素和其它元素之间的匹配度，当匹配度越高时，说明该元素对其的影响越大，则 $a</em>{ij}$ 的值也就越大。</p><p>因此，得出 $a_{ij}$ 的过程如下图：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730092534619.png" srcset="/img/loading.gif" lazyload alt="image-20210730092534619" style="zoom:50%"></p><p>其中，$h_i$ 表示 Encoder 的转换函数，$F(h_j,H_i)$ 表示预测与目标的匹配打分函数。将以上过程串联起来，则注意力模型的结构如下图所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730092637651.png" srcset="/img/loading.gif" lazyload alt="image-20210730092637651"></p><p>阅读到一篇有关 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&amp;mid=2247485860&amp;idx=1&amp;sn=e926a739784090b3779711164217b968&amp;chksm=c06981f9f71e08efb5f57441444f71a09f1d27fc667af656a5ad1173e32ad394201d02195a3a&amp;mpshare=1&amp;scene=1&amp;srcid=0618HMAYi4gzzwWfedLoOuSD&amp;key=cb6098335ab487a8ec84c95399379f16f975d33ce91588d73ecf857c54b543666b5927e231ad3a9b17bff0c20fff20fc49c262912dca050dee9465801de8a4cdc79e3d8f4fbc058345331fb691bcbacb&amp;ascene=1&amp;uin=MTE3NTM4MTY0NA%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=ikhBXxX7PL%2Fal9hbIGXbRFA96ei74EF%2BcP8KdbP6UcV6mIpOfPWzVuju%2Bqw86q5r">动画图解Attention机制</a> 的文章，这里主要是对 Attention 层的实现做下总结，详细内容请查看原文。注意力机制可以看作是神经网络架构中的一层神经网络，注意力层的实现可以分为 6 个步骤</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210729163220982.png" srcset="/img/loading.gif" lazyload alt="image-20210729163220982" style="zoom:50%"></p><p><strong>Step 0: 准备隐藏状态.</strong></p><p>首先准备第一个解码器的隐藏状态(红色)和所有可用的编码器隐藏状态(绿色)。在栗子中，有4个编码器隐藏状态和当前解码器隐藏状态。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/step0.gif" srcset="/img/loading.gif" lazyload alt="注意力的准备工作"></p><p><strong>Step 1: 得到每一个编码器隐藏状态的得分.</strong></p><p>score 分数(标量)由 score 函数 (也称为对齐函数或 <em>alignment model</em>)获得。</p><p>在本例中，score函数是解码器和编码器隐藏状态之间的点积(dot product)。</p><script type="math/tex;mode=display">\operatorname{score}(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s})</script><p>对齐函数是用来衡量第 $j$ 个源句子词与目标句子第 $i$ 个词的匹配程度，其实就是在预测目标句子第 $i$ 个单词时，Attention机制 赋予源句子中第 $j$ 个单词的<strong><em>注意力大小</em></strong></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/640 (1" srcset="/img/loading.gif" lazyload alt="640 (1)">.gif)</p><div class="table-container"><table><thead><tr><th style="text-align:left">decoder_hidden = [10, 5, 10]</th><th style="text-align:left">encoder_hidden</th><th>alignment score ($e_{ij}$)</th></tr></thead><tbody><tr><td style="text-align:left">[10, 5, 10]</td><td style="text-align:left">[0, 1, 1]</td><td>15 (= 10×0 + 5×1 + 10×1, the dot product)</td></tr><tr><td style="text-align:left">[10, 5, 10]</td><td style="text-align:left">[5, 0, 1]</td><td>60</td></tr><tr><td style="text-align:left">[10, 5, 10]</td><td style="text-align:left">[1, 1, 0]</td><td>15</td></tr><tr><td style="text-align:left">[10, 5, 10]</td><td style="text-align:left">[0, 5, 1]</td><td>35</td></tr></tbody></table></div><p><strong>Step 2: 把所有的得分送到softmax层跑一下.</strong></p><p>将分数送到softmax层中，这样softmax之后的分数(标量)加起来等于1。这些softmax的分数代表了<strong><em>注意力的分布</em></strong>。</p><script type="math/tex;mode=display">\alpha_{i j} =\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T} \exp \left(e_{ik} \right)}</script><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/640 (2" srcset="/img/loading.gif" lazyload alt="640 (2)">.gif)</p><div class="table-container"><table><thead><tr><th>encoder_hidden</th><th>alignment score ($e_{ij}$)</th><th>$\alpha_{i j}$</th></tr></thead><tbody><tr><td>[0, 1, 1]</td><td>15</td><td>0</td></tr><tr><td>[5, 0, 1]</td><td>60</td><td>1</td></tr><tr><td>[1, 1, 0]</td><td>15</td><td>0</td></tr><tr><td>[0, 5, 1]</td><td>35</td><td>0</td></tr></tbody></table></div><blockquote><p>注意力的分配仅按预期放在了<code>[5, 0, 1]</code>上。实际上，这些数字不是二进制的，而是0到1之间的一个浮点数。</p></blockquote><p><strong>Step 3</strong>: <strong>用每个编码器的隐藏状态乘以softmax之后的得分.</strong></p><p>通过将每个编码器的隐藏状态与其softmax之后的分数(标量)相乘，我们得到<strong><em>对齐向量</em></strong> 或<strong><em>标注向量</em></strong>。这正是对齐产生的机制。</p><script type="math/tex;mode=display">\alpha_{i j} \cdot  h_j</script><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/640 (3" srcset="/img/loading.gif" lazyload alt="640 (3)">.gif)</p><div class="table-container"><table><thead><tr><th>encoder_hidden</th><th>$\alpha_{ij}$</th><th>alignment vector 对齐向量</th></tr></thead><tbody><tr><td>[0, 1, 1]</td><td>0</td><td>[0, 0, 0]</td></tr><tr><td>[5, 0, 1]</td><td>1</td><td>[5, 0, 1]</td></tr><tr><td>[1, 1, 0]</td><td>0</td><td>[0, 0, 0]</td></tr><tr><td>[0, 5, 1]</td><td>0</td><td>[0, 0, 0]</td></tr></tbody></table></div><p>在这里，我们看到除了<code>[5, 0, 1]</code>外，所有编码器隐藏状态的对齐都被降低到0，这是因为注意力得分较低。这意味着我们可以期望第一个被翻译的单词应该与输入单词使用<code>[5, 0, 1]</code>嵌入表示的单词匹配。</p><p><strong>Step 4</strong>: <strong>把所有对齐的向量加起来.</strong></p><p>对 <strong><em>对齐向量</em></strong> 进行求和，生成 <strong><em>上下文向量</em></strong> $c_i$。上下文向量是前一步的对齐向量的聚合信息。</p><script type="math/tex;mode=display">c_{i}=\sum_{j=1}^{T_{x}} \alpha_{i j} h_{j}</script><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/640 (4" srcset="/img/loading.gif" lazyload alt="640 (4)">.gif)</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs subunit">根据上个表格<br>context = [0<span class="hljs-string">+5</span><span class="hljs-string">+0</span><span class="hljs-string">+0</span>, 0<span class="hljs-string">+0</span><span class="hljs-string">+0</span><span class="hljs-string">+0</span>, 0<span class="hljs-string">+1</span><span class="hljs-string">+0</span><span class="hljs-string">+0</span>] = [5, 0, 1]<br></code></pre></td></tr></table></figure><p><strong>Step 5</strong>: <strong>把上下文向量送到解码器中.</strong></p><p>这取决于体系结构设计。在下面会看到架构如何使用上下文向量作为解码器。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/640 (5" srcset="/img/loading.gif" lazyload alt="640 (5)">.gif)</p><p>Attention 机制完整的流程如下图：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/640 (6" srcset="/img/loading.gif" lazyload alt="640 (6)">.gif)</p><p><strong>Attention 到底是怎么工作的？</strong></p><p>答案：反向传播。反向传播将尽一切努力确保输出接近基本事实。这是通过改变RNNs和score函数(如果有的话)中的权重来实现的。这些权重将影响编码器的隐藏状态和解码器的隐藏状态，从而影响注意力得分。</p><h4 id="Bahdanau-Attention"><a href="#Bahdanau-Attention" class="headerlink" title="Bahdanau Attention"></a>Bahdanau Attention</h4><p>Attention 初次提出是在 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">”Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau <em>et. al</em>, 2015)”</a> 这篇论文。标题中使用了 align 这个词，意思是在训练模型的同时调整直接负责分数的权重。</p><p>Bahdanau ‘s idea:</p><ol><li><p>编码器是一个双向(前向+后向)门控循环单元(BiGRU)。解码器是一个 GRU，其初始隐藏状态是由向后编码器GRU的最后一个隐藏状态修改而来的向量(下图中未显示)。</p></li><li><p>注意层中的score函数是 <strong>additive/concat</strong>。<strong>（$a$ 就是 alignment model​, score函数）</strong></p><script type="math/tex;mode=display">e_{i j} = a\left(z_{i-1}, h_{j}\right) 
=v_{a}^{T} \tanh \left(W_{a} z_{i-1}+U_{a} h_{j}\right)</script></li><li><p>下一个解码器时间步的输入是前一个解码器时间步(粉红色)的输出与当前时间步(深绿色)的上下文向量之间的拼接。</p></li></ol><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210730153601466.png" srcset="/img/loading.gif" lazyload alt="image-20210730153601466"></p><h4 id="Luong-Attention"><a href="#Luong-Attention" class="headerlink" title="Luong Attention"></a>Luong Attention</h4><h5 id="GLobal-Attention"><a href="#GLobal-Attention" class="headerlink" title="GLobal Attention"></a>GLobal Attention</h5><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">“Effective Approaches to Attention-based Neural Machine Translation (Luong <em>et. al</em>, 2015)”</a> 的作者提出了 Global Attention 和 Local Attention，并提出了三种对齐函数的计算方式。作者指出，简化和泛化 Bahdanau et. al 的体系结构非常重要。方法如下(这里 Luong Attention 一般均指 Global Attention)</p><ol><li><p>编码器是一个两层的长短时记忆(LSTM)网络。解码器也具有相同的结构，其初始隐藏状态是最后一个编码器的隐藏状态。</p></li><li><p>他们实验过的score函数是 (1) <strong>点积</strong>，(2) <strong>general</strong>，(3) <strong>additive/concat</strong>，和(4) <strong>location-based</strong>。</p><blockquote><p>早期的 location-based 函数</p></blockquote><script type="math/tex;mode=display">\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)=\left\{\begin{array}{l}
\boldsymbol{h}_{t}^{\top} \overline{\boldsymbol{h}}_{s}, \quad \text{ dot } \\
\boldsymbol{h}_{t}^{\top} \boldsymbol{W}_{\boldsymbol{a}} \overline{\boldsymbol{h}}_{s}, \quad \text { general } \\
\boldsymbol{v}_{a}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{a}}\left[\boldsymbol{h}_{t} ; \overline{\boldsymbol{h}}_{s}\right]\right), \quad \text { concat }
\end{array}\right.</script></li><li><p>将当前解码器时间步的输出 $h_t$ 与当前时间步的上下文向量 $c_t$ 拼接，输入前向神经网络，得到当前解码器时间步的最终输出(粉红色)。</p></li></ol><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/640 (2" srcset="/img/loading.gif" lazyload alt="640 (2)">.jpg)</p><h5 id="Local-Attention"><a href="#Local-Attention" class="headerlink" title="Local Attention"></a>Local Attention</h5><p>Global Attention 有缺陷：<strong>当输入的句子特别长的时候（比如输入是段落或文档），Global Attention 的计算量将会变得很大</strong>。因为我们要求源句子中所有的词都参与每一时刻的计算，所以作者才提出了Local Attention，即只注意源句子的一个小子集，而不是所有单词。Local Attention 的思想也来源于前人的工作，有关 Soft Attention 和 Hard Attention 之间的折中权衡。 <strong>不像软对齐那样注意所有的输入而导致计算量过大，也不像硬对齐那样只选择一个输入而导致过程不可微，需要加入复杂的技巧（variance reduction、reinforcement learning）来训练模型。</strong>因此 Local Attention 既可微，能训练，同时计算量小。</p><p><strong>Local Attention 的思想是只关注每个目标词的源位置的子集。</strong>该模型首先预测当前目标词的对齐位置。然后，以源位置 $p_t$ 为中心的窗口用于计算上下文向量 $c_t$，即窗口中源隐藏状态的加权平均值。从窗口中的当前目标状态 $h_t$ 和源状态 $\bar h_s$ 推断出权重 $a_t$。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210727091743663.png" srcset="/img/loading.gif" lazyload alt="image-20210727091743663" style="zoom:67%"></p><h5 id="Score-函数"><a href="#Score-函数" class="headerlink" title="Score 函数"></a>Score 函数</h5><p>说到 Score 函数。score函数涉及点积运算(点积、余弦相似度等)，其思想是度量两个向量之间的相似度。对于前馈神经网络评分函数，其思想是让模型在变换的同时学习对齐权值。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/640.jpg" srcset="/img/loading.gif" lazyload alt="640"></p><h3 id="Attention-的本质"><a href="#Attention-的本质" class="headerlink" title="Attention 的本质"></a>Attention 的本质</h3><div class="note note-primary"><p>这块还是不懂，等再深入学习一下回来补充。</p></div><p>attention 是一组注意力分配系数的，attention 函数用来得到 attention value 的。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210712224747001.png" srcset="/img/loading.gif" lazyload alt="image-20210712224747001" style="zoom:50%"></p><p>attention value 的本质：它其实就是一个查询(query)到一系列键值(key-value)对的映射。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210712224959532.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>attention函数共有三步完成得到 attention value。</p><ul><li>Q 与 K 进行相似度计算得到权值</li><li>对上部权值归一化</li><li>用归一化的权值与 V 加权求和</li></ul><p>此时加权求和的结果就为注意力值。</p><script type="math/tex;mode=display">\text { Attention }\left(\text { Query }_{j}, \text { Source }\right)=\sum_{i=1}^{N} \text { Similarity }\left(\text { Query }_{j}, \text { Key }_{i}\right) * \text { Value }_{i}</script><p>在自然语言任务中，往往 Key 和 Value 是相同的。需要注意的是，计算出来的 attention value 是一个向量，代表序列元素 $x_j$ 的编码向量，包含了元素 $x_j$ 的上下文关系，即同时包含全局联系和局部联系。全局联系很好理解，因为在计算时考虑了该元素与其他所有元素的相似度计算；而局部联系则是因为在对元素 $x_j$ 进行编码时，重点考虑与其相似度较高的局部元素，尤其是其本身。</p><h3 id="Attention-机制的优劣"><a href="#Attention-机制的优劣" class="headerlink" title="Attention 机制的优劣"></a>Attention 机制的优劣</h3><p>相比于传统的 RNN 和 CNN，attention 机制具有如下优点：</p><ul><li>一步到位的全局联系捕捉，且关注了元素的局部联系；attention 函数在计算 attention value 时，是进行序列的每一个元素和其它元素的对比，在这个过程中每一个元素间的距离都是一；而在时间序列 RNNs 中，元素的值是通过一步步递推得到的长期依赖关系获取的，而越长的序列捕捉长期依赖关系的能力就会越弱。</li><li>并行计算减少模型训练时间；Attention 机制每一步的计算都不依赖于上一步的计算结果，因此可以并行处理。</li><li>模型复杂度小，参数少</li></ul><p>但 attention 机制的缺点也比较明显，因为是对序列的所有元素并行处理的，所以无法考虑输入序列的元素顺序，这在自然语言处理任务中比较糟糕。因为在自然语言中，语言的顺序是包含了十分多的信息的，如果缺失了该部分的信息，则得到的结果往往会大大折扣。</p><h3 id="Attention-总结"><a href="#Attention-总结" class="headerlink" title="Attention 总结"></a>Attention 总结</h3><p>简而言之，<strong>Attention 机制就是对输入的每个元素考虑不同的权重参数</strong>，从而更加关注与输入的元素相似的部分，而抑制其它无用的信息。其最大的优势就是能一步到位的考虑全局联系和局部联系，且能并行化计算，这在大数据的环境下尤为重要。同时需要注意的是， Attention 机制作为一种思想，并不是只能依附在 Encoder-Decoder 框架下的，而是可以根据实际情况和多种模型进行结合。</p><p>好的教程可以看：</p><ul><li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37601161">深度学习中的注意力模型</a>，非常详细的讲解了 Attention机制 的形成过程。</p></li><li><p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#soft-vs-hard-attention">注意力？注意力！</a>，叙述了注意力的发明和各种注意力机制和模型，例如 Transformer 和 SNAIL。</p></li><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35571412">浅谈Attention机制的理解</a>，讲了 Attention 的本质。</li><li><a target="_blank" rel="noopener" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a>，关于 model 的可视化</li></ul></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/NLP/">NLP</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/NLP/">NLP</a> <a class="hover-with-bg" href="/tags/Attention/">Attention</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/07/26/Bahdanau%20Attention%EF%BC%9ANeural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Bahdanau Attention：Neural Machine Translation by Jointly Learning to Align and Translate</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/07/05/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN/"><span class="hidden-mobile">循环神经网络 RNN</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>