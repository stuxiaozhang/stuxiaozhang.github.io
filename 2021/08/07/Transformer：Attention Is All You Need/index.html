<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="Transformer：Attention Is All You Need
            实在是看不动了，留一些坑：位置编码还未理解。可以研究一下这篇  Transformer 中的 Positional Encoding一些关于Transformer的问题整理图解 Transformer
          
Transformer 是谷歌在 2017 年发表的论文 attention"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>Transformer：Attention Is All You Need - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="Transformer：Attention Is All You Need"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-08-07 08:55" pubdate>2021年8月7日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 5.1k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 39 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">Transformer：Attention Is All You Need</h1><p class="note note-info">本文最后更新于：2021年9月20日</p><div class="markdown-body"><h1 id="Transformer：Attention-Is-All-You-Need"><a href="#Transformer：Attention-Is-All-You-Need" class="headerlink" title="Transformer：Attention Is All You Need"></a>Transformer：Attention Is All You Need</h1><div class="note note-primary"><p>实在是看不动了，留一些坑：</p><p><strong><em>位置编码还未理解。</em></strong>可以研究一下这篇 <a target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1453/">Transformer 中的 Positional Encoding</a></p><p><a target="_blank" rel="noopener" href="https://www.nowcoder.com/discuss/258321">一些关于Transformer的问题整理</a></p><p><a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-transformer/">图解 Transformer</a></p></div><p>Transformer 是谷歌在 2017 年发表的论文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">attention is all you need</a> 中所提出的用于机器翻译的模型。</p><p>循环模型通常是对输入和输出序列的符号位置进行因子计算。 通过在计算期间将位置与步骤对齐，它们根据前一步的隐藏状态<em>h**t</em>-1和输入产生位置<em>t</em>的隐藏状态序列<em>h**t</em>。这种固有的顺序特性阻碍样本训练的并行化，这在更长的序列长度上变得至关重要，因为有限的内存限制样本的批次大小。在各种任务中，attention机制已经成为序列建模和转导模型不可或缺的一部分，它可以建模依赖关系而不考虑其在输入或输出序列中的距离</p><p>Transformer，这种模型架构避免循环并完全依赖于attention机制来绘制输入和输出之间的全局依赖关系。</p><h2 id="Transformer-Model"><a href="#Transformer-Model" class="headerlink" title="Transformer Model"></a>Transformer Model</h2><p>这是 Transformer 的整体结构。Transformer 模型主要分为两大部分，分别是 <strong>Encoder</strong> 和 <strong>Decoder</strong>。<strong>Encoder</strong> 负责把输入（语言序列）隐射成隐藏层，然后解码器再把隐藏层映射为自然语言序列。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210731221911908.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>如论文中所设置的，编码器由6个编码block组成，同样解码器是6个解码block组成。与所有的生成模型相同的是，编码器的输出会作为解码器的输入，如图所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210801223702902.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p><strong>编码器：</strong>编码器由 <em>N</em> = 6 个完全相同的层堆叠而成。编码器在结构上都是相同的（但它们不共享权重）。每一层又分为两个子层： 每一层都有两个子层。 第一个子层是一个 multi-head self-attention 机制，第二个子层是一个简单的、位置完全连接的前馈网络。 我们对每个子层再采用一个<strong>残差连接</strong>，接着进行<strong>层标准化</strong>。 也就是说，每个子层的输出是 <code>LayerNorm(x + Sublayer(x))</code>，其中 <code>Sublayer(x)</code> 是由子层本身实现的函数。 为了方便这些残差连接，模型中的所有子层以及嵌入层产生的输出维度都为 $d_{model} = 512$。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210801224428779.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p><strong>解码器：</strong> 解码器同样由 <em>N</em> = 6 个完全相同的层堆叠而成。除了每个编码器层中的两个子层之外，解码器还插入第三个子层，该层对编码器堆栈的输出执行 multi-head attention。与编码器类似，我们在每个子层再采用<strong>残差连接</strong>，然后进行<strong>层标准化</strong>。此外，还修改解码器堆栈中的 self-attention 子层，以防止位置关注到后面的位置。这种掩码（<strong><em>Masked</em></strong>）结合将输出嵌入偏移一个位置，确保对位置的预测 <em>i</em> 只能依赖小于 <em>i</em> 的已知输出。解码器具有这两层，但它们之间是一个注意力层（<strong><em>Cross Attention</em></strong>），帮助解码器专注于输入句子的相关部分（类似于 seq2seq 模型中的注意力）。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210801224523731.png" srcset="/img/loading.gif" lazyload alt=""></p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210808092314336.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><h3 id="1-位置嵌入-Positional-Encoding"><a href="#1-位置嵌入-Positional-Encoding" class="headerlink" title="1. 位置嵌入 Positional Encoding"></a>1. 位置嵌入 Positional Encoding</h3><p>由于 Transformer 模型<strong>没有</strong>循环神经网络的迭代操作，所以我们必须提供每个字的位置信息给 Transformer，这样它才能识别出语言中的顺序关系。具体地说，位置嵌入会在词向量中加入了单词的位置信息，这样 Transformer 就能区分不同位置的单词了。</p><p>现在定义一个<strong>位置嵌入</strong>的概念，也就是 Positional Encoding，位置嵌入的维度为 <code>[max_sequence_length, embedding_dimension]</code>, 位置嵌入的维度与词向量的维度是相同的，都是 <code>embedding_dimension</code>。</p><p><code>max_sequence_length</code> 属于超参数，指的是限定每个句子最长由多少个词构成</p><p>注意，我们一般以<strong>字</strong>为单位训练 Transformer 模型。首先初始化字编码的大小为 <code>[vocab_size, embedding_dimension]</code>，<code>vocab_size</code> 为字库中所有字的数量，<code>embedding_dimension</code> 为字向量的维度，对应到 PyTorch 中，其实就是 <code>nn.Embedding(vocab_size, embedding_dimension)</code></p><p>论文中使用了 sin 和 cos 函数的线性变换来提供给模型位置信息:</p><script type="math/tex;mode=display">\begin{array}{c}
P E(p o s, 2 i)=\sin \left(\frac{p o s}{10000^{\frac{2 i}{d_{\text {model }}}}}\right) \\
P E(p o s, 2 i+1)=\cos \left(\frac{\text { pos }}{10000^{\frac{2 i}{d_{\text {model }}}}}\right)
\end{array}</script><p>上式中 $pos$ 指的是一句话中某个字的位置，取值范围是 <code>[0,max_sequence_length)</code>，$i$ 指的是字向量的维度序号，取值范围是 <code>[0,embedding_dimension/2)</code>，$d_{model}$ 指的是 embedding_dimension 的值</p><p>上面有 sin 和 cos 一组公式，也就是对应着 embedding_dimension 维度的一组奇数和偶数的序号的维度，例如 0,1 一组，2,3 一组，分别用上面的 sin 和 cos 函数做处理，从而产生不同的周期性变化，而位置嵌入在 embedding_dimension 维度上随着维度序号增大，周期变化会越来越慢，最终产生一种包含位置信息的纹理，就像论文原文中第六页讲的，位置嵌入函数的周期从 $2π$ 到 $10000∗2π$ 变化，而每一个位置在 embedding_dimension 维度上都会得到不同周期的 sin 和 cos 函数的取值组合，从而产生独一的纹理位置信息，最终使得模型学到位置之间的依赖关系和自然语言的时序特性</p><div class="note note-primary"><p>这块还是不理解，以后还需再学习！(T^T)</p></div><h3 id="2-Self-Attention"><a href="#2-Self-Attention" class="headerlink" title="2. Self-Attention"></a>2. Self-Attention</h3><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210801104634603.png" srcset="/img/loading.gif" lazyload></p><p>从矩阵运算的角度表示 Self-Attention 运作的过程如下：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210801215613269.png" srcset="/img/loading.gif" lazyload></p><p>Self-Attention 的整个计算过程分为以下几步：</p><ol><li>将输入单词转化为嵌入向量 $I$（word embedding）</li><li>根据嵌入向量 $I$ 分别乘上三个权重矩阵 $W_q,W_k,W_v$, 得到 $Q,K, V $ （其中，三个权重矩阵参数是通过学习 data 得到的）</li><li><p>这三个矩阵，接下来 $Q$ 乘上 $K$ 的 $transpose$,得到 $A$，再经过 softmax 归一化（除以 $\sqrt{d_{k}}$）后的 $A’$ 叫做 Attention Matrix，<strong>生成 $Q$ 矩阵就是为了得到 Attention 的 score</strong></p></li><li><p>$A’$ 再乘上 $V$ ，（得到加权的每个输入向量的评分再求和），就得到 $O$，$O$ 就是 Self-attention 这个 layer 的输出</p></li></ol><div class="note note-info"><h4 id="有关-Query，Key，Value"><a href="#有关-Query，Key，Value" class="headerlink" title="有关 Query，Key，Value"></a>有关 Query，Key，Value</h4><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210920094029271.png" srcset="/img/loading.gif" lazyload alt="image-20210920094029271" style="zoom:60%"></p><p>attention函数共有三步完成得到attention value。</p><ul><li>Q与K进行相似度计算得到权值</li><li>对上部权值归一化</li><li>用归一化的权值与V加权求和</li></ul><p>此时加权求和的结果就为注意力值。公式如下:<script type="math/tex">Attention Value = similarity(QK^T)V</script><br>在自然语言任务中，往往K和V是相同的。这时计算出的 attention value 是一个向量，代表序列元素 $x_j$ 的编码向量。此向量中包含了元素 $x_j$ 的上下文关系，即包含全局联系也拥有局部联系。全局联系是因为在求相似度的时候，序列中元素与其他所有元素的相似度计算，然后加权得到了编码向量。局部联系可以这么解释，因为它所计算出的 attention value 是属于当前输入的 $x_j$ 的。这也就是 attention 的强大优势之一，它可以灵活的捕捉长期和local依赖，而且是一步到位的。</p><p><strong>优点：</strong></p><ol><li><p>一步到位的全局联系捕捉</p><p>上文说了一些，attention机制可以灵活的捕捉全局和局部的联系，而且是一步到位的。另一方面从attention函数就可以看出来，它先是进行序列的每一个元素与其他元素的对比，在这个过程中每一个元素间的距离都是一，因此它比时间序列RNNs的一步步递推得到长期依赖关系好的多，越长的序列RNNs捕捉长期依赖关系就越弱。</p></li><li><p>并行计算减少模型训练时间</p><p>Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。但是CNN也只是每次捕捉局部信息，通过层叠来获取全局的联系增强视野。</p></li><li><p>模型复杂度小，参数少<br>模型复杂度是与CNN和RNN同条件下相比较的。</p></li></ol><p><strong>缺点：</strong></p><p>缺点很明显，attention机制不是一个”distance-aware”的，它不能捕捉语序顺序(这里是语序哦，就是元素的顺序)。这在NLP中是比较糟糕的，自然语言的语序是包含太多的信息。如果确实了这方面的信息，结果往往会是打折扣的。</p></div><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><script type="math/tex;mode=display">\operatorname{Self-Attention}(Q, K, V)=\operatorname{softmax}  \left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p>两个最常用的 attention 函数是<u>加法 attention</u> (参考 Bahdanau )和<u>点积（乘法）attention</u>。 除了缩放因子 $\sqrt{d_{k}}$ 之外，点积 attention 与作者提出的 缩放点积 attention 算法相同。 加法 attention 使用具有单个隐藏层的前馈网络计算兼容性函数。 虽然两者在理论上的复杂性相似，但在实践中<strong>点积 attention 的速度更快、更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。</strong></p><p>当 $d_k$ 的值比较小的时候，这两个机制的性能相差相近，当 $d_k$ 比较大时，加法 attention 比 不带缩放的点积attention 性能好。 作者怀疑，对于很大的 $d_k$ 值，点积大幅度增长，将 softmax 函数推向具有极小梯度的区域。 为了抵消这种影响，我们缩小点积 $d_k$ 倍。</p><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210731224918618.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>这篇论文还提出了 Multi-Head Attention 的概念。前面定义的一组权重矩阵 $W_q,W_k,W_v$, 可以找到一个的词相关的词。现在可以定义多组权重，就可以找到不同的相关性。从 $W_q,W_k,W_v$ 变为 $W_q^1,W_k^1,W_v^1$, $W_q^2,W_k^2,W_v^2$ …</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210802194319164.png" srcset="/img/loading.gif" lazyload alt=""></p><h4 id="Attention-在模型中的应用"><a href="#Attention-在模型中的应用" class="headerlink" title="Attention 在模型中的应用"></a>Attention 在模型中的应用</h4><p>Transformer使用以3种方式使用multi-head attention：</p><ul><li>在“编码器—解码器attention”层，query来自上面的解码器层，key和value来自编码器的输出。 这允许解码器中的每个位置能关注到输入序列中的所有位置。 这模仿序列到序列模型中典型的编码器—解码器的attention机制，例如。</li><li>编码器包含self-attention层。 在self-attention层中，所有的key、value和query来自同一个地方，在这里是编码器中前一层的输出。 编码器中的每个位置都可以关注编码器上一层的所有位置。</li><li>类似地，解码器中的self-attention层允许解码器中的每个位置都关注解码器中直到并包括该位置的所有位置。 我们需要防止解码器中的向左信息流来保持自回归属性。 通过屏蔽softmax的输入中所有不合法连接的值（设置为-∞），我们在缩放版的点积attention中实现。</li></ul><h3 id="3-残差连接和-Layer-Normalization"><a href="#3-残差连接和-Layer-Normalization" class="headerlink" title="3. 残差连接和 Layer Normalization"></a>3. 残差连接和 Layer Normalization</h3><p>在上一步得到了经过 Self-Attention 加权之后输出，也就是 $\operatorname{Self-Attention}(Q, K, V)$，然后把他们加起来做残差连接(residual connection)</p><script type="math/tex;mode=display">X_{embedding} + \operatorname{Self-Attention}(Q, K, V)</script><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210807172726267.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p>将 residual 的结果，接着做 Layer Normalization。Layer Normalization 的作用是把神经网络中隐藏层归一为标准正态分布，也就是 $i.i.d$ 独立同分布(independent and identically distributed) ，以起到加快训练速度，加速收敛的作用.</p><p>以矩阵的列 (column) 为单位求<strong>均值</strong>:</p><script type="math/tex;mode=display">\mu_{j}=\frac{1}{m} \sum_{i=1}^{m} x_{i j}</script><p>以矩阵的列（column）为单位求<strong>方差</strong>:</p><script type="math/tex;mode=display">\sigma_{j}^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(x_{i j}-\mu_{j}\right)^{2}</script><p>则 Layer Normalization，即用<strong>每一列</strong>的<strong>每一个元素</strong>减去<strong>这列的均值</strong>，再除以<strong>这列的标准差</strong>，从而得到归一化后的数值，加 $ϵ$ 是为了防止分母为 0</p><script type="math/tex;mode=display">LayerNorm(x)=\frac{x_{i j}-\mu_{j}}{\sqrt{\sigma_{j}^{2}+\epsilon}}</script><div class="note note-info"><h5 id="batch-normalization-与-layer-normalization"><a href="#batch-normalization-与-layer-normalization" class="headerlink" title="batch normalization 与 layer normalization"></a>batch normalization 与 layer normalization</h5><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210807175718882.png" srcset="/img/loading.gif" lazyload style="zoom:80%"></p><p><strong>batch normalization 是对不同 example,不同 feature 的同一个 dimension,去计算 mean 跟 standard deviation</strong></p><p>但 <strong>layer normalization 是对同一个 feature,同一个 example 里面,不同的 dimension ,去计算 mean 跟 standard deviation</strong></p></div><p>下图展示了更多细节：输入 $x_1,x_2$ 经 self-attention 层之后变成 $z_1,z_2$，然后和输入 $x_1,x_2$ 进行残差连接，经过 LayerNorm 后输出给全连接层。全连接层也有一个残差连接和一个 LayerNorm，最后再输出给下一个 Encoder（每个 Encoder Block 中的 FeedForward 层权重都是共享的）</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210807175858497.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><h3 id="4-Transformer-Encoder-整体结构"><a href="#4-Transformer-Encoder-整体结构" class="headerlink" title="4. Transformer Encoder 整体结构"></a>4. Transformer Encoder 整体结构</h3><p>经过上面 3 个步骤，我们已经基本了解了 <strong>Encoder</strong> 的主要构成部分，下面我们用公式把一个 Encoder block 的计算过程整理一下：</p><p>1) 字向量与位置编码:</p><script type="math/tex;mode=display">X=\text { Embedding-Lookup }(X)+\text { Positional-Encoding }</script><p>2) 自注意力机制</p><script type="math/tex;mode=display">Q=\operatorname{Linear}_{q}(X)=X W_{Q} \\
K=\operatorname{Linear}_{k}(X)=X W_{K} \\
V=\operatorname{Linear}_{v}(X)=X W_{V} \\
X_{a t t e n t i o n}=\text { Self-Attention }(Q, K, V)</script><p>3) Self-Attention 残差连接，接着 Layer Normalization</p><script type="math/tex;mode=display">X_{\text {attention }}=X+X_{\text {attention }} \\
X_{\text {attention }}=\text { LayerNorm }\left(X_{\text {attention }}\right)</script><p>4) 这 Encoder block 结构图中的<strong>第 4 部分</strong>，也就是 FeedForward，其实就是两层线性映射并用激活函数激活，比如 ReLU</p><script type="math/tex;mode=display">X_{\text {hidden }}=\text { Linear }\left(\operatorname{ReLU}\left(\operatorname{Linear}\left(X_{\text {attention }}\right)\right)\right)</script><p>5) FeedForward 残差连接与 Layer Normalization</p><script type="math/tex;mode=display">X_{\text {hidden }}=X_{\text {attention }}+X_{\text {hidden }} \\
X_{\text {hidden }}=\text { LayerNorm }\left(X_{\text {hidden }}\right)</script><p>其中</p><script type="math/tex;mode=display">X_{\text {hidden }} \in \mathbb{R}^{\text {batch_size } * \text { seq_len. } * \text { embed_dim }}</script><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>先看一下整体的结构，从上至下分别是：</p><ul><li>Masked Multi-Head Self-Attention</li><li>Multi-Head Encoder-Decoder Attention</li><li>FeedForward Network</li></ul><p>Decoder 和 Encoder 一样，上面三个部分的每一个部分，都有一个残差连接，后接一个 <strong>Layer Normalization</strong>。Decoder 由于其特殊的功能，因此在训练时会涉及到一些细节。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210809113239618.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><h3 id="1-Masked-Self-Attention"><a href="#1-Masked-Self-Attention" class="headerlink" title="1. Masked Self-Attention"></a>1. Masked Self-Attention</h3><p>传统 Seq2Seq 中 Decoder 使用的是 RNN 模型，因此在训练过程中输入 t 时刻的词，模型无论如何也看不到未来时刻的词，因为循环神经网络是时间驱动的，只有当 t 时刻运算结束了，才能看到 t+1 时刻的词。而 Transformer Decoder 抛弃了 RNN，改为 Self-Attention，由此就产生了一个问题，在训练过程中，整个 ground truth 都暴露在 Decoder 中，这显然是不对的，我们需要对 Decoder 的输入进行一些处理，该处理被称为 Mask.</p><p>Masked 就是输出层在计算第 i 个词与其他词的 attention 时，只考虑 i 前面的词，因为 i 后面的词我们不知道是什么，我们需要预测后面的词，如果不掩盖的话，训练的时候相当于模型能看到后面的答案，而我们要求的是， decoder 给定一个开始标志后，一步一步向后计算来预测接下来的单词，所以需要 Mask</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210807151511470.png" srcset="/img/loading.gif" lazyload alt=""></p><p>举个例子，Decoder 的 ground truth 为 “&lt; start&gt; I am fine”，我们将这个句子输入到 Decoder 中，经过 WordEmbedding 和 Positional Encoding 之后，将得到的矩阵做三次线性变换（$W^q,W^k,W^v$）。然后进行 self-attention 操作，首先通过 $\frac{Q K^{T}}{\sqrt{d_{k}}}$ 得到 Scaled Scores，接下来非常关键，我们要对 Scaled Scores 进行 Mask，举个例子，当我们输入 “I” 时，模型目前仅知道包括 “I” 在内之前所有字的信息，即 “&lt; start&gt;” 和 “I” 的信息，不应该让其知道 “I” 之后词的信息。道理很简单，我们做预测的时候是按照顺序一个字一个字的预测，怎么能这个字都没预测完，就已经知道后面字的信息了呢？Mask 非常简单，首先生成一个下三角全 0，上三角全为负无穷的矩阵，然后将其与 Scaled Scores 相加即可</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210810095712339.png" srcset="/img/loading.gif" lazyload alt=""></p><p>之后再做 softmax，就能将 - inf 变为 0，得到的这个矩阵即为每个字之间的权重</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210810095741605.png" srcset="/img/loading.gif" lazyload alt=""></p><h3 id="2-Encoder-Decoder-Attention"><a href="#2-Encoder-Decoder-Attention" class="headerlink" title="2. Encoder-Decoder Attention"></a>2. Encoder-Decoder Attention</h3><p>首先，Encoder 输入一排向量，然后输出一排向量 $a^1,a^2,a^3$。Decoder 输入 BEGIN 这个 Special Token，经过 Masked Self-Attention 后，得到一个向量 $q$。然后 Encoder 这边的 $a^1,a^2,a^3$ 产生 $k^1,k^2,k^3$ 和 $v^1,v^2,v^3$ ，去计算 Attention 的分数，再经过 Softmax 得到 $a_1’,a_2’,a_3’$ ，乘以 $v^1,v^2,v^3$，再把它 Weighted Sum 加起来会得到 v，通过 Fully-Connected Network。这就是 Encoder-Decoder Attention(也叫 Cross Attention)。</p><p>这一部分的计算流程和前面 Masked Self-Attention 很相似，结构也一样，唯一不同的是 <strong>Encoder-Decoder Attention 里的 $K,V$ 为 Encoder 的输出，$Q$ 为 Decoder 中 Masked Self-Attention 的输出</strong></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210809122413314.png" srcset="/img/loading.gif" lazyload alt=""></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><h3 id="Transformer-为什么需要进行-Multi-head-Attention？"><a href="#Transformer-为什么需要进行-Multi-head-Attention？" class="headerlink" title="Transformer 为什么需要进行 Multi-head Attention？"></a>Transformer 为什么需要进行 Multi-head Attention？</h3><p>原论文中说到进行 Multi-head Attention 的原因是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，最后再将各个方面的信息综合起来。其实直观上也可以想到，如果自己设计这样的一个模型，必然也不会只做一次 attention，多次 attention 综合的结果至少能够起到增强模型的作用，也可以类比 CNN 中同时使用<strong>多个卷积核</strong>的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征 / 信息</strong></p><h3 id="Transformer-相比于-RNN-LSTM，有什么优势？为什么？"><a href="#Transformer-相比于-RNN-LSTM，有什么优势？为什么？" class="headerlink" title="Transformer 相比于 RNN/LSTM，有什么优势？为什么？"></a>Transformer 相比于 RNN/LSTM，有什么优势？为什么？</h3><ol><li>RNN 系列的模型，无法并行计算，因为 T 时刻的计算依赖 T-1 时刻的隐层计算结果，而 T-1 时刻的计算依赖 T-2 时刻的隐层计算结果</li><li>Transformer 的特征抽取能力比 RNN 系列的模型要好</li></ol><h3 id="为什么说-Transformer-可以代替-seq2seq？"><a href="#为什么说-Transformer-可以代替-seq2seq？" class="headerlink" title="为什么说 Transformer 可以代替 seq2seq？"></a>为什么说 Transformer 可以代替 seq2seq？</h3><p>这里用代替这个词略显不妥当，seq2seq 虽已老，但始终还是有其用武之地，seq2seq 最大的问题在于<strong>将 Encoder 端的所有信息压缩到一个固定长度的向量中</strong>，并将其作为 Decoder 端首个隐藏状态的输入，来预测 Decoder 端第一个单词 (token) 的隐藏状态。在输入序列比较长的时候，这样做显然会损失 Encoder 端的很多信息，而且这样一股脑的把该固定向量送入 Decoder 端，<strong>Decoder 端不能够关注到其想要关注的信息</strong>。Transformer 不但对 seq2seq 模型这两点缺点有了实质性的改进 (多头交互式 attention 模块)，而且还引入了 self-attention 模块，让源序列和目标序列首先 “自关联” 起来，这样的话，源序列和目标序列自身的 embedding 表示所蕴含的信息更加丰富，而且后续的 FFN 层也增强了模型的表达能力，并且 Transformer 并行计算的能力远远超过了 seq2seq 系列模型</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><blockquote><p>来自李宏毅老师的讲解。希望未来我也可以有这样的疑惑与思考。</p></blockquote><h3 id="1-为什么-transformer-的-encoder-要这么设计？"><a href="#1-为什么-transformer-的-encoder-要这么设计？" class="headerlink" title="1. 为什么 transformer 的 encoder 要这么设计？"></a>1. 为什么 transformer 的 encoder 要这么设计？</h3><p>不一定要这样设计,这个 encoder 的 network 架构,现在设计的方式,本文是按照原始的论文讲给你听的,但<strong>原始论文的设计 不代表它是最好的,最optimal的设计</strong></p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210809125324619.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><ul><li>有一篇文章叫 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.04745">on layer normalization in the transformer architecture</a>，它问的问题就是为什么 layer normalization是放在那个地方呢？为什么我们是先做 residual 再做 layer normalization，能不能够把 layer normalization 放到每一个 block 的 input？也就是说 你做 residual 以后,再做 layer normalization ,再加进去 你可以看到说左边这个图,是原始的 transformer ,右边这个图是稍微把 block 更换一下顺序以后的 transformer。更换一下顺序以后 结果是会比较好的,这就代表说,原始的 transformer 的架构,并不是一个最 optimal 的设计,你永远可以思考看看,有没有更好的设计方式</li></ul><h3 id="2-为什么是-layer-norm？-为什么是别的-不是别的？为什么不做-batch-normalization？"><a href="#2-为什么是-layer-norm？-为什么是别的-不是别的？为什么不做-batch-normalization？" class="headerlink" title="2. 为什么是 layer norm？ 为什么是别的,不是别的？为什么不做 batch normalization？"></a>2. 为什么是 layer norm？ 为什么是别的,不是别的？为什么不做 batch normalization？</h3><p>也许这篇paper可以回答你的问题,这篇paper是 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.07845">Power Norm：,Rethinking Batch Normalization In Transformers</a>,它首先告诉你说 为什么 batch normalization不如,layer normalization. 在 Transformers 里面为什么 batch normalization 不如 layer normalization,接下来在说,它提出来一个 power normalization,一听就是很 power 的意思,都可以比 layer normalization ,还要 performance 差不多或甚至好一点</p><h3 id="3-为什么-Decoder-不管哪一层-都是拿-Encoder-的最后一层的输出？"><a href="#3-为什么-Decoder-不管哪一层-都是拿-Encoder-的最后一层的输出？" class="headerlink" title="3. 为什么 Decoder 不管哪一层,都是拿 Encoder 的最后一层的输出？"></a>3. 为什么 Decoder 不管哪一层,都是拿 Encoder 的最后一层的输出？</h3><p>原始 paper 是这么做的，但是<strong>不一定要这样</strong>。有人尝试不同的 Cross Attension 的方式</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210809125850572.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/NLP/">NLP</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/NLP/">NLP</a> <a class="hover-with-bg" href="/tags/Attention/">Attention</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/08/13/TransE%EF%BC%9ATranslating%20Embeddings%20for%20Modeling%20Multi-relational%20Data/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">TransE：Translating Embeddings for Modeling Multi-relational Data</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/08/01/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/"><span class="hidden-mobile">知识图谱构建技术综述</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>