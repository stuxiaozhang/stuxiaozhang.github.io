<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="【TransR】Learning Entity and Relation Embeddings for Knowledge Graph Completion
TransH 在 TransE 基础上做出改进，提出超平面的想法解决了同一实体的多关系问题，提高了知识表示的效果，在一定程度上解决了复杂关系的处理。然而 TransH 也存在一定的问题。作者发现 TransH 模型虽然有效的处理了复杂语义关"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>【TransR】Learning Entity and Relation Embeddings for Knowledge Graph Completion - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="【TransR】Learning Entity and Relation Embeddings for Knowledge Graph Completion"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-08-22 23:27" pubdate>2021年8月22日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 29 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">【TransR】Learning Entity and Relation Embeddings for Knowledge Graph Completion</h1><p class="note note-info">本文最后更新于：2022年3月4日</p><div class="markdown-body"><h1 id="transrlearning-entity-and-relation-embeddings-for-knowledge-graph-completion">【TransR】Learning Entity and Relation Embeddings for Knowledge Graph Completion</h1><p>TransH 在 TransE 基础上做出改进，提出<strong>超平面</strong>的想法解决了同一实体的多关系问题，提高了知识表示的效果，在一定程度上解决了复杂关系的处理。然而 TransH 也存在一定的问题。作者发现 TransH 模型虽然有效的处理了复杂语义关系表示，但两个实体仍然处于相同的语义空间，因此不能够充分表示实体与关系的语义联系。于是提出了 TransR 使实体和关系处于不同的空间，验证了其能达到不错的效果。</p><h2 id="basic-idea">Basic Idea</h2><p><strong>TransE 和 TransH 都假设实体和关系嵌入在同一个空间 <span class="math inline">\(R^k\)</span> 中。</strong>他们都假设实体和关系是语义空间中的向量，因此相似的实体在同一实体空间中会非常接近。然而，每个实体可以有许多属性，不同的关系关注实体的不同属性。关系和实体是完全不同的对象，在同一个语义空间中可能不足以表示它们。虽然 TransH 使用关系超平面扩展了建模的灵活性, 但它并没有完全打破这种假设。为了解决这个问题，作者让 TransR 在两个不同的空间，即<strong>实体空间</strong>和<strong>多个关系空间</strong>(关系特定的实体空间)中建模实体和关系，并在对应的关系空间 relation space 中进行转换，因此命名为 TransR。</p><blockquote><p>TransR 认为不同的关系拥有不同的语义空间。对每个三元组，首先应将实体投影到对应的关系空间中，然后再建立从头实体到尾实体的平移关系来建模头尾实体在该关系空间中的关联性。</p><p>e.g. 虽然 ‘‘奥巴马’’ 和 ‘‘乔治·布什’’ 这 样的实体在实体空间中可能彼此相距很远，但它们在某些特定关系（如总统）的 空间中是相似且彼此接近的；同样地，对于 ‘‘北京’’ 和 ‘‘上海’’ 这样的实体在实体 空间中可能彼此〸分接近，但它们在某些特定关系（如首都）的空间中是极其不 相似且相距很远的。</p></blockquote><p>TransR 的创新点是将 TransH 的投影到超平面更进一步——投影到空间，<strong>本质是将投影向量换为投影矩阵</strong>，实体还是用一个向量表示，关系用一个向量和一个矩阵表示。效果提升并不大，但计算量显著增大。</p><h2 id="method">Method</h2><p>对于每个三元组 <span class="math inline">\((h,r,t)\)</span>，首先根据当前的关系 <span class="math inline">\(r\)</span> 将头尾实体分别映射到关系空间中 <span class="math inline">\(h_r, t_r\)</span>，然后在关系空间中建模 <span class="math inline">\(h_{r}+r \approx t_{r}\)</span></p><p><strong>特定的关系投影（彩色的圆圈表示）能够使得头/尾实体在这个关系下真实的靠近彼此，使得不具有此关系（彩色的三角形表示）的实体彼此远离。</strong></p><blockquote><p>此外，在特定关系下，头尾实体对通常表现出不同的模式，因此不能单纯的将关系直接与实体对进行操作。根据分段线性回归的思想，我们通过将不同的头尾实体对 聚类到组中,并学习每个组的不同关系向量来扩展 TransR，称为基于聚类的 TransR（CTransR）。</p></blockquote><h3 id="transr">TransR</h3><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210820195325259.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><p>在 TransR 中，对于每个三元组 <span class="math inline">\((h,r,t)\)</span>，实体嵌入被设置为 <span class="math inline">\(\mathbf{h}, \mathbf{t} \in \mathbb{R}^{k}\)</span> 和关系嵌入设置为 <span class="math inline">\(\mathbf{r} \in \mathbb{R}^{d}\)</span>。注意，实体嵌入和关系嵌入的维度不一定相同，即 <span class="math inline">\(k \ne d\)</span>。对于每个关系 <span class="math inline">\(r\)</span>，设置一个投影矩阵 <span class="math inline">\(\mathbf{M}_r\in \mathbb{R}^{k \times d}\)</span>，可以将实体从实体空间投影到关系空间。通过映射矩阵，我们将实体的投影向量定义为： <span class="math display">\[ \mathbf{h}_{r}=\mathbf{h} \mathbf{M}_{r}, \quad \mathbf{t}_{r}=\mathbf{t} \mathbf{M}_{r} \]</span> 得分函数定义为： <span class="math display">\[ f_{r}(h, t)=\left\|\mathbf{h}_{r}+\mathbf{r}-\mathbf{t}_{r}\right\|_{2}^{2} \]</span> 同时对嵌入向量 <span class="math inline">\(h,r,t\)</span> 和映射矩阵用范数做限制： <span class="math display">\[ \|\mathbf{h}\|_{2} \leq 1,\|\mathbf{r}\|_{2} \leq 1,\|\mathbf{t}\|_{2} \leq 1,\left\|\mathbf{h} \mathbf{M}_{\mathbf{r}}\right\|_{2} \leq 1,\left\|\mathbf{t} \mathbf{M}_{\mathbf{r}}\right\|_{2} \leq 1 \]</span></p><blockquote><p>我觉得这个 ”relation space for r“ 应该是关系特定的实体空间。</p></blockquote><h3 id="ctransr">CTransR</h3><p>TransE、TransH 包括 TransR，为每个关系学习一个唯一的向量，该向量可能无法代表该关系下的所有实体对，因为这些关系通常是相当多样的。为了更好地模拟这些关系，引入了分段线性回归（piecewise linear regression）的思想来扩展 TransR。</p><p>基本思想是：</p><ol type="1"><li>首先将输入实例分为几个组。形式上，对于一个特定的关系 <span class="math inline">\(r\)</span>，训练数据中的所有实体对 <span class="math inline">\((h,t)\)</span> 都被聚集到多个簇 cluster 中，每个 簇 cluster 中的实体对都被期望与关系 <span class="math inline">\(r\)</span> 有关。所有实体对 <span class="math inline">\((h,t)\)</span> 都用它们的向量偏移量 <span class="math inline">\((h-t)\)</span> 表示，用于聚类，其中 <span class="math inline">\(h\)</span> 和 <span class="math inline">\(t\)</span> 通过 TransE 获得。</li><li>然后，分别为每个簇学习一个单独的关系向量 <span class="math inline">\(\mathbf{r}_{c}\)</span> 和每个关系学习一个矩阵 <span class="math inline">\(\mathbf{M}_{r}\)</span>。然后将每个簇中的头实体和尾实体映射到对应关系空间中: <span class="math inline">\(\mathbf{h}_{r, c}=\mathbf{h} \mathbf{M}_{r}\)</span> 和 <span class="math inline">\(\mathbf{t}_{r, c}=\mathbf{t} \mathbf{M}_{r}\)</span>，得分函数定义为：</li></ol><p><span class="math display">\[ f_{r}(h, t)=\left\|\mathbf{h}_{r, c}+\mathbf{r}_{c}-\mathbf{t}_{r, c}\right\|_{2}^{2}+\alpha\left\|\mathbf{r}_{c}-\mathbf{r}\right\|_{2}^{2} \]</span></p><p>其中，<span class="math inline">\(||\mathbf{r}_{c}-\mathbf{r}\|_{2}^{2}\)</span> 目的是确保特定于集群的关系向量 <span class="math inline">\(\mathbf{r}_{c}\)</span> 不会离原始关系向量 <span class="math inline">\(\mathbf{r}\)</span> 太远，<span class="math inline">\(α\)</span> 控制该约束的效果。此外，和 TransR 一样，CTransR 也对嵌入 <span class="math inline">\(h、r、t\)</span> 和映射矩阵的范数施加约束。</p><p><strong>TransR 和 CTransR 的区别在于两者的关系空间不同，前者只有一个关系空间，亦即对所有的关系都在同一个空间中；后者则是根据不同的关系，对属于同一个关系的所有实体对聚集在一个簇中，每个关系代表不同的空间</strong>。</p><h3 id="loss-function">Loss Function</h3><p>margin-based score function： <span class="math display">\[ L=\sum_{(h, r, t) \in S} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in S^{\prime}} \max \left(0, f_{r}(h, t)+\gamma-f_{r}\left(h^{\prime}, t^{\prime}\right)\right) \]</span> 其中，<span class="math inline">\(max(x，y)\)</span> 旨在获得 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span> 之间的最大值，<span class="math inline">\(γ\)</span> 是 margin，<span class="math inline">\(S\)</span> 是正确三元组的集合，<span class="math inline">\(S&#39;\)</span> 是错误三元组的集合</p><p>现有的知识图谱只包含正确的三元组。损坏正确的三元组的常规操作(TransH)是替换头实体或尾实体来构造不正确的三元组。当破坏三重实体时，按照 TransH 为头/尾实体替换分配不同的概率。对于那些 1-to-N、N-to-1 和 N-to-N 关系，通过给予更多的机会来替换“一”方，将减少产生 False Negative 实例的机会。在实验中，将传统的抽样方法表示为“unif”，而在（Wang et al.2014）中的新方法表示为“bern”。TransR 和 CTransR 的学习过程使用随机梯度下降（SGD）进行。为了避免过度拟合，我们使用 TransE 的结果初始化实体和关系嵌入，并将关系矩阵初始化为单位矩阵。</p><blockquote><p>TransR 与 TransH 的训练策略和负样本的构建是一样的。</p></blockquote><h2 id="experiments">Experiments</h2><p>在本文中，我们使用了来自 WordNet 的两个数据集，即 WN18 和 WN11。还使用了来自 Freebase 的两个数据集，即 FB15K 和 FB13。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210820221606208.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><h3 id="link-prediction">Link Prediction</h3><p>链接预测旨在预测关系事实三元组 <span class="math inline">\((h,r,t)\)</span> 中缺失的 <span class="math inline">\(h\)</span> 或 <span class="math inline">\(t\)</span>。在这个任务中，对于缺失实体的每个位置，系统被要求从知识图谱中对一组候选实体进行排序，而不是只给出一个最佳结果。在测试阶段，对于每个测试三元组，用知识图谱中的所有实体替换头部/尾部实体，并按照得分函数 <span class="math inline">\(f_r\)</span> 计算的相似性分数降序排列这些实体。接下来，用两个度量作为评估指标：（1）正确实体的平均等级 Mean Rank；（2）前10名实体中正确实体的比例 Hits@10 。 一个好的 link predictor 应该达到更低的 Mean Rank 或 更高的 Hits@10。然而，在替换实体时，可能会出现替换后还正确的三元组，因为是正确的所以排名会较高。因此，在进行排名之前，可以过滤掉出现的这些损坏但正确的三元组。我们将第一个评估设置命名为“Raw”，后一个命名为“Filter”。</p><blockquote><p>TransE、TransH 都是这么做的。</p></blockquote><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210820223659727.png" srcset="/img/loading.gif" lazyload></p><p>WN18 和 FB15K 的评估结果如表2所示。从表中我们观察到：</p><ol type="1"><li>TransR 和 CTransR 显著且一致地优于其他 baseline，包括 TransE 和 TransH。这表明 TransR 在模型复杂性和表达能力之间找到了一个更好的平衡点</li><li>CTransR 的性能优于 TransR，这表明应该建立细粒度模型来处理每种关系类型下的复杂内部相关性。CTransR是一项初步探索，作者今后的工作将是为此目的建立更复杂的模型</li><li>“bern”采样技巧对 TransH 和 TransR 都很有效，特别是在关系类型更多的FB15K上。</li></ol><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210820224054136.png" srcset="/img/loading.gif" lazyload></p><p>在表3中，通过映射 FB15K 上关系1的属性显示了单独的评估结果。我们可以看到：</p><ol type="1"><li>TransR在所有关系映射类别上都取得了巨大的改进，特别是当预测“1对1”关系时，这表明 TransR 为实体和关系及其复杂关联提供了更精确的表示</li><li>预测“1-to-N”和“N-to-1”关系的1，这展现了 TransR 通过关系特定投影区分相关实体和无关实体的能力。</li></ol><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210820224456357.png" srcset="/img/loading.gif" lazyload alt="image-20210820224456357" style="zoom:50%"></p><p>表4给出了 FB15K 训练三元组中“位置包含”关系的一些聚类示例。显然，通过聚类可以学习到更精确、更细粒度的关系嵌入，这有助于进一步提高知识图谱完成的性能。</p><h3 id="triple-classification">Triple Classification</h3><p>三元组分类的目的是判断给定的三元组是否正确。这是一项二分类任务。本任务使用三个数据集WN11、FB13 和 FB15K</p><p>对于三元组分类，我们设置了一个特定于关系的阈值 <span class="math inline">\(δ_r\)</span>。对于三元组 <span class="math inline">\((h,r,t)\)</span>，如果获得的不相似性 <span class="math inline">\(f_r\)</span> 得分低于 <span class="math inline">\(δ_r\)</span>，则三元组将被归类为正，否则为负。<span class="math inline">\(δ_r\)</span> 通过最大化验证集上的分类精度进行优化.</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210820224956476.png" srcset="/img/loading.gif" lazyload alt="image-20210820224956476" style="zoom:50%"></p><p>三元组分类的评估结果如表5所示。可以看到：</p><ol type="1"><li><p>在WN11上，TransR 显著优于baseline，包括 TransE 和 TransH</p></li><li><p>TransE、TransH 和 TransR 都无法超越 FB13 上表现力最强的 NTN model。相比之下，在更大的数据集 FB15K 上，TransE、TransH 和 TransR 的表现要比 NTN 好得多。</p><p>结果可能与数据集的特征相关：FB15K 中有 1345 种关系类型，而 FB13 中只有 13 种关系类型。同时，两个数据集中的实体数量和关系事实非常接近。如（Wang等人，2014）所述，FB13 中的知识图谱比 FB15K 甚至 WN11 更密集。似乎最具表现力的模型 NTN 可以使用张量变换从 FB13 的稠密图中学习复杂的关联。相比之下，更简单的模型能够更好地处理 FB15K 的稀疏图，具有良好的泛化能力</p></li><li><p>“bern” 采样技术提高了 TransE、TransH 和 TransR 在所有三个数据集上的性能。</p></li></ol><p>如（Wang等人，2014）所示，TransE 和 TransH 的训练时间分别约为5分钟和30分钟。TransR 的计算复杂度高于 TransE 和 TransH，训练时间约为3小时。</p><h3 id="relation-extraction-from-text">Relation Extraction from Text</h3><p>关系抽取的目的是从大规模纯文本中提取关系事实，它是丰富知识图谱的重要信息源。大多数现有方法以知识图谱作为 distant supervision，在大规模文本语料库中自动标注句子作为训练实例，然后提取文本特征构建关系分类器。这些方法只使用纯文本来推理新的关系事实；同时，知识图谱嵌入只在已有知识图谱的基础上进行链接预测。</p><p>本节研究 TransR 与基于文本的关系提取模型相结合时的性能。在实验中，作者实现了（Weston et al.2013）中提出的相同的基于文本的提取模型，即 Sm2r。对于知识图谱部分，作者自己生成了一个较小的数据集 FB40K，其中包含 NYT 和 1336 种关系类型中的所有实体。并验证了此数据集的公平性。因此可以安全地使用 FB40K 来证明 TransR 的有效性。按照（Weston et al.2013）中的相同方法，作者将基于文本的关系提取模型的分数与知识图嵌入的分数相结合，对测试三元组进行排序，并得到 TransE、TransH 和 TransR 的 PR 曲线。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210820225013758.png" srcset="/img/loading.gif" lazyload alt="image-20210820225013758" style="zoom:50%"></p><p>从表中我们观察到，当召回范围 <code>[0,0.05]</code> 时，TransR 优于 TransE，与 TransH 相当，当召回范围 <code>[0.05,1]</code> 时，TransR 优于包括 TransE 和 TransH 在内的所有 baseline。最近，嵌入的思想也被广泛用于表示单词和文本，可用于基于文本的关系提取。</p><h2 id="summary">Summary</h2><p>本文提出了一种新的知识图谱嵌入模型 TransR。TransR 将实体和关系嵌入到不同的实体空间和关系空间中，并通过投影实体之间的转换来学习嵌入。此外，作者还提出了 CTransR，其目的是基于 分段线性回归 的思想对每种关系类型内部的复杂相关性进行建模。</p><p>作者认为未来的工作：</p><ul><li>利用推理信息增强图谱的表征；</li><li>可能会探索一种统一的文本和知识图嵌入模型。</li><li>基于 CTransR， 研究更复杂的模型。</li></ul><p>TransR 模型巧妙的借鉴了 TransH 模型的空间投影想法，更细致的将不同的关系作为不同的投影空间，试想一下，每个三元组中的两个实体之所以在同一个三元组，很大程度上是因为两个实体的某些特性符合当前的关系，而这些特性在这个关系所在的语义空间中满足一定的规律，亦即 <span class="math inline">\(h_{r}+r \approx t_{r}\)</span>。</p><p>TransR 还有一些缺点，例如：</p><ul><li><p>将头尾实体通过相同的转换矩阵投影到关系空间，而没有考虑到头尾实体的不同语义类型。例如，<code>(Bill Gates, founder, Microsoft)</code>。<code>'Bill Gate'</code> 是一个人，<code>'Microsoft'</code> 是一个公司，这是两个不同的类别。所以他们应该以不同的方式进行转换。</p></li><li>投影矩阵与实体和关系有关，但投影矩阵仅由关系决定。</li><li><p>TransR 的参数大于 TransE 和 TransH。引入的空间投影策略<strong>增加了计算量</strong>。通过实验可以看到，TransR 及其变体都达到了最优，但这是以巨大的计算量复杂度换来的。根据原文的说法，TransR 的训练时间约是 TransE 的 36 倍、是 TransH 的 6 倍。由于其复杂性，TransR/CTransR 难以应用于大规模知识图谱。</p></li></ul><p>TransD 模型将试图改进这些不足之处。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/NLP/">NLP</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/NLP/">NLP</a> <a class="hover-with-bg" href="/tags/TransR/">TransR</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/08/29/CS224n%2002%20GloVe/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">CS224n 02 GloVe</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/08/20/ConvE%EF%BC%9AConvolutional%202D%20Knowledge%20Graph%20Embeddings/"><span class="hidden-mobile">【ConvE】Convolutional 2D Knowledge Graph Embeddings</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>