<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="CS224n 02 GloVe
上一讲 CS224n 01 Introduction and Word Vectors. 主要介绍了Word2Vec模型，它是一种基于 local context window 的 direct prediction 预测模型，对于学习 word vector，还有另一类模型是 count based global matrix factorization，这一讲"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>CS224n 02 GloVe - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="CS224n 02 GloVe"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-08-29 19:06" pubdate>2021年8月29日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 4.2k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 31 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">CS224n 02 GloVe</h1><p class="note note-info">本文最后更新于：2021年9月8日</p><div class="markdown-body"><h1 id="cs224n-02-glove">CS224n 02 GloVe</h1><p>上一讲 <a href="https://stuxiaozhang.github.io/2021/07/29/CS224n%2001%20Introduction%20and%20Word%20Vectors/">CS224n 01 Introduction and Word Vectors.</a> 主要介绍了Word2Vec模型，它是一种基于 local context window 的 direct prediction 预测模型，对于学习 word vector，还有另一类模型是 count based global matrix factorization，这一讲主要介绍了后一类模型以及 Manning 教授组结合两者优点提出的 <strong>GloVe 模型</strong>。</p><h2 id="基于共现矩阵的词向量-vs.-word2vec词向量">基于共现矩阵的词向量 vs. Word2Vec词向量</h2><p>回顾一下 Word2Vec 的思想：</p><center>让相邻的词的向量表示相似。</center><p>实际上还有一种更加简单的思路——使用<strong>词语共现性</strong>，来构建词向量，也可以达到这样的目的。即，我们直接统计哪些词是经常一起出现的，那么这些词肯定就是相似的。那么，每一个词，都可以做一个这样的统计，得到一个共现矩阵。 这里直接贴一个 CS224n 上的例子：</p><ul><li>I like deep learning.</li><li>I like NLP.</li><li>I enjoy flying.</li></ul><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210829203506781.png" srcset="/img/loading.gif" lazyload style="zoom:50%"></p><p>上面的例子中，给出了三句话，假设这就是我们全部的语料。我们使用一个size=1的窗口，对每句话依次进行滑动，相当于只统计紧邻的词。这样就可以得到一个共现矩阵。</p><p>共现矩阵的每一列，自然可以当做这个词的一个向量表示。这样的表示明显优于one-hot表示，因为它的每一维都有含义——共现次数，因此这样的向量表示可以求词语之间的相似度。</p><p>然后这样表示还有有一些问题：</p><ul><li>维度 = 词汇量大小，还是太大了；</li><li>还是太过于稀疏，在做下游任务的时候依然不够方便。</li></ul><p>但是，维度问题，我们有解决方法——<strong>SVD矩阵分解</strong>！ 我们将巨大的共现矩阵进行SVD分解后，只选取最重要的几个特征值，得到每一个词的低维表示。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210831200135983.png" srcset="/img/loading.gif" lazyload style="zoom:60%"></p><p>图中的 A 在我们的场景中就是共现矩阵，<span class="math inline">\(U、\Sigma、V\)</span>就是分解出的三个矩阵。我们只<strong>选择 U 矩阵的前 r 维来作为词的向量表示</strong>。</p><p>上述的过程使用python编程十分简单，这里也是直接引用cs224n课程中的例子：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210831200549707.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>可见，即使这么简单的三句话构建的语料，我们通过构建共现矩阵、进行SVD降维、可视化，依然呈现出了类似Word2Vec的效果。</p><p>但是，由于共现矩阵巨大，SVD分解的计算代价也是很大的。另外，像a、the、is这种词，与其他词共现的次数太多，也会很影响效果。所以，我们需要使用很多技巧，来改善这样的词向量。例如，直接把一些常见且意义不大的词忽略掉；把极度不平衡的计数压缩到一个范围；使用皮尔森相关系数，来代替共现次数等等很多技巧。</p><p>上面的介绍中，我们发现基于共现矩阵的词向量，也可以表现出很多优秀的性质，它也可以得到一个低维的向量表示，进行相似度的计算，甚至也可以做一定的推理（即存在man to king is like women to queen这样的关系）。 但是，它主要的问题在于两方面：</p><ol type="1"><li>SVD要分解一个巨型的稀疏矩阵（共现矩阵），计算开销大，甚至无法计算；</li><li>需要进行复杂麻烦的预处理，例如计数的规范化、清除常见词、使用皮尔森系数等等。</li></ol><p>而 <strong>Word2Vec</strong> 的算法，<strong>不需要一次性处理这么大量的数据</strong>，而是通过<strong>迭代</strong>的方式，一批一批地进行处理，不断迭代词向量参数，使得我们可以处理海量的语料，构建十分稳健的词向量。所以在实验中，Word2Vec 的表现，一般都要优于传统的SVD类方法。</p><p>但是，<strong>基于共现矩阵的方法也有其优势</strong>，那就是<strong>充分利用了全局的统计信息</strong>。因为我们进行矩阵分解，是对整个共现矩阵进行分解，这个矩阵中包含着全局的信息。而 Word2Vec 由于是一个窗口一个窗口（或几个窗口）地进行参数的更新，所以学到的词向量更多的是局部的信息。</p><p>总之，二者各有优劣，这启发了斯坦福的一群研究者，GloVe 词向量就是在这样的动机下产生的。</p><h2 id="glove-是如何实现的">GloVe 是如何实现的？</h2><p>GloVe 的实现分为以下三步：</p><ol type="1"><li><p>根据语料库（corpus）构建一个共现矩阵（Co-ocurrence Matrix）X，<strong>矩阵中的每一个元素 <span class="math inline">\(X_{ij}\)</span> 代表单词 <span class="math inline">\(i\)</span> 和上下文单词 <span class="math inline">\(j\)</span> 在特定大小的上下文窗口（context window）内共同出现的次数。</strong>一般而言，这个次数的最小单位是 1，但是 GloVe 不这么认为：它根据两个单词在上下文窗口的距离 <span class="math inline">\(d\)</span>，提出了一个衰减函数（decreasing weighting）：<span class="math inline">\(decay=1/d\)</span> 用于计算权重，也就是说<strong>距离越远的两个单词所占总计数（total count）的权重越小</strong>。</p><blockquote><p><strong>共现count矩阵：</strong>里面的数值都是出现的次数，即 count，具体步骤如下：</p><ol type="1"><li>首先构建一个空矩阵，大小为 <span class="math inline">\(V\)</span>，即词汇表*词汇表，值全为空</li><li>确定一个滑动窗口的大小，比如：5</li><li>从语料库的第一个单词开始，滑动该窗口</li><li>因为是按照语料库的顺序开始的，所以中心词为到达的那个单词：<span class="math inline">\(i\)</span></li><li>上下文词为以中心词 <span class="math inline">\(i\)</span> 为滑动窗口的中心，两边的单词，为上下文词：<span class="math inline">\(j_1, j_2,…,j_m\)</span>（假如有m个j）</li><li>若窗口左右无单词，一般出现在语料库的首尾，则空着，不需要统计</li><li>在窗口内，统计上下文词j出现的次数，添在矩阵中</li><li>滑动过程中，不断将统计的信息添加在矩阵中，即可得到共现矩阵</li></ol></blockquote></li><li><p>构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系： <span class="math display">\[ w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} = \log(X_{ij}) \tag{1} \]</span> 其中，<strong><span class="math inline">\(w_i^T\)</span> 和 <span class="math inline">\(\tilde w_j\)</span> 是我们最终要求解的词向量；</strong><span class="math inline">\(b_i\)</span> 和 <span class="math inline">\(\tilde b_j\)</span> 分别是两个词向量的 bias term；</p><p>为什么要使用这个公式，为什么要构造两个词向量 <span class="math inline">\(w_i^T\)</span> 和 <span class="math inline">\(\tilde w_j\)</span>？下文会详细介绍。</p></li><li><p>有了公式1之后我们就可以构造它的 loss function： <span class="math display">\[ J=\sum_{i, j=1}^{V} f\left(X_{i j}\right)\left(w_{i}^{T} \tilde{w}_{j}+b_{i}+\tilde{b_{j}}-\log \left(X_{i j}\right)\right)^{2} \]</span> 这个 loss function 的基本形式就是最简单的 mean square loss，只不过在此基础上加了一个权重函数 <span class="math inline">\(f(X_{ij})\)</span>，那么这个函数起了什么作用，为什么要添加这个函数呢？我们知道在一个语料库中，肯定存在很多单词他们在一起出现的次数是很多的（frequent co-occurrences），那么我们希望：</p><ol type="1"><li>这些单词的权重要大于那些很少在一起出现的单词（rare co-occurrences），所以这个函数要是非递减函数（non-decreasing）</li><li>但我们也不希望这个权重过大（overweighted），当到达一定程度之后应该不再增加</li><li>如果两个单词没有在一起出现，也就是 <span class="math inline">\(X_{ij}=0\)</span>，那么他们应该不参与到 loss function 的计算当中去，也就是 <span class="math inline">\(f(x)\)</span> 要满足 <span class="math inline">\(f(0)=0\)</span></li></ol><p>满足以上两个条件的函数有很多，作者采用了如下形式的分段函数： $$ f(x)=<span class="math display">\[\begin{equation} \begin{cases} (x/x_{max})^{\alpha} &amp; \text{if} \ x &lt; x_{max} \\ 1 &amp; \text{otherwise} \end{cases} \end{equation}\]</span> $$ 这个函数图像如下所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210831131845208.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p></li></ol><p>这篇论文中的所有实验，α 的取值都是 0.75，而 <span class="math inline">\(x_{max}\)</span> 取值都是100。以上就是 GloVe 的实现细节，那么 GloVe 是如何训练的呢？</p><h2 id="glove-是如何训练的">GloVe 是如何训练的？</h2><p>虽然很多人称 GloVe 是一种无监督（unsupervised learing）的学习方式（因为它确实不需要人工标注label），但其实它还是有 label 的，这个 label 就是公式2中的 <span class="math inline">\(\log(X_{ij})\)</span>，而公式2中的向量 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(\tilde{w}\)</span> 就是要不断更新/学习的参数，所以本质上它的训练方式跟监督学习的训练方法没什么不一样，都是基于梯度下降的。</p><p>具体地，这篇论文里的实验是这么做的：<strong>采用了 AdaGrad 的梯度下降算法，对矩阵 <span class="math inline">\(X\)</span> 中的所有非零元素进行随机采样，学习曲率（learning rate）设为0.05，在 vector size 小于300的情况下迭代了50次，其他大小的 vectors 上迭代了100次，直至收敛。</strong>最终学习得到的是两个 vector 是 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(\tilde{w}\)</span>，因为 <span class="math inline">\(X\)</span> 是对称的（symmetric），所以从原理上讲 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(\tilde{w}\)</span> 是也是对称的，他们唯一的区别是初始化的值不一样，而导致最终的值不一样。所以这两者其实是等价的，都可以当成最终的结果来使用。<strong>但是为了提高鲁棒性，我们最终会选择两者之和作为最终的 vector（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）。</strong>在训练了400亿个token组成的语料后，得到的实验结果如下图所示：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210831132316240.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>这个图一共采用了三个指标：语义准确度，语法准确度以及总体准确度。那么我们不难发现 Vector Dimension 在300时能达到最佳，而 context Windows size 大致在6到10之间。</p><h2 id="glove-与-lsaword2vec-的比较">Glove 与 LSA、word2vec 的比较</h2><h3 id="glove-与-lsa-比较">Glove 与 LSA 比较：</h3><ul><li>两者都是基于共现矩阵在操作</li><li>LSA（Latent Semantic Analysis）可以基于 co-occurance matrix 构建词向量，实质上是基于全局语料采用 SVD 进行矩阵分解，然而 SVD 计算复杂度高，计算代价比较大。还有一点是它对所有单词的统计权重都是一致的。</li><li>Glove 没有直接利用共现矩阵，而是通过 ratio 的特性，将词向量和 ratio 联系起来，建立损失函数，采用 Adagrad 对最小平方损失进行优化（可看作是对 LSA 一种优化的高效矩阵分解算法）</li></ul><h3 id="glove-与-word2vec-比较">Glove 与 Word2vec 比较：</h3><ul><li>Word2vec 是局部语料库训练的，其特征提取是基于滑窗的；而 Glove 的滑窗是为了构建 co-occurance matrix（上面详细描述了窗口滑动的过程），统计了全部语料库里在固定窗口内的词共线的频次，是基于全局语料的，可见 Glove 需要事先统计共现概率；因此，Word2vec 可以进行在线学习，Glove 则需要统计固定语料信息。</li><li>Word2vec 是无监督学习。Glove 通常被认为是无监督学习，但实际上 Glove 还是有 label 的，在统计共现矩阵信息的时候，需要对语料进行标注，即共现次数 <span class="math inline">\(log(X_i,j)\)</span></li><li>Word2vec 损失函数实质上是带权重的交叉熵，权重固定；Glove 的损失函数是最小平方损失函数，权重可以做映射变换。</li><li>Glove利用了全局信息，使其在训练时收敛更快，训练周期较word2vec较短且效果更好。</li></ul><p>虽然 GloVe 的作者在原论文中说 GloVe 结合了 SVD 与 Word2Vec 的优势，训练速度快并且在各项任务中性能优于 Word2Vec，但是我们应该持有怀疑的态度看待这一结果，可能作者在比较结果时对于 GloVe 模型参数选择较为精细而 Word2Vec 参数较为粗糙导致 GloVe 性能较好，或者换另一个数据集，改换样本数量，两者的性能又会有不同。实际上，在另一篇论文 <a href="https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/D15-1036">Evaluation methods for unsupervised word embeddings</a> 中基于各种 intrinsic 和 extrinsic 任务的性能比较中，Word2Vec 结果要优于或不亚于 GloVe。实际应用中也是 Word2Vec 被采用的更多，对于新的任务，不妨对各种 embedding 方法都做尝试，选择合适自己问题的方法。</p><h2 id="公式推导">公式推导</h2><blockquote><p>一个疑惑就是公式1到底是怎么来. 即使记录了下面的推导我还是很糊涂emm</p></blockquote><p>先定义一些变量：</p><ul><li>对于共现矩阵 <span class="math inline">\(X\)</span>，<span class="math inline">\(X_{ij}\)</span> 代表了单词 <span class="math inline">\(j\)</span> 出现在单词 <span class="math inline">\(i\)</span> 的上下文中的次数</li><li><span class="math inline">\(X_i\)</span> 代表单词 <span class="math inline">\(i\)</span> 的上下文中所有单词出现的总次数，即 <span class="math inline">\(X_{i}=\sum_{k} X_{i k}\)</span></li><li><span class="math inline">\(P_{i j}=P(j \mid i)=X_{i j} / X_{i}\)</span> ，代表单词 <span class="math inline">\(j\)</span> 出现在单词 <span class="math inline">\(i\)</span> 上下文中的概率。</li></ul><p>有了这些定义之后，我们来看一个表格：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20210831143340274.png" srcset="/img/loading.gif" lazyload style="zoom:67%"></p><p>理解这个表格的重点在最后一行，它表示的是两个概率的比值（ratio），<strong>我们可以使用它观察出两个单词 <span class="math inline">\(i\)</span> 和 <span class="math inline">\(j\)</span> 相对于单词 <span class="math inline">\(k\)</span> 哪个更相关（relevant）。</strong>比如，ice 和 solid 更相关，而 stream 和 solid 明显不相关，于是我们会发现 <span class="math inline">\(P(solid|ice)/P(solid|steam)\)</span> 比1大更多。同样的 gas 和 steam 更相关，而和 ice 不相关，那么 <span class="math inline">\(P(gas|ice)/P(gas|steam)\)</span> 就远小于1；当都有关（比如water）或者都没有关(fashion)的时候，两者的比例接近于1；这个是很直观的。因此，<strong>以上推断可以说明通过概率的比例而不是概率本身去学习词向量可能是一个更恰当的方法</strong>，因此下文所有内容都围绕这一点展开。</p><p>于是为了捕捉上面提到的概率比例，我们可以构造如下函数： <span class="math display">\[ F(w_i,w_j,\tilde{w_k}) = \frac{P_{ik}}{P_{jk}},\tag{4} \]</span> 其中，函数 F 的参数和具体形式未定，它有三个参数 <span class="math inline">\(w_i,w_j\)</span> 和 <span class="math inline">\(\tilde{w_k}\)</span>， <span class="math inline">\(w\)</span> 表示的是一个词向量，<span class="math inline">\(\tilde{w_k}\)</span> 表示其在 context_windows内的某个词。显然我们希望这个 F 能表示上述比率的关系。 因为向量空间是线性结构的，所以要表达出两个概率的比例差，最简单的办法是作差，于是我们得到： <span class="math display">\[ F(w_i-w_j,\tilde{w_k}) = \frac{P_{ik}}{P_{jk}},\tag{5} \]</span> 使其仅仅依赖两目标向量的不同之处。这时我们发现公式5的右侧是一个数量，而左侧则是一个向量，为了保证 F 是个线性结构，于是我们把左侧转换成两个向量的内积形式： <span class="math display">\[ F((w_i-w_j)^T \tilde{w_k}) = \frac{P_{ik}}{P_{jk}},\tag{6} \]</span> 这样就避免了过多的维度计算，回归到简单的线性关系。</p><p>我们注意到 <span class="math inline">\(w_i\)</span> 到 <span class="math inline">\(w_j\)</span> 的距离与 <span class="math inline">\(w_j\)</span> 到 <span class="math inline">\(w_i\)</span> 的距离是相等的，并且共现矩阵是一个对称的矩阵，即 <span class="math inline">\(X^T==X\)</span>，我们把 <span class="math inline">\(w_i\)</span> 称为主单词， <span class="math inline">\(\tilde w\)</span> 称为 <span class="math inline">\(w_i\)</span> 的上下文的某个单词，从某种角度看， <span class="math inline">\(w_i\)</span> 与 <span class="math inline">\(\tilde w\)</span> 的角色是可以互换的，它们的地位是相等的，那么我们就希望模型 F 能隐含这种特性。再看看上式： <span class="math display">\[ F((w_{i}-w_{j})^{T}*{\tilde{w}_{k}}) = \frac{P_{ik}}{P_{jk}}\neq \frac{P_{ki}}{P_{kj}} \]</span> 于是论文中这样做的： <span class="math display">\[ F((w_{i}-w_{j})^{T}*{\tilde{w}_{k}}) ＝ \frac{F(w_i^T\tilde{w}_{k})}{F(w_j^T\tilde{w}_{k})}== \frac{F(\tilde{w}_{k}^Tw_i)}{F(\tilde{w}_{k}^Tw_j)} \]</span> 上式中 <span class="math display">\[ F((w_{i}-w_{j})^{T}*{\tilde{w}_{k}}) ＝F(w_i^T*\tilde{w}_{k}-w_j^T*\tilde{w}_{k}) =\frac{F(w_i^T\tilde{w}_{k})}{F(w_j^T\tilde{w}_{k})} \]</span> 显然 <span class="math inline">\(exp\)</span> 函数具有这个特性，再有： <span class="math display">\[ F(w_i^T \tilde{w_k}) = P_{ik} = \frac{X_{ik}}{X_i},\tag{8} \]</span> 于是我们有： <span class="math display">\[ w_i^T \tilde{w_k} = \log(P_{ik}) = \log(X_{ik}) – \log({X_i}),\tag{9} \]</span> 此时，我们发现因为等号右侧的 $log⁡(X_i) $ 的存在，公式9是不满足对称性（symmetry）的，而且这个 $log⁡(X_i) $ 其实是跟 <span class="math inline">\(k\)</span> 独立的，它只跟 <span class="math inline">\(i\)</span> 有关，于是我们可以针对 <span class="math inline">\(w_i\)</span> 增加一个 bias term <span class="math inline">\(b_i\)</span> 把它替换掉，于是我们有： <span class="math display">\[ w_i^T \tilde{w_k} + b_i= \log(X_{ik}), \tag{10} \]</span> 但是公式10还是不满足对称性，于是我们针对 <span class="math inline">\(w_k\)</span> 增加一个bias term <span class="math inline">\(b_k\)</span>，从而得到公式1的形式：</p><p><span class="math display">\[ w_i^T \tilde{w_k} + b_i + b_k= \log(X_{ik}), \tag{1} \]</span> 以上内容其实不能完全称之为推导，因为有很多不严谨的地方，只能说是解释作者如何一步一步构造出这个公式的，仅此而已。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/CS224n/">CS224n</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/NLP/">NLP</a> <a class="hover-with-bg" href="/tags/GloVe/">GloVe</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/09/07/CS224n%2003%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">CS224n 03 神经网络</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/08/22/%E3%80%90TransR%E3%80%91Learning%20Entity%20and%20Relation%20Embeddings%20for%20Knowledge%20Graph/"><span class="hidden-mobile">【TransR】Learning Entity and Relation Embeddings for Knowledge Graph Completion</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>