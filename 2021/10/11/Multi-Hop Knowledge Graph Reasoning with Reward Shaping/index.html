<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="Multi-Hop Knowledge Graph Reasoning with Reward Shaping
该论文基于上篇 MINERVA（Das ICLR 2018）指出了强化学习在 incomplete knowledge graph 中进行 reasoning 时存在的两个问题，

虚假负reward：查询出的路径由于知识图谱中的信息缺失导致判定为错误路径
虚假路径：由假路径导致"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>Multi-Hop Knowledge Graph Reasoning with Reward Shaping - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="Multi-Hop Knowledge Graph Reasoning with Reward Shaping"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-10-11 07:50" pubdate>2021年10月11日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.4k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 12 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">Multi-Hop Knowledge Graph Reasoning with Reward Shaping</h1><p class="note note-info">本文最后更新于：2021年10月12日</p><div class="markdown-body"><h1 id="multi-hop-knowledge-graph-reasoning-with-reward-shaping">Multi-Hop Knowledge Graph Reasoning with Reward Shaping</h1><p>该论文基于上篇 MINERVA（Das ICLR 2018）指出了强化学习在 incomplete knowledge graph 中进行 reasoning 时存在的两个问题，</p><ol type="1"><li>虚假负reward：查询出的路径由于知识图谱中的信息缺失导致判定为错误路径</li><li>虚假路径：由假路径导致的偶然正确答案。由于强化学习是一种基于策略的算法，鼓励过去的行动，并反馈回较高的reward，因此会更偏向初期存在的虚假正确的情况。(false positive path)</li></ol><p>针对以上问题,本文提出解决方案：Reward Shaping 和 Action dropout</p><h2 id="background">Background</h2><p>多跳推理是解决 incomplete knowledge graph 问题的一种有效方法。该问题可以在强化学习（RL）设置中表述，其中基于 policy 的代理顺序扩展其推理路径，直到到达目标。然而，在一个不完整的 KG 环境中，代理会收到训练数据中的 false negatives 所破坏造成的低质量奖励，这会损害测试时的泛化。此外，由于没有正确动作序列用于训练，代理可能会被虚假的搜索轨迹误导，从而意外地导致正确答案(false positive path)。</p><p>最近的工作 MINERVA 将多跳推理表述为一个顺序决策问题，并利用强化学习（RL）执行有效的路径搜索。MINERVA 使用强化算法训练多跳 KG query answering 的端到端模型：给定查询关系和源实体，经过训练的代理从源开始搜索 KG，并在不访问任何预计算路径的情况下到达候选答案。</p><h2 id="model">Model</h2><h3 id="rl-formulation">RL Formulation</h3><p>使用马尔科夫随机过程(Markov Decision Process)：</p><ul><li><p><strong>States.</strong> <span class="math inline">\(s_{t}=\left(e_{t},\left(e_{s}, r_{q}\right)\right) \in \mathcal{S}\)</span> ，<span class="math inline">\(e_t\)</span> 是 step t 下的实体，<span class="math inline">\((e_{s}, r_{q})\)</span> 是原实体和查询关系。</p></li><li><p><strong>Actions.</strong> <span class="math inline">\(A_{t}=\left\{\left(r^{\prime}, e^{\prime}\right) \mid\left(e_{t}, r^{\prime}, e^{\prime}\right) \in \mathcal{G}\right\}\)</span> ，<span class="math inline">\(e_t\)</span> 的出边</p></li><li><p><strong>Transition.</strong> <span class="math inline">\(\delta\left(s_{t}, A_{t}\right)=\delta\left(e_{t},\left(e_{s}, r_{q}\right), A_{t}\right)\)</span></p></li><li><p><strong>Rewards.</strong> 在默认公式中，如果代理在搜索结束时到达正确的目标实体，则代理将收到1的最终奖励，否则为0。</p></li></ul><p><span class="math display">\[ R_{b}\left(s_{T}\right)=\mathbb{1}\left\{\left(e_{s}, r_{q}, e_{T}\right) \in \mathcal{G}\right\} \]</span></p><h3 id="policy-network">Policy Network</h3><p>搜索策略使用 state 信息和 全局上下文 以及 搜索历史记录 <span class="math inline">\(h_t\)</span> 进行参数化.</p><p>搜索历史记录 <span class="math inline">\(h_t\)</span> 由步骤 t 之前采取的行动序列 $ h_t = (e_{s}, r_{1}, e_{1}, , r_{t}, e_{t}) $ 组成，用 LSTM 进行编码: <span class="math display">\[ \begin{aligned} \mathbf{h}_{0} &amp;=\operatorname{LSTM}\left(\mathbf{0},\left[\mathbf{r}_{0} ; \mathbf{e}_{s}\right]\right) \\ \mathbf{h}_{t} &amp;=\operatorname{LSTM}\left(\mathbf{h}_{t-1}, \mathbf{a}_{t-1}\right), t&gt;0 \end{aligned} \]</span> 其中，<span class="math inline">\(r_0\)</span> 是从 <span class="math inline">\(e_s\)</span> 开始的关系，<span class="math inline">\(a_t = (t_{t+1}, e_{t+1})\)</span></p><p>Policy Network 定义为： <span class="math display">\[ \pi_{\theta}\left(a_{t} \mid s_{t}\right)=\sigma\left(\mathbf{A}_{t} \times W_{2} \operatorname{ReLU}\left(W_{1}\left[\mathbf{e}_{t} ; \mathbf{h}_{t} ; \mathbf{r}_{q}\right]\right)\right) \]</span> 其中，动作空间 <span class="math inline">\(A_t\)</span> 是通过叠加所有动作的嵌入来编码的。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20211012135638682.png" srcset="/img/loading.gif" lazyload></p><h3 id="knowledge-based-reward-shaping">Knowledge-Based Reward Shaping</h3><p>MINERVA 中的奖励函数就是，正确奖励1，其余为0. 然而 G 本质上是不完整的，这种方法惩罚 false negative 搜索的方式与 true negatives 相同。 <span class="math display">\[ R\left(s_{T}\right)=R_{b}\left(s_{T}\right)+\left(1-R_{b}\left(s_{T}\right)\right) f\left(e_{s}, r_{q}, e_{T}\right) \]</span></p><h3 id="optimization">Optimization</h3><p>Policy Network 通过最大化预期 reward 进行训练。其实我觉得就是目标函数。 <span class="math display">\[ J(\theta)=\mathbb{E}_{\left(e_{s}, r, e_{o}\right) \in \mathcal{G}}\left[\mathbb{E}_{a_{1}, \ldots, a_{T} \sim \pi_{\theta}}\left[R\left(s_{T} \mid e_{s}, r\right)\right]\right] \]</span> 用 SGD 更新参数 <span class="math inline">\(\theta\)</span>： <span class="math display">\[ \nabla_{\theta} J(\theta) \approx \nabla_{\theta} \sum_{t=1}^{T} R\left(s_{T} \mid e_{s}, r\right) \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) \]</span></p><h3 id="action-dropout">Action dropout</h3><blockquote><p>一般情况下，虚假路径多于正确路径，因此通常首先发现虚假路径，随后的探索可能会越来越偏向于这些路径。具有较大扇入（入度）和扇出（出度）的实体，这种现象更严重。</p></blockquote><p>为了避免虚假路径误导模型，模型根据状态计算当前动作的概率，加入随机的伯努利分布，从而使得部分动作被屏蔽，公式中用于平滑当动作被屏蔽时的概率分布，使得被误导的模型也能有机会找到正确的滤镜加以训练。 <span class="math display">\[ \begin{array}{l} \tilde{\pi}_{\theta}\left(a_{t} \mid s_{t}\right) \propto\left(\pi_{\theta}\left(a_{t} \mid s_{t}\right) \cdot \mathbf{m}+\epsilon\right) \\ m_{i} \sim \operatorname{Bernoulli}(1-\alpha), i=1, \ldots\left|A_{t}\right| \end{array} \]</span> 其中，<span class="math inline">\(\mathbf{m} \in\{0,1\}^{\left|A_{t}\right|}\)</span> 是一个从伯努利分布中采样的二元变量，参数为 <span class="math inline">\(1-α\)</span>。 在 m=0 的情况下，使用一个小值 <span class="math inline">\(ϵ\)</span> 来平滑分布，其中 <span class="math inline">\(\tilde{\pi}_{\theta}\left(a_{t} \mid s_{t}\right)\)</span> 变得均匀。</p><h2 id="summary">Summary</h2><p>为了解决 MINERVA，作者在其基础上提出了两个 RL 方法进行改进：</p><ol type="1"><li>Reward Shaping：通过采用 pretrained one-hop embedding model，对任意三元组有一个预期奖励，有效避免训练时 False Negatives 对测试效果造成的影响。</li><li>Action dropout：对每一个路径上的中间实体，随机对其出边进行dropout，以强制有效探索不同的路径集，并稀释虚假路径的负面影响。</li></ol></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/NLP/">NLP</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/NLP/">NLP</a> <a class="hover-with-bg" href="/tags/KGR/">KGR</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/2021/10/08/Go%20for%20a%20Walk%20and%20Arrive%20at%20the%20Answer%EF%BC%9AReasoning%20Over%20Paths%20in%20Knowledge%20Bases%20using%20Reinforcement%20Learning/"><span class="hidden-mobile">Go for a Walk and Arrive at the Answer：Reasoning Over Paths in Knowledge Bases using Reinforcement Learning</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>