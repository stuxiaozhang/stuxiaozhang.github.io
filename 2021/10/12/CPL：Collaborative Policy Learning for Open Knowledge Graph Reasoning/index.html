<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="CPL：Collaborative Policy Learning for Open Knowledge Graph Reasoning
CPL 是一篇发表在 EMNLP 2019 上的文章. 提出了一个强化学习框架 CPL 联合了两个 agent：multi-hop graph reasoner 和 fact extractor.
Basic Idea
开放知识图谱推理 OKGR 是一项旨"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>CPL：Collaborative Policy Learning for Open Knowledge Graph Reasoning - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="CPL：Collaborative Policy Learning for Open Knowledge Graph Reasoning"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-10-12 22:31" pubdate>2021年10月12日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2.8k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 23 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">CPL：Collaborative Policy Learning for Open Knowledge Graph Reasoning</h1><p class="note note-info">本文最后更新于：2021年10月31日</p><div class="markdown-body"><h1 id="cplcollaborative-policy-learning-for-open-knowledge-graph-reasoning">CPL：Collaborative Policy Learning for Open Knowledge Graph Reasoning</h1><p><a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/D19-1269.pdf">CPL</a> 是一篇发表在 EMNLP 2019 上的文章. 提出了一个强化学习框架 CPL 联合了两个 agent：multi-hop graph reasoner 和 fact extractor.</p><h2 id="basic-idea">Basic Idea</h2><p><u>开放知识图谱推理 OKGR 是一项旨在通过背景文本语料库对图中缺失的事实进行推理的任务。</u>该任务的一个关键挑战是过滤掉从语料库中抽取的“无关”事实，以便在路径推理过程中保持有效的搜索空间。本文提出了一个新的强化学习框架 CPL 来联合训练两个 collaborative agent，即 a multi-hop graph reasoner and a fact extractor。fact extractor agent 从语料库生成事实三元组，以动态地丰富 graph；而 reasoning agent 则向 fact extractor 提供反馈，并引导其提升有助于解释性推理的事实。</p><h2 id="background">Background</h2><blockquote><p>目前这些推理方法将链接推理任务定义为图上的路径查找问题。</p></blockquote><p>目前的推理方法面临以下两个主要挑战：</p><ol type="1"><li>它们的性能往往对图的稀疏性和完整性敏感，缺少边（即潜在的 false negatives）使得寻找到达目标实体的证据路径变得更加困难。</li><li>现有模型假设 graph 是静态的，并且不能适应 不断添加新事实的动态丰富的 graph 。此外，向图中添加大量边将创建无效的搜索空间，并导致路径查找模型的可伸缩性问题。</li></ol><p>因此，需要设计一种能够过滤掉不相关事实的方法来扩充推理模型。</p><blockquote><p>通常，KGR 方法通过对找到的路径进行排序来生成多个候选答案，而传统的 KG 完成方法通过穷举枚举对所有可能的答案三元组进行排序。</p></blockquote><h2 id="some-defination-about-mdp">Some Defination About MDP</h2><ul><li><span class="math inline">\(G\)</span>：三元组集合</li><li><em>background corpus</em>：背景语料库是 标有各自实体对的句子 的集合。<span class="math inline">\(C = \left\{\left(s_{i}:\left(e_{k}, e_{j}\right)\right) \mid s_{i} \in S, e_{k}, e_{j} \in E\right\}\)</span> ，S 是句子集，语料库与 G 共享同一实体集</li></ul><p>CPL 联合训练两个 agent。给定一个查询 <span class="math inline">\((e_s,r_q,e_q)\)</span>，reasoning agent 试图通过在（扩充的） G 上找到一条推理路径来推断 <span class="math inline">\(e_q\)</span>，而 fact extraction agent 则旨在从 C 中选择信息量最大的事实来动态丰富 G。有了这样一个抽取器，框架可以有效地克服<strong>边缘稀疏问题</strong>，同时保持合理的效率（与将所有可能的事实添加到 G 中的 naive 解决方案相比）。通过根据 reasoning agent 的性能对 fact extraction agent 进行奖励来训练 fact extraction agent 。因此， fact extraction agent 可以学习如何抽取信息量最大的事实，以利于推理。</p><h3 id="graph-reasoning-agent">Graph Reasoning Agent</h3><p><strong>推理代理的目标是通过 KG 上的路径查找学习推理。</strong></p><p>MDP 定义：</p><p><strong>State.</strong>：<span class="math inline">\(s_{R}^{t}=\left(e_{s}, r_{q}, h^{t}\right) \in \mathcal{S}_{R}\)</span> ，<span class="math inline">\(h_t\)</span> 是历史路径，<span class="math inline">\((e_s,r_q)\)</span> 是所有状态共享的 context。用 LSTM 编码历史路径： <span class="math display">\[ h^{t}=\operatorname{LSTM}\left(h^{t-1},\left[r^{t}, e^{t}\right]\right) \]</span> 其中，<span class="math inline">\(e^t\)</span> 是当前推理位置，<span class="math inline">\(r^t\)</span> 是前一个连接 <span class="math inline">\(e^t\)</span> 的关系</p><p><strong>Action.</strong> <span class="math inline">\(e^t\)</span> 的出边。其实就是 <span class="math inline">\(G \cup C\)</span>，下文会提.</p><p><strong>Transtion.</strong> 转移函数 <span class="math inline">\(f(s_{R}^{t}, a_{R}^{t})=(e^{s}, r^{q}, h^{t+1})\)</span></p><p><strong>Reward.</strong> 只有当它到达正确的目标实体时，才会收到终端奖励1，否则为0。所有中间状态始终获得奖励0。</p><h3 id="fact-extraction-agent">Fact Extraction Agent</h3><p><strong>事实抽取代理学习在推理代理的当前推理步骤中，提出最相关的事实。</strong>即，推理代理在时间 t 到达图中实体 <span class="math inline">\(e^t\)</span>，事实抽取代理将从语料库中抽取 <span class="math inline">\(\left(e^{t}, r^{\prime}, e^{\prime}\right) \notin G\)</span> ，并将它们临时添加到图表中。</p><p><strong>State.</strong> 找 <span class="math inline">\(e_t\)</span> 的出边。<span class="math inline">\(b_{e^t}\)</span> 是标有 <span class="math inline">\((e^t,e&#39;)\)</span> 的<font color="red">句子袋?</font>，定义事实抽取代理的状态 <span class="math inline">\(s_{E}^{t}=\left(b_{e^{t} }, e^{t}\right) \in \mathcal{S}_{E}\)</span>，<span class="math inline">\(\mathcal{S}_{E}\)</span> 是整个 state space。</p><p><strong>Action.</strong> 事实抽取器的目标是从语义上选择语料库中包含的与推理相关的事实。当推理代理在时间 t 移动到 <span class="math inline">\(e_t\)</span>，事实抽取代理从句子袋中抽取 <span class="math inline">\((e^{t}, r^{\prime}, e^{\prime})\)</span> ，可以获得动作 <span class="math inline">\(a_{E}^{t}=\left(r^{\prime}, e^{\prime}\right)\)</span> 。Action space 定义为 <span class="math inline">\(A^t_E=\left\{\left(r^{\prime}, e^{\prime}\right)\right\}\left(e^{t}, r^{\prime}, e^{\prime}\right) \in b_{e t} \subset \mathcal{A}_{E}\)</span></p><p><strong>Transtion.</strong> 转移函数 <span class="math inline">\(f\left(s_{E}^{t}, a_{E}^{t}\right)=\left(r^{\prime}, e^{\prime}\right)\)</span></p><p><strong>Reward.</strong> 事实抽取代理将从推理代理获得一个 step-wise delayed reward 。当事实抽取代理提的建议有利于推理过程时，将得到积极的奖励。</p><h2 id="cplcollaborative-policy-learning">CPL：Collaborative Policy Learning</h2><p>作者采用一种替代性的训练过程来联合更新两个代理：训练其中一个代理进行几次迭代，同时冻结另一个代理；反之亦然。两个代理的策略都通过强化算法更新。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20211017100442824.png" srcset="/img/loading.gif" lazyload></p><h3 id="augmented-action-space-for-reasoning.">Augmented Action Space for Reasoning.</h3><p>事实抽取代理抽取高置信度的边，添加到推理代理的动作空间中。</p><p><span class="math inline">\(A_{K}^{t}\)</span> 是 current KG 里的出边。 <span class="math display">\[ A_{K}^{t}=\left\{(r, e) \mid\left(e^{t}, r, e\right) \in G\right\} \]</span> <span class="math inline">\(A_{C}^{t}\)</span> 是抽取代理抽取出的出边。 <span class="math display">\[ A_{C}^{t}=\left\{\left(r^{\prime}, e^{\prime}\right) \mid\left(e^{t}, r^{\prime}, e^{\prime}\right) \in C\right\} \]</span> 所以推理代理的 Action space <span class="math inline">\(A_{R}^{t}\)</span> 为： <span class="math display">\[ A_{R}^{t}=A_{K}^{t} \cup A_{C}^{t}, A_{R}^{t} \subset \mathcal{A}_{R} \]</span></p><h3 id="reasoning-feedback-for-fact-extraction.">Reasoning Feedback for Fact Extraction.</h3><p>Reasoner 通过 extractor 对推理的贡献程度的反馈，帮助 extractor 学习提取策略。因此，定义事实提取代理从推理代理处获得 step-wise reward。</p><p>具体来说，当推理代理正确到达目标时，认为这条路径是有效的，正确的推理。如果抽取代理有助于这条正确的推理路径，就可以获得积极的奖励。即，如果抽取器在时间 t 抽取了正确推理路径上的边（<span class="math inline">\(0≤ t≤ T\)</span>），抽取器将在时间 t 奖励 1，否则奖励 0。触发 positive reward 的提取边将保留在图中，而当两个代理移动到下一个状态时，其他边将被删除。</p><blockquote><p>即使正确到达目标，路径就一定正确吗？如何去衡量这条路径？我只知道PCRA....有没有其他新一点的方法？</p></blockquote><h3 id="policy-update.">Policy Update.</h3><p>使用强化学习来训练两个代理，目标是最大化 reward： <span class="math display">\[ J(\theta)=\mathbb{E}_{\pi_{\theta}(a \mid s)}[R(s, a)] \]</span> 其中，<span class="math inline">\(R(s,a)\)</span> 是给定 s 选择 a 的 reward，</p><p>给定训练 <span class="math inline">\(\pi_{\theta} = \left\{\left(s^{1}, a^{1}, r^{1}\right), \ldots,\left(s^{T}, a^{T}, r^{T}\right)\right\}, r^{t}=R\left(s^{t}, a^{t}\right)\)</span>，参数通过 RL 更新： <span class="math display">\[ \begin{array}{c} \theta \leftarrow \theta+\alpha \nabla_{\theta} \log \pi_{\theta}\left(a^{t} \mid s^{t}\right) G^{t} \\ G^{t}=\sum_{k=t}^{T} \gamma^{k-t} R\left(s^{k}, a^{k}\right) \end{array} \]</span> 其中，<span class="math inline">\(G^t\)</span> 是 discounted accumulated reward。可以发现，参数只有当 <span class="math inline">\(G^t\)</span> 非0的时候才更新，而 <span class="math inline">\(G^t\)</span> 又由参数 <span class="math inline">\(\gamma\)</span> 决定。如果 <span class="math inline">\(γ&gt;0\)</span>，对于正确训练序列，容易验证所有状态的 <span class="math inline">\(G^t\)</span>值都将为非零。因此，内部状态将得到积极奖励，模型参数将通过内部状态的梯度进行更新。对于不同的任务，应该仔细选择 <span class="math inline">\(γ\)</span> 的值。对于抽取器设置 <span class="math inline">\(γ=0\)</span>，以避免对零奖励状态根据经验进行策略更新。这是因为零回报的经历大多是负面的例子。具体地说，如果提取器的状态为零，可以推断建议的边没有被推理器选择，或者选择的边没有帮助到达目标。我们不能允许模型根据这些经验进行更新，因此设置 <span class="math inline">\(γ=0\)</span> 以避免未来的影响。相反，推理器设置 <span class="math inline">\(γ=1\)</span>，因为所有中间选择的边都是有意义的，只要它最终指向目标。</p><h4 id="train">Train</h4><p>使用 model pre-training 和 adaptive sampling 来提高训练效率。特别是，首先在原始 KG 上训练 reasoner agent，以获得更好的初始化。同样，作者在远程监督标记的语料库上训练事实抽取器。接下来，在为两个代理生成训练经验时，作者使用自适应采样自适应地增加抽取的边的选择优先级。自适应采样旨在鼓励推理代理探索更多新的边，并促进联合培训期间的协作。Replay memories 也用于提高训练效率。</p><h4 id="inference">Inference</h4><p>在推理时，作者使用经过训练的模型通过 path finding 来预测缺失的事实。该过程类似于 training 阶段的生成训练经验，即，使用 reasoner 进行路径推理，而 extractor 不断地从语料库中抽取 edge。唯一的区别是推断不要求 reward，作者使用 beam search 在图上生成多条推理路径，并根据 reasoner 的得分对它们进行排序。</p><h2 id="policy-network-architectures">Policy Network Architectures</h2><h3 id="reasoning-agent.">Reasoning Agent.</h3><p>state embedding，action embedding： <span class="math display">\[ s_{R}^{t}=\left[e_{s}, r_{q}, h^{t}\right] \\ h^{t}=\mathrm{LSTM}\left(h^{t-1},\left[r^{t}, e^{t}\right]\right) \\ a_{R}^{t}=[r, e],\left(e^{t}, r, e\right) \in G \cup C \\ \]</span> Policy network： <span class="math display">\[ \pi_{\theta}\left(a_{R}^{t} \mid s_{R}^{t}\right)=\sigma\left(\mathbf{A}_{R}^{t} W_{2}\left(\operatorname{Relu}\left(W_{1}\left[e_{s}, r_{q}, h^{t}\right]\right)\right)\right) \]</span></p><h3 id="fact-extraction-agent.">Fact extraction Agent.</h3><p><span class="math inline">\(b_{e^t}\)</span> 是标有 <span class="math inline">\((e^t,e&#39;)\)</span> 的句子袋，将 <span class="math inline">\(b_{e^t}\)</span> 输入 PCNN-ATT 获得 句子袋级别的 embeddings <span class="math inline">\(E^t_b\)</span> <span class="math display">\[ \pi_{\theta}\left(a_{E}^{t} \mid s_{E}^{t}\right)=\sigma\left(\mathbf{A}_{E}^{t} W E_{b}^{t}\right) \]</span></p><blockquote><p>PCNN-ATT 没看，还有什么是句子袋？？</p></blockquote><h2 id="summary">Summary</h2><p>本文作者在“开放世界”环境下研究知识图推理，从背景语料库中抽取的新事实可用于促进路径发现。然后，依据此提出了一种新的协作策略学习（CPL）框架，以相互增强的方式联合训练两个RL代理。在CPL中，除了训练用于路径发现的 Reasoner Agent 之外，还引入了Fact Extractor Agent，该代理根据推理过程和语料库的上下文学习从语料库中抽取相关事实的策略。在推理时，事实抽取代理仅使用信息量最大的边动态扩充图，从而使推理代理能够有效地识别正路径。</p><div class="note note-primary"><p>推理代理通过抽取代理的置信度分数进行正反馈，帮助抽取代理学习策略。作者认为推理代理正确到达目标时，认为这条路径是有效的，正确的推理。</p><p>但是我觉着，即使正确到达目标，路径就一定正确吗？作者是如何去衡量这条路径的(我没看源码==)？我只知道PCRA....有没有其他新一点的方法？</p><p>2021.10.31 有. THU吕欣 提出了衡量多跳路径可解释性得分的 BIMR，包括三个指标：PR、LI 和 GI</p></div></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/NLP/">NLP</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/KGR/">KGR</a> <a class="hover-with-bg" href="/tags/CPL/">CPL</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/10/13/Linux%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E8%AE%B0%E5%BD%95/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Linux 常用指令记录</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/10/11/%E3%80%90Multi-Hop%20KG%E3%80%91Multi-Hop%20Knowledge%20Graph%20Reasoning%20with%20Reward%20Shaping/"><span class="hidden-mobile">【Multi-Hop KG】Multi-Hop Knowledge Graph Reasoning with Reward Shaping</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>