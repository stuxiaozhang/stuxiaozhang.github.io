<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.3.0"><script></script><link rel="apple-touch-icon" sizes="180x180" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/avatar-removebg-preview.png"><link rel="icon" type="image/png" sizes="32x32" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/avatar-removebg-preview.png"><link rel="icon" type="image/png" sizes="16x16" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/avatar-removebg-preview.png"><link rel="mask-icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/avatar-removebg-preview.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!1,style:"flat"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!0,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[toc] 1. 基本概念 1.1 神经网络组成？ 神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，先从最简单的神经网络说起。 1.1.1 感知机"><meta property="og:type" content="article"><meta property="og:title" content="深度学习基础"><meta property="og:url" content="http://stuxiaozhang.github.io/2020/10/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/index.html"><meta property="og:site_name" content="小张同学的博客"><meta property="og:description" content="[toc] 1. 基本概念 1.1 神经网络组成？ 神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，先从最简单的神经网络说起。 1.1.1 感知机"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-1.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-2.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-3.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.1.1.5.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026142212607.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-8.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-9.jpg"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201025110330265.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.2.2.1.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.2.3.1.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.2.3.2.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026111418991.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.2.3.5.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.2.4.1.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026145935759.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026154055521.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026154213233.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026154246777.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026150019062.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026150052566.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026150223734.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026150312840.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026225015021.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026212202448.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026212415442.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026222538375.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026223700750.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026223843460.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026224250502.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026224331391.png"><meta property="article:published_time" content="2020-10-24T03:37:20.000Z"><meta property="article:modified_time" content="2020-12-12T02:33:13.420Z"><meta property="article:author" content="小张同学"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-1.png"><link rel="canonical" href="http://stuxiaozhang.github.io/2020/10/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>深度学习基础 | 小张同学的博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">小张同学的博客</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">天道酬勤</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a></li><li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>动态</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://stuxiaozhang.github.io/2020/10/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/codedog.jpg"><meta itemprop="name" content="小张同学"><meta itemprop="description" content="不要停止奔跑 不要回顾来路"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="小张同学的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">深度学习基础</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-10-24 11:37:20" itemprop="dateCreated datePublished" datetime="2020-10-24T11:37:20+08:00">2020-10-24</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/2020/10/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2020/10/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>19k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>18 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>[toc]</p><h1 id="基本概念">1. 基本概念</h1><h2 id="神经网络组成">1.1 神经网络组成？</h2><p>神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，先从最简单的神经网络说起。</p><h3 id="感知机">1.1.1 感知机</h3><p>多层感知机中的特征神经元模型称为感知机，由 <em>Frank Rosenblatt</em> 于 1957 年发明。</p><p>简单的感知机如下图所示：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-1.png" alt="3-1"></p><p>其中 <span class="math inline">\(x_1\)</span>，<span class="math inline">\(x_2\)</span>，<span class="math inline">\(x_3\)</span> 为感知机的输入，其输出为： <span class="math display">\[ output = \left\{\begin{aligned} 0, \quad if \ \ \sum_i w_i x_i \leqslant threshold \\ 1, \quad if \ \ \sum_i w_i x_i &gt; threshold \end{aligned}\right. \]</span></p><p>假如把感知机想象成一个加权投票机制，比如 3 位评委给一个歌手打分，打分分别为 $ 4 $ 分、<span class="math inline">\(1\)</span> 分、$-3 <span class="math inline">\(分，这 \)</span> 3$ 位评分的权重分别是 <span class="math inline">\(1、3、2\)</span>，则该歌手最终得分为 <span class="math inline">\(4 \times 1 + 1 \times 3 + (-3) \times 2 = 1\)</span> 。按照比赛规则，选取的 <span class="math inline">\(threshold\)</span> 为 <span class="math inline">\(3\)</span>，说明只有歌手的综合评分大于 $ 3$ 时，才可顺利晋级。对照感知机，该选手被淘汰，因为：</p><p><span class="math display">\[ \sum_i w_i x_i &lt; threshold=3, output = 0 \]</span></p><p>用 <span class="math inline">\(-b\)</span> 代替 <span class="math inline">\(threshold\)</span>，输出变为：</p><p><span class="math display">\[output = \left \{ \begin{aligned} 0, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b \leqslant 0 \\ 1, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b &gt; 0 \end{aligned} \right. \]</span></p><p>设置合适的 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span class="math inline">\(b\)</span> ，一个简单的感知机单元的与非门表示如下：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-2.png" alt="3-2"></p><p>当输入为 <span class="math inline">\(0\)</span>，<span class="math inline">\(1\)</span> 时，感知机输出为 $ 0 (-2) + 1 (-2) + 3 = 1$。</p><p>复杂一些的感知机由简单的感知机单元组合而成：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-3.png" alt="3-3"></p><h3 id="多层感知机">1.1.2 多层感知机</h3><p>多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第 $ i $ 层的每个神经元和第 $ i-1 $ 层的每个神经元都有连接。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.1.1.5.png" alt="3.1.1.5" style="zoom:80%"></p><p>输出层可以不止有 $ 1$ 个神经元。隐藏层可以只有 $ 1$ 层，也可以有多层。输出层为多个神经元的神经网络例如下图所示：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026142212607.png" alt="image-20201026142212607" style="zoom:67%"></p><h2 id="为什么使用深层表示">1.2 为什么使用深层表示?</h2><ol type="1"><li>深度神经网络是一种特征递进式的学习算法，浅层的神经元直接从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而深层的特征则基于已学习到的浅层特征继续学习更高级的特征，从计算机的角度学习深层的语义信息。</li><li>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</li></ol><h2 id="为什么深层神经网络难以训练">1.3 为什么深层神经网络难以训练？</h2><h3 id="梯度消失">1.3.1 梯度消失</h3><p>梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。</p><p>梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。<strong>在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。</strong></p><p>下图是不同隐含层的学习速率：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-8.png" alt="3-8" style="zoom:50%"></p><h3 id="梯度爆炸">1.3.2 梯度爆炸</h3><p>在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为 <span class="math inline">\(NaN\)</span> 值，再也无法更新。</p><h3 id="权重矩阵的退化导致模型的有效自由度减少">1.3.3 权重矩阵的退化导致模型的有效自由度减少。</h3><p>参数空间中学习的退化速度减慢，导致减少了模型的有效维数，网络的可用自由度对学习中梯度范数的贡献不均衡，随着相乘矩阵的数量（即网络深度）的增加，矩阵的乘积变得越来越退化。在有硬饱和边界的非线性网络中（例如 ReLU 网络），随着深度增加，退化过程会变得越来越快。Duvenaud 等人 2014 年的论文里展示了关于该退化过程的可视化：</p><p>可以发现：随着深度的增加，输入空间（左上角所示）会在输入空间中的每个点处被扭曲成越来越细的单丝，只有一个与细丝正交的方向影响网络的响应。沿着这个方向，网络实际上对变化变得非常敏感。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3-9.jpg" alt="3-9" style="zoom:67%"></p><h2 id="深度学习和机器学习有什么不同">1.4 深度学习和机器学习有什么不同？</h2><p><strong>机器学习</strong>：利用计算机、概率论、统计学等知识，输入数据，让计算机学会新知识。机器学习的过程，就是训练数据去优化目标函数。</p><p><strong>深度学习</strong>：是一种特殊的机器学习，具有强大的能力和灵活性。它通过学习将世界表示为嵌套的层次结构，每个表示都与更简单的特征相关，而抽象的表示则用于计算更抽象的表示。</p><p>传统的机器学习需要定义一些手工特征，从而有目的的去提取目标信息， 非常依赖任务的特异性以及设计特征的专家经验。而深度学习可以从大数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，不依赖人工的特征工程，这也是深度学习在大数据时代受欢迎的一大原因。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201025110330265.png" alt="image-20201025110330265" style="zoom:50%"></p><h1 id="网络操作与计算">2. 网络操作与计算</h1><h2 id="前向传播与反向传播">2.1 前向传播与反向传播</h2><p>神经网络的计算主要有两种：</p><ul><li><p>前向传播（foward propagation, FP）作用于每一层的输入，<strong>通过逐层计算得到输出结果；</strong></p></li><li><p>反向传播（backward propagation, BP）作用于网络的输出，<strong>通过计算梯度由深到浅更新网络参数。</strong></p></li></ul><p>使用反向传播算法的多层感知器又称为 BP 神经网络。BP 算法是一个迭代算法，它的基本思想为：</p><ol type="1"><li>先计算每一层的状态和激活值，直到最后一层（即信号是前向传播的）</li><li>计算每一层的误差，误差的计算过程是从最后一层向前推进的（这就是反向传播算法名字的由来）；</li><li>更新参数（目标是误差变小）。</li><li>迭代前面两个步骤，直到满足停止准则（比如相邻两次迭代的误差的差别很小）。</li></ol><p>== 详情请看这篇文章。第三章的 3.2.5 有详细介绍，到时候写 bpboke 记得参考！==</p><h2 id="如何计算神经网络的输出">2.2 如何计算神经网络的输出？</h2><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.2.2.1.png" alt="3.2.2.1" style="zoom:80%"></p><p>如上图，输入层有三个节点，我们将其依次编号为 1、2、3；隐藏层的 4 个节点，编号依次为 4、5、6、7；最后输出层的两个节点编号为 8、9。比如，隐藏层的节点 4，它和输入层的三个节点 1、2、3 之间都有连接，其连接上的权重分别为是 $ w_{41}, w_{42}, w_{43} $。</p><p>为了计算节点 4 的输出值，我们必须先得到其所有上游节点（也就是节点 1、2、3）的输出值。节点 1、2、3 是输入层的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点 1、2、3 的输出值分别是 $ x_1, x_2, x_3 $。</p><p><span class="math display">\[a_4 = \sigma(w^T \cdot a) = \sigma(w_{41}x_4 + w_{42}x_2 + w_{43}x_3 + w_{4b}) \]</span></p><p>其中 $ w_{4b} $ 是节点 4 的偏置项。</p><p>同样，我们可以继续计算出节点 5、6、7 的输出值 $ a_5, a_6, a_7 $。</p><p>计算输出层的节点 8 的输出值 $ y_1 $：</p><p><span class="math display">\[y_1 = \sigma(w^T \cdot a) = \sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b}) \]</span></p><p>其中 $ w_{8b} $ 是节点 8 的偏置项。</p><p>同理，我们还可以计算出 $ y_2 $。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量 $ x_1, x_2, x_3, x_4 $ 时，神经网络的输出向量 $ y_1, y_2 $ 。这里我们也看到，输出向量的维度和输出层神经元个数相同。</p><h2 id="如何计算卷积神经网络输出值">2.3 如何计算卷积神经网络输出值？</h2><p>假设有一个 5*5 的图像，使用一个 3*3 的 filter 进行卷积，想得到一个 3*3 的 Feature Map，如下所示：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.2.3.1.png" alt="3.2.3.1" style="zoom:80%"></p><p>$ x_{i,j} $ 表示图像第 $ i $ 行第 $ j $ 列元素。$ w_{m,n} $ 表示 filter​ 第 $ m $ 行第 $ n $ 列权重。 $ w_b $ 表示 <span class="math inline">\(filter\)</span> 的偏置项。 表 <span class="math inline">\(a_i,_j\)</span> 示 feature map 第 $ i$ 行第 $ j $ 列元素。 <span class="math inline">\(f\)</span> 表示激活函数，这里以 $ ReLU$ 函数为例。</p><p>卷积计算公式如下：</p><p><span class="math display">\[a_{i,j} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b ) \]</span></p><p>当步长为 <span class="math inline">\(1\)</span> 时，计算 feature map 元素 $ a_{0,0} $ 如下：</p><p><span class="math display">\[a_{0,0} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{0+m, 0+n} + w_b ) = relu(w_{0,0} x_{0,0} + w_{0,1} x_{0,1} + w_{0,2} x_{0,2} + w_{1,0} x_{1,0} + \\ w_{1,1} x_{1,1} + w_{1,2} x_{1,2} + w_{2,0} x_{2,0} + w_{2,1} x_{2,1} + w_{2,2} x_{2,2}) \\ = 1 + 0 + 1 + 0 + 1 + 0 + 0 + 0 + 1 \\ = 4 \]</span></p><p>其计算过程图示如下：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.2.3.2.png" alt="3.2.3.2" style="zoom:80%"></p><p>以此类推，计算出全部的 Feature Map。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026111418991.png" alt="image-20201026111418991" style="zoom:70%"></p><p>当步幅为 2 时，Feature Map 计算如下</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.2.3.5.png" alt="3.2.3.5" style="zoom:80%"></p><div class="note warning"><p>图像大小、步幅和卷积后的 Feature Map 大小是有关系的。它们满足下面的关系：</p><p><span class="math display">\[W_2 = (W_1 - F + 2P)/S + 1\\ H_2 = (H_1 - F + 2P)/S + 1 \]</span></p><p>其中:</p><ul><li>$ W_2 $ 是卷积后 Feature Map 的宽度；</li><li>$ W_1 $ 是卷积前图像的宽度；</li><li>$ F $ 是 filter 的宽度；</li><li>$ P $ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 <span class="math inline">\(0\)</span>，如果 <span class="math inline">\(P\)</span> 的值是 <span class="math inline">\(1\)</span>，那么就补 <span class="math inline">\(1\)</span> 圈 <span class="math inline">\(0\)</span>；</li><li><span class="math inline">\(S\)</span> 是步幅 Stride；</li><li>$ H_2 $ 卷积后 Feature Map 的高度；</li><li>$ H_1 $ 是卷积前图像的宽度。</li></ul></div><blockquote><p>举栗：假设图像宽度 $ W_1 = 5 $，filter 宽度 $ F=3 $，Zero Padding $ P=0 $，步幅 $ S=2 <span class="math inline">\(，\)</span> Z $ 则</p><p><span class="math display">\[W_2 = (W_1 - F + 2P)/S + 1 = (5-3+0)/2 + 1 = 2 \]</span></p><p>说明 Feature Map 宽度是 2。同样，我们也可以计算出 Feature Map 高度也是 2。</p></blockquote><div class="note warning"><p>如果卷积前的图像深度为 $ D $，那么相应的 filter 的深度也必须为 $ D $。深度大于 1 的卷积计算公式：</p><p><span class="math display">\[a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b) \]</span></p><p>​ 其中，$ D $ 是深度；$ F $ 是 filter 的大小；$ w_{d,m,n} $ 表示 filter 的第 $ d $ 层第 $ m $ 行第 $ n $ 列权重；$ a_{d,i,j} $ 表示 feature map 的第 $ d $ 层第 $ i $ 行第 $ j $ 列像素；其它的符号含义前面相同，不再赘述。</p></div><p>每个卷积层可以有多个 filter。每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。卷积后 Feature Map 的深度 (个数) 和卷积层的 filter 个数相同。</p><blockquote><p>举栗：下面的图示显示了包含两个 filter 的卷积层的计算。<span class="math inline">\(7*7*3\)</span> 输入，经过两个 <span class="math inline">\(3*3*3\)</span> filter 的卷积(步幅为 <span class="math inline">\(2\)</span>)，得到了 <span class="math inline">\(3*3*2\)</span> 的输出。图中的 Zero padding 是 <span class="math inline">\(1\)</span>，也就是在输入元素的周围补了一圈 <span class="math inline">\(0\)</span>。</p></blockquote><p>以上就是卷积层的计算方法。这里面体现了 <strong>局部连接和权值共享</strong>：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个 $ 3 * 3 * 3 $ 的 fitler 的卷积层来说，其参数数量仅有 $ (3 * 3 * 3+1) * 2 = 56 $ 个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。</p><h2 id="如何计算 -pooling- 层输出值输出值">2.4 如何计算 Pooling 层输出值输出值？</h2><p><strong>Pooling 层主要的作用是下采样，通过去掉 Feature Map 中不重要的样本，进一步减少参数数量。</strong></p><p>Pooling 的方法很多，最常用的是 Max Pooling。Max Pooling 实际上就是在 n*n 的样本中取最大值，作为采样后的样本值。下图是 2*2 max pooling：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /3.2.4.1.png" alt="3.2.4.1" style="zoom:80%"></p><p>除了 Max Pooing 之外，常用的还有 Average Pooling ——取各样本的平均值。</p><div class="note warning"><p>对于深度为 $ D $ 的 Feature Map，各层独立做 Pooling，因此 Pooling 后的深度仍然为 $ D $。</p></div><h2 id="神经网络更深有什么意义">2.5 神经网络更“深”有什么意义？</h2><p>前提：在一定范围内。</p><ul><li>在神经元数量相同的情况下，深层网络结构具有更大容量，分层组合带来的是指数级的表达空间，能够组合成更多不同类型的子结构，这样可以更容易地学习和表示各种特征。</li><li>隐藏层增加则意味着由激活函数带来的非线性变换的嵌套层数更多，就能构造更复杂的映射关系。</li></ul><h1 id="超参数">3. 超参数</h1><h2 id="什么是超参数">3.1 什么是超参数？</h2><p><strong>超参数 </strong>: 在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要<u> 对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</u></p><p>超参数通常存在于：</p><ul><li>定义关于模型的更高层次的概念，如复杂性或学习能力。</li><li>不能直接从标准模型培训过程中的数据中学习，需要预先定义。</li><li>可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定</li></ul><p>超参数具体来讲比如算法中的学习率（learning rate）、梯度下降法迭代的数量（iterations）、隐藏层数目（hidden layers）、隐藏层单元数目、激活函数（ activation function）都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。</p><h2 id="如何寻找超参数的最优值">3.2 如何寻找超参数的最优值？</h2><p>在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有：</p><ol type="1"><li><p>猜测和检查：根据经验或直觉，选择参数，一直迭代。</p></li><li><p>网格搜索：让计算机尝试在一定范围内均匀分布的一组值。</p></li><li><p>随机搜索：让计算机随机挑选一组值。</p></li><li><p>贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。</p></li><li><p>MITIE 方法，好初始猜测的前提下进行局部优化。它使用 BOBYQA 算法，并有一个精心选择的起始点。由于 BOBYQA 只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在 MITIE 的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。</p></li><li><p>最新提出的 LIPO 的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。</p></li></ol><h2 id="超参数搜索一般过程">3.3 超参数搜索一般过程？</h2><p>超参数搜索一般过程：</p><ol type="1"><li>将数据集划分成训练集、验证集及测试集。</li><li>在训练集上根据模型的性能指标对模型参数进行优化。</li><li>在验证集上根据模型的性能指标对模型的超参数进行搜索。</li><li>步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。</li></ol><p>其中，搜索过程需要搜索算法，一般有：网格搜索、随机搜过、启发式智能搜索、贝叶斯搜索。</p><h1 id="激活函数">4. 激活函数</h1><h2 id="为什么需要非线性激活函数">4.1 为什么需要非线性激活函数？</h2><h3 id="为什么需要激活函数">4.1.1 为什么需要激活函数？</h3><ol type="1"><li>激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。</li><li>激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。</li><li>激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</li></ol><h3 id="为什么激活函数需要非线性函数">4.1.2 为什么激活函数需要非线性函数？</h3><ol type="1"><li>如果不用激励函数（其实相当于激励函数是 f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与只有一个隐藏层效果相当，这种情况就是多层感知机（MLP）了。</li><li>使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。</li></ol><h2 id="激活函数有哪些性质">4.2 激活函数有哪些性质？</h2><ol type="1"><li><p>非线性： 当激活函数是线性的，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数，一个两层的神经网络就可以基本上逼近所有的函数。</p></li><li><p>可微性： 反向传播中，损失函数要对参数求偏导，如果激活函数不可微，那就无法使用梯度下降方法更新参数了。(ReLU 只在零点不可微，但是梯度下降几乎不可能收敛到梯度为 0)</p></li><li><p>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；</p></li><li><p>非饱和性：饱和指在某些区间梯度接近于零，使参数无法更新。Sigmoid 和 tanh 都有这个问题，而 ReLU 就没有，所以普遍效果更好。</p><blockquote><p>sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在 sigmoid 接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成信息丢失），这种现象称为饱和。</p></blockquote></li><li><p>计算简单：神经元 (units) 越多，激活函数计算的次数就越多，复杂的激活函数会降低训练速度。</p></li><li><p>输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，即使有很大的输入，激活函数的输出也不会太大。当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。</p></li></ol><h2 id="常见的激活函数及图像">4.3 常见的激活函数及图像</h2><h3 id="sigmoid- 激活函数">4.3.1 sigmoid 激活函数</h3><p>函数的值域为 $ (0,1) $，定义为： <span class="math display">\[f(x) = \frac{1}{1 + e^{-x}} \]</span> 函数图像如下：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026145935759.png" alt="image-20201026145935759" style="zoom:67%"></p><h3 id="softmax- 激活函数">4.3.2 softmax 激活函数</h3><p><strong>Softmax 多用于多分类神经网络输出。</strong>函数定义为： <span class="math display">\[f(z_j) = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \]</span></p><h4 id="sigmoid- 函数和 -softmax- 函数 - 的区别">sigmoid 函数和 softmax 函数 的区别？</h4><p>wiki 百科对 softmax 函数的定义：</p><blockquote><p>softmax is a generalization of logistic function that “squashes”(maps) a K-dimensional vector z of arbitrary real values to a K-dimensional vector σ(z) of real values in the range (0, 1) that add up to 1.</p></blockquote><p>这句话既表明了 softmax 函数与 logistic 函数的关系，也同时阐述了 softmax 函数的本质就是将一个 K 维的任意实数向量压缩（映射）成另一个 K 维的实数向量，其中向量中的每个元素取值都介于（0，1）之间。</p><p>即：sigmoid 将一个 real value 映射到（0,1）的区间（当然也可以是（-1,1）），这样可以用来做 <strong>二分类 </strong>。 而 softmax 把一个 k 维的 real value 向量 <span class="math inline">\((a1,a2,a3,a4…)\)</span> 映射成一个 <span class="math inline">\((b1,b2,b3,b4…)\)</span> 其中 bi 是一个 0-1 的常数，然后可以根据 bi 的大小来进行<strong> 多分类 </strong>的任务，如取权重最大的一维。</p><h4 id="softmax- 函数如何应用于多分类">softmax 函数如何应用于多分类？</h4><p>softmax 用于多分类过程中，它将多个神经元的输出，映射到 $ (0,1) $ 区间内，可以看成概率来理解，从而来进行多分类！</p><p>假设有一个数组，$ V_j $ 表示 $ V $ 中的第 <span class="math inline">\(j\)</span> 个元素，那么这个元素的 softmax 值就是</p><p><span class="math display">\[S_j = \frac{e^{V_j}}{\sum_i e^{V_i}} \]</span> 从下图看，神经网络中包含了输入层，然后通过两个特征层处理，最后通过 softmax 分析器就能得到不同条件下的概率，这里需要分成三个类别，最终会得到 $ y=0, y=1, y=2 $ 的概率值。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026154055521.png" alt="image-20201026154055521" style="zoom:67%"></p><p>继续看下面的图，三个输入通过 softmax 后得到一个数组 $ [0.05 , 0.10 , 0.85] $，这就是 softmax 的功能。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026154213233.png" alt="image-20201026154213233" style="zoom:60%"></p><p>更形象的映射过程如下图所示：</p><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026154246777.png" alt="image-20201026154246777"><figcaption>image-20201026154246777</figcaption></figure><p>softmax 直白来说就是将原来输出是 $ 3,1,-3 $ 通过 softmax 函数一作用，就映射成为 $ (0,1) $ 的值，而这些值的累和为 $ 1 $（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，就可以选取概率最大（也就是值对应最大的）结点，作为预测目标！</p><h3 id="tanh- 激活函数">4.3.3 tanh 激活函数</h3><p>函数的值域为 $ (-1,1) $, 定义为： <span class="math display">\[f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</span> 函数图像如下：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026150019062.png" alt="image-20201026150019062" style="zoom:67%"></p><h4 id="为什么 tanh 收敛速度比 sigmoid 快">为什么 tanh 收敛速度比 sigmoid 快？</h4><p>首先看如下两个函数的求导： <span class="math display">\[tanh^{,}(x)=1-tanh(x)^{2}\in (0,1) \]</span></p><p><span class="math display">\[s^{,}(x)=s(x)*(1-s(x))\in (0,\frac{1}{4}] \]</span></p><p>由上面两个公式可知 tanh(x) 梯度消失的问题比 sigmoid 轻，所以 Tanh 收敛速度比 Sigmoid 快。</p><h3 id="relu- 激活函数">4.3.4 ReLU 激活函数</h3><p>函数的值域为 $ [0,+∞) $，定义为： <span class="math display">\[f(x) = max(0, x) \]</span> 函数图像如下：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026150052566.png" alt="image-20201026150052566" style="zoom:67%"></p><h4 id="怎样理解 -relu-0- 时是非线性激活函数">怎样理解 Relu（&lt; 0 时）是非线性激活函数？</h4><p>观察 Relu 函数图像发现以下特点：</p><ol type="1"><li><p>单侧抑制；</p></li><li><p>相对宽阔的兴奋边界；</p></li><li><p>稀疏激活性；</p></li></ol><p>ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。</p><p>因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</p><p><strong>稀疏激活性 </strong>：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，<u> 更好更快地提取稀疏特征</u>。当 $ x&lt;0 $ 时，ReLU 硬饱和，而当 $ x&gt;0 $ 时，则不存在饱和问题。ReLU 能够在 $ x&gt;0 $ 时保持梯度不衰减，从而缓解梯度消失问题。</p><h4 id="使用 -relu- 激活函数的优点 - 为什么 relu 要好过 sigmoid 和 tanh">使用 ReLu 激活函数的优点？/ 为什么 ReLu 要好过 sigmoid 和 tanh？</h4><ol type="1"><li>采用 sigmoid 等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法和指数运算，计算量相对大，而采用 Relu 激活函数，整个过程的计算量节省很多。</li><li>对于深层网络，sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在 sigmoid 接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成信息丢失），这种现象称为饱和，从而无法完成深层网络的训练，为什么会无法完成训练呢，因为神经网络更新 w 的时候就是依靠导数来更新的，如果导数接近 0，那 w 更新之后还是原来的 w 了。而 ReLU 就不会有饱和倾向，不会有特别小的梯度出现。</li><li>需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性。但他也不是没有问题，主要问题有两个：<ol type="1"><li>0 点附近不可微。所以通常在 0 点只求右导数</li><li>它会“谋杀”一些神经元。就是说在 BP 的过程中很快会让一些神经元的导数永远是 0，于是这些神经元等于被抛弃了，也就是被谋杀了。这提高了速度和精度，但是也有些武断，于是为了解决这个问题，引入了 Leaky ReLU。Leaky ReLu 不会产生这个问题。</li></ol></li></ol><h3 id="leaky-relu- 激活函数">4.3.5 Leaky Relu 激活函数</h3>函数定义为： $ f(x) = {<span class="math display">\[\begin{aligned} ax, \quad x&lt;0 \\ x, \quad x&gt;0 \end{aligned}\]</span><p>. $，值域为 $ (-∞,+∞) $。</p><p>图像如下（$ a = 0.5 $）：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026150223734.png" alt="image-20201026150223734" style="zoom:67%"></p><h3 id="softplus- 激活函数">4.3.6 SoftPlus 激活函数</h3><p>函数的定义为：$ f(x) = ln(1 + e^x) $，值域为 $ (0,+∞) $。</p><p>函数图像如下:</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026150312840.png" alt="image-20201026150312840" style="zoom:67%"></p><h2 id="常见激活函数的导数计算">4.4 常见激活函数的导数计算？</h2><p>对常见激活函数，导数计算如下：</p><table style="width:100%"><colgroup><col style="width:8%"><col style="width:24%"><col style="width:33%"><col style="width:33%"></colgroup><thead><tr class="header"><th>原函数</th><th>函数表达式</th><th>导数</th><th>备注</th></tr></thead><tbody><tr class="odd"><td>Sigmoid 激活函数</td><td><span class="math inline">\(f(x)=\frac{1}{1+e^{-x}}\)</span></td><td><span class="math inline">\(f^{&#39;}(x)=\frac{1}{1+e^{-x}}\left(1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))\)</span></td><td>当<span class="math inline">\(x=10\)</span>, 或<span class="math inline">\(x=-10\)</span>，<span class="math inline">\(f^{&#39;}(x) \approx0\)</span>, 当<span class="math inline">\(x=0\)</span><span class="math inline">\(f^{&#39;}(x) =0.25\)</span></td></tr><tr class="even"><td>Tanh 激活函数</td><td><span class="math inline">\(f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\)</span></td><td><span class="math inline">\(f^{&#39;}(x)=-(tanh(x))^2\)</span></td><td>当<span class="math inline">\(x=10\)</span>, 或<span class="math inline">\(x=-10\)</span>，<span class="math inline">\(f^{&#39;}(x) \approx0\)</span>, 当<span class="math inline">\(x=0\)</span><span class="math inline">\(f^{`}(x) =1\)</span></td></tr><tr class="odd"><td>Relu 激活函数</td><td><span class="math inline">\(f(x)=max(0,x)\)</span></td><td><span class="math inline">\(c(u)=\begin{cases} 0,x&lt;0 \\ 1,x&gt;0 \\ undefined,x=0\end{cases}\)</span></td><td>通常 <span class="math inline">\(x=0\)</span> 时，给定其导数为 1 和 0</td></tr></tbody></table><h2 id="如何选择激活函数">4.5 如何选择激活函数？</h2><p>选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。</p><p>以下是常见的选择情况：</p><ol type="1"><li>如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。</li><li>如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。</li><li>sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。</li><li>tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。</li><li>ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。</li><li>如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。</li></ol><h2 id="交叉熵损失函数">4.6 交叉熵损失函数</h2><p>神经元的输出就是 a = σ(z)，其中 <span class="math inline">\(z=\sum w_{j}i_{j}+b\)</span> 是输⼊的带权和。</p><p><span class="math display">\[C=-\frac{1}{n}\sum[ylna+(1-y)ln(1-a)] \]</span> 其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的， y 是对应的⽬标输出。</p><p>== 详情请看这篇文章。（又是一个待办事项...）==</p><h1 id="dropout- 系列问题">5. Dropout 系列问题</h1><h2 id="为什么要正则化">5.1 为什么要正则化？</h2><ol type="1"><li>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。<br></li><li>如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差。</li></ol><h2 id="为什么正则化有利于预防过拟合">5.2 为什么正则化有利于预防过拟合？</h2><p>左图是高偏差，右图是高方差，中间是 Just Right.</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026225015021.png" alt="image-20201026225015021" style="zoom:80%"></p><h2 id="理解 dropout 正则化">5.3 理解 dropout 正则化</h2><p>Dropout 可以随机删除网络中的神经单元，它为什么可以通过正则化发挥如此大的作用呢？</p><p>直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout 将产生收缩权重的平方范数的效果，和之前讲的 L2 正则化类似；实施 dropout 的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2 对不同权重的衰减是不同的，它取决于激活函数倍增的大小。</p><h2 id="dropout 率的选择">5.4 dropout 率的选择</h2><ol type="1"><li>经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。</li><li>dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8）</li><li>对参数 $ w $ 的训练进行球形限制 (max-normalization)，对 dropout 的训练非常有用。</li><li>球形半径 $ c $ 是一个需要调整的参数，可以使用验证集进行参数调优。</li><li>dropout 自己虽然也很牛，但是 dropout、max-normalization、large decaying learning rates and high momentum 组合起来效果更好，比如 max-norm regularization 就可以防止大的 learning rate 导致的参数 blow up。</li><li>使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $ 1/p $。</li></ol><h2 id="dropout 有什么缺点">5.5 dropout 有什么缺点？</h2><p>dropout 一大缺点就是代价函数 J 不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数 J 每次迭代后都会下降，因为我们所优化的代价函数 J 实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。作者通常会关闭 dropout 函数，将 keep-prob 的值设为 1，运行代码，确保 J 函数单调递减。然后打开 dropout 函数，希望在 dropout 过程中，代码并未引入 bug。作者觉得也可以尝试其它方法，虽然他们并没有关于这些方法性能的数据统计，但我可以把它们与 dropout 方法一起使用。 (母鸡~)</p><h1 id="batch_size 这块不太懂">6. Batch_Size(这块不太懂 ==)</h1><p>直观的理解：</p><p>Batch Size 定义：一次训练所选取的样本数。</p><p>Batch Size 的大小影响模型的优化程度和速度。同时其直接影响到 GPU 内存的使用情况，假如你 GPU 内存不大，该数值最好设置小一点 hh。</p><h2 id="为什么需要 -batch_size">6.1 为什么需要 Batch_Size？</h2><p>Batch 的选择，首先决定的是下降的方向。</p><p>如果数据集比较小，可采用全数据集的形式，好处是：</p><ol type="1"><li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。</li><li>由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。</li></ol><p>对于更大的数据集，假如采用全数据集的形式，坏处是：</p><ol type="1"><li>随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。</li><li>以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。</li></ol><h2 id="batch_size- 值的选择">6.2 Batch_Size 值的选择</h2><p>假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。</p><p>既然 Batch_Size 为全数据集或者 Batch_Size = 1 都有各自缺点，可不可以选择一个适中的 Batch_Size 值呢？</p><p>此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。</p><h2 id="在合理范围内增大 batch_size 有何好处">6.3 在合理范围内，增大 Batch_Size 有何好处？</h2><ol type="1"><li>内存利用率提高了，大矩阵乘法的并行化效率提高。</li><li>跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</li><li>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。</li></ol><h2 id="盲目增大 -batch_size- 有何坏处">6.4 盲目增大 Batch_Size 有何坏处？</h2><ol type="1"><li>内存利用率提高了，但是内存容量可能撑不住了。</li><li>跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。</li><li>Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</li></ol><h2 id="调节 -batch_size- 对训练效果影响到底如何">6.5 调节 Batch_Size 对训练效果影响到底如何？</h2><ol type="1"><li>Batch_Size 太小，模型表现效果极其糟糕(error 飙升)。</li><li>随着 Batch_Size 增大，处理相同数据量的速度越快。</li><li>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</li><li>由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。</li><li>由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。</li></ol><h1 id="归一化">7. 归一化</h1><h2 id="归一化含义">7.1 归一化含义？</h2><ol type="1"><li><p>归纳统一样本的统计分布性。归一化在 $ 0-1$ 之间是统计的概率分布，归一化在 $ [-1, +1] $ 之间是统计的坐标分布。</p></li><li><p>无论是为了建模还是为了计算，首先基本度量单位要同一，神经网络是以样本在事件中的统计分别几率来进行训练（概率计算）和预测，且 sigmoid 函数的取值是 0 到 1 之间的，网络最后一个节点的输出也是如此，所以经常要对样本的输出归一化处理。</p></li><li><p>归一化是统一在 $ 0-1 $ 之间的统计概率分布，当所有样本的输入信号都为正值时，与第一隐含层神经元相连的权值只能同时增加或减小，从而导致学习速度很慢。</p></li><li><p>另外在数据中常存在奇异样本数据，奇异样本数据存在所引起的网络训练时间增加，并可能引起网络无法收敛。为了避免出现这种情况及后面数据处理的方便，加快网络学习速度，可以对输入信号进行归一化，使得所有样本的输入信号其均值接近于 0 或与其均方差相比很小。</p></li></ol><h2 id="为什么要归一化">7.2 为什么要归一化？</h2><ol type="1"><li>为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。</li><li>为了程序运行时收敛加快。</li><li>同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。</li><li>避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。</li><li>保证输出数据中数值小的不被吞食。</li></ol><h2 id="为什么归一化能提高求解最优解速度">7.3 为什么归一化能提高求解最优解速度？</h2><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026212202448.png" alt="image-20201026212202448"><figcaption>image-20201026212202448</figcaption></figure><p>上图是代表数据是否均一化的最优解寻解过程（圆圈可以理解为等高线）。左图表示未经归一化操作的寻解过程，右图表示经过归一化后的寻解过程。</p><p>当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。</p><p>因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。</p><h2 id="d- 图解未归一化">7.4 3D 图解未归一化</h2><p>栗子：</p><p>假设 $ w1 $ 的范围在 $ [-10, 10] $，而 $ w2 $ 的范围在 $ [-100, 100] $，梯度每次都前进 1 单位，那么在 $ w1 $ 方向上每次相当于前进了 $ 1/20 $，而在 $ w2 $ 上只相当于 $ 1/200 $！某种意义上来说，在 $ w2 $ 上前进的步长更小一些, 而 $ w1 $ 在搜索过程中会比 $ w2 $ “走”得更快。</p><p>这样会导致，在搜索过程中更偏向于 $ w1 $ 的方向。走出了“L”形状，或者成为“之”字形。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026212415442.png" alt="image-20201026212415442" style="zoom:67%"></p><h2 id="归一化有哪些类型">7.5 归一化有哪些类型？</h2><h3 id="线性归一化">7.5.1 线性归一化</h3><p><span class="math display">\[x^{\prime} = \frac{x-min(x)}{max(x) - min(x)} \]</span></p><ul><li><p>适用范围：比较适用在数值比较集中的情况。</p></li><li><p>缺点：如果 max 和 min 不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。</p></li></ul><h3 id="标准差标准化">7.5.2 标准差标准化</h3><p><span class="math display">\[x^{\prime} = \frac{x-\mu}{\sigma} \]</span></p><ul><li>含义：经过处理的数据符合标准正态分布，即均值为 0，标准差为 1 其中 $ $ 为所有样本数据的均值，$ $ 为所有样本数据的标准差。</li></ul><h3 id="非线性归一化">7.5.3 非线性归一化</h3><ul><li>适用范围：经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 $ log $、指数，正切等。</li></ul><h2 id="什么是批归一化 batch-normalization">7.6 什么是批归一化（Batch Normalization）</h2><p>以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过 $ (WX+b) $ 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。</p><p>这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化 Batch Normalization（BN）。</p><h3 id="批归一化 bn 优点">7.6.1 批归一化（BN）优点</h3><ol type="1"><li>减少了人为选择参数。<u>在某些情况下可以取消 dropout 和 L2 正则项参数</u>, 或者采取更小的 L2 正则项约束参数；</li><li>减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛；</li><li>破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 1% 的精度）。</li><li>减少梯度消失，加快收敛速度，提高训练精度。</li><li>可以不再使用局部响应归一化。BN 本身就是归一化网络(局部响应归一化在 AlexNet 网络中存在)</li></ol><h3 id="批归一化 bn 流程">7.6.2 批归一化（BN）流程</h3><p>输入：上一层输出结果 $ X = {x_1, x_2, ..., x_m} $，学习参数 $ , $</p><p>算法流程：</p><ol type="1"><li><strong>计算上一层输出数据的均值</strong></li></ol><p><span class="math display">\[\mu_{\beta} = \frac{1}{m} \sum_{i=1}^m(x_i) \]</span></p><p>​ 其中，$ m $ 是此次训练样本 batch 的大小。</p><ol start="2" type="1"><li><strong>计算上一层输出数据的标准差</strong></li></ol><p><span class="math display">\[\sigma_{\beta}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\beta})^2 \]</span></p><ol start="3" type="1"><li><strong>归一化处理，得到</strong></li></ol><p><span class="math display">\[\hat x_i = \frac{x_i + \mu_{\beta}}{\sqrt{\sigma_{\beta}^2} + \epsilon} \]</span></p><p>​ 其中 $ $ 是为了避免分母为 0 而加进去的接近于 0 的很小值</p><ol start="4" type="1"><li><strong>重构，对经过上面归一化处理得到的数据进行重构，得到</strong></li></ol><p><span class="math display">\[ y_i = \gamma \hat x_i + \beta \]</span></p><p>​ 其中，$ , $ 为可学习参数。</p><p>注：上述是 BN 训练时的过程，但是当在投入使用时，往往只是输入一个样本，没有所谓的均值 $ <em>{} $ 和标准差 $ </em>{}^2 $。此时，均值 $ <em>{} $ 是计算所有 batch $ </em>{} $ 值的平均值得到，标准差 $ <em>{}^2 $ 采用每个 batch $ </em>{}^2 $ 的无偏估计得到。</p><h3 id="batch-normalization 在什么时候用比较合适">7.6.3 Batch Normalization 在什么时候用比较合适？</h3><p>在 CNN 中，BN 应作用在非线性映射前。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试 BN 来解决。另外，在一般使用情况下也可以加入 BN 来加快训练速度，提高模型精度。</p><p>BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle，否则效果会差很多。另外，由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量，因此不适用于动态的网络结构和 RNN 网络。</p><h1 id="预训练与微调 fine-tuning">8. 预训练与微调(fine tuning)</h1><h2 id="为什么无监督预训练可以帮助深度学习">8.1 为什么无监督预训练可以帮助深度学习？</h2><p>深度网络存在问题:</p><ol type="1"><li><p>网络越深，需要的训练样本数越多。若用监督则需大量标注样本，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。</p></li><li><p>多层神经网络参数优化是个高阶非凸优化问题，经常得到收敛较差的局部解；</p></li><li><p>梯度消失问题，BP 算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。</p></li></ol><p><strong>解决方法：</strong></p><p>逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。</p><p>经过预训练最终能得到比较好的局部最优解。</p><h2 id="什么是模型微调 -fine-tuning">8.2 什么是模型微调 fine tuning</h2><p>用别人的参数、修改后的网络和自己的数据进行训练，使得参数适应自己的数据，这样一个过程，通常称之为微调（fine tuning).</p><p><strong>模型的微调举例说明：</strong></p><p>我们知道，CNN 在图像识别这一领域取得了巨大的进步。如果想将 CNN 应用到我们自己的数据集上，这时通常就会面临一个问题：通常我们的 dataset 都不会特别大，一般不会超过 1 万张，甚至更少，每一类图片只有几十或者十几张。这时候，直接应用这些数据训练一个网络的想法就不可行了，因为深度学习成功的一个关键性因素就是大量带标签数据组成的训练集。如果只利用手头上这点数据，即使我们利用非常好的网络结构，也达不到很高的 performance。这时候，fine-tuning 的思想就可以很好解决我们的问题：我们通过对 ImageNet 上训练出来的模型（如 CaffeNet,VGGNet,ResNet) 进行微调，然后应用到我们自己的数据集上。</p><h2 id="微调时候网络参数是否更新">8.3 微调时候网络参数是否更新？</h2><p>答案：会更新。</p><ol type="1"><li>finetune 的过程相当于继续训练，跟直接训练的区别是初始化的时候。</li><li>直接训练是按照网络定义指定的方式初始化。</li><li>finetune 是用你已经有的参数文件来初始化。</li></ol><h2 id="fine-tuning- 模型的三种状态">8.4 fine-tuning 模型的三种状态</h2><ol type="1"><li><p>状态一：只预测，不训练。 特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效；</p></li><li>状态二：训练，但只训练最后分类层。 特点：fine-tuning 的模型最终的分类以及符合要求，现在只是在他们的基础上进行类别降维。</li><li><p>状态三：完全训练，分类层 + 之前卷积层都训练 特点：跟状态二的差异很小，当然状态三比较耗时和需要训练 GPU 资源，不过非常适合 fine-tuning 到自己想要的模型里面，预测精度相比状态二也提高不少。</p></li></ol><h1 id="学习率">9. 学习率</h1><h2 id="学习率的作用">9.1 学习率的作用</h2><p>在机器学习中，监督式学习通过定义一个模型，并根据训练集上的数据估计最优参数。梯度下降法是一个广泛被用来最小化模型误差的参数优化算法。梯度下降法通过多次迭代，并在每一步中最小化成本函数（cost 来估计模型的参数）。学习率 (learning rate)，在迭代过程中会控制模型的学习进度。</p><p>在梯度下降法中，都是给定的统一的学习率，整个优化过程中都以确定的步长进行更新， 在迭代优化的前期中，学习率较大，则前进的步长就会较长，这时便能以较快的速度进行梯度下降，而在迭代优化的后期，逐步减小学习率的值，减小步长，这样将有助于算法的收敛，更容易接近最优解。故而如何对学习率的更新成为了研究者的关注点。</p><p>在模型优化中，常用到的几种学习率衰减方法有：分段常数衰减、多项式衰减、指数衰减、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减</p><h2 id="学习率衰减常用参数有哪些">9.2 学习率衰减常用参数有哪些</h2><table><thead><tr class="header"><th>参数名称</th><th>参数说明</th></tr></thead><tbody><tr class="odd"><td>learning_rate</td><td>初始学习率</td></tr><tr class="even"><td>global_step</td><td>用于衰减计算的全局步数，非负，用于逐步计算衰减指数</td></tr><tr class="odd"><td>decay_steps</td><td>衰减步数，必须是正值，决定衰减周期</td></tr><tr class="even"><td>decay_rate</td><td>衰减率</td></tr><tr class="odd"><td>end_learning_rate</td><td>最低的最终学习率</td></tr><tr class="even"><td>cycle</td><td>学习率下降后是否重新上升</td></tr><tr class="odd"><td>alpha</td><td>最小学习率</td></tr><tr class="even"><td>num_periods</td><td>衰减余弦部分的周期数</td></tr><tr class="odd"><td>initial_variance</td><td>噪声的初始方差</td></tr><tr class="even"><td>variance_decay</td><td>衰减噪声的方差</td></tr></tbody></table><h3 id="分段常数衰减">9.2.1 分段常数衰减</h3><p>分段常数衰减需要事先定义好的训练次数区间，在对应区间置不同的学习率的常数值，一般情况刚开始的学习率要大一些，之后要越来越小，要根据样本量的大小设置区间的间隔大小，样本量越大，区间间隔要小一点。下图即为分段常数衰减的学习率变化图，横坐标代表训练次数，纵坐标代表学习率。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026222538375.png" alt="image-20201026222538375" style="zoom:50%"></p><h3 id="指数衰减">9.2.2 指数衰减</h3><p>以指数衰减方式进行学习率的更新，学习率的大小和训练次数指数相关，其更新规则为： <span class="math display">\[decayed{\_}learning{\_}rate =learning{\_}rate*decay{\_}rate^{\frac{global{\_step}}{decay{\_}steps}} \]</span> 这种衰减方式简单直接，收敛速度快，是最常用的学习率衰减方式，如下图所示，绿色的为学习率随 训练次数的指数衰减方式，红色的即为分段常数衰减，它在一定的训练区间内保持学习率不变。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026223700750.png" alt="image-20201026223700750" style="zoom:50%"></p><h3 id="自然指数衰减">9.2.3 自然指数衰减</h3><p>它与指数衰减方式相似，不同的在于它的衰减底数是<span class="math inline">\(e\)</span>，故而其收敛的速度更快，一般用于相对比较 容易训练的网络，便于较快的收敛，其更新规则如下： <span class="math display">\[decayed{\_}learning{\_}rate =learning{\_}rate*e^{\frac{-decay{\_rate}}{global{\_}step}} \]</span> 下图为为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，很明可以看到自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026223843460.png" alt="image-20201026223843460" style="zoom:50%"></p><h3 id="多项式衰减 - 看不懂">9.2.4 多项式衰减 (看不懂 ==)</h3><p>应用多项式衰减的方式进行更新学习率，这里会给定初始学习率和最低学习率取值，然后将会按照 给定的衰减方式将学习率从初始值衰减到最低值, 其更新规则如下式所示。 <span class="math display">\[global{\_}step=min(global{\_}step,decay{\_}steps) \]</span></p><p><span class="math display">\[decayed{\_}learning{\_}rate =(learning{\_}rate-end{\_}learning{\_}rate)* \left(1-\frac{global{\_step}}{decay{\_}steps}\right)^{power} \\ +end{\_}learning{\_}rate \]</span></p><p>需要注意的是，有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay_steps 的倍数，取第一个大于 global_steps 的结果，如下式所示. 它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。 <span class="math display">\[decay{\_}steps = decay{\_}steps*ceil \left(\frac{global{\_}step}{decay{\_}steps}\right) \]</span> 如下图所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026224250502.png" alt="image-20201026224250502" style="zoom:50%"></p><h3 id="余弦衰减">9.2.5 余弦衰减</h3><p>余弦衰减就是采用余弦的相关方式进行学习率的衰减，衰减图和余弦函数相似。其更新机制如下式所示：</p><p><span class="math display">\[global{\_}step=min(global{\_}step,decay{\_}steps) \]</span></p><p><span class="math display">\[cosine{\_}decay=0.5*\left(1+cos\left( \pi* \frac{global{\_}step}{decay{\_}steps}\right)\right) \]</span></p><p><span class="math display">\[decayed=(1-\alpha)*cosine{\_}decay+\alpha \]</span></p><p><span class="math display">\[decayed{\_}learning{\_}rate=learning{\_}rate*decayed \]</span></p><p>如下图所示，红色即为标准的余弦衰减曲线，学习率从初始值下降到最低学习率后保持不变。蓝色的线是线性余弦衰减方式曲线，它是学习率从初始学习率以线性的方式下降到最低学习率值。绿色噪声线性余弦衰减方式。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/ 机器学习 / 深度学习 /image-20201026224331391.png" alt="image-20201026224331391" style="zoom:50%"></p><h1 id="深度学习中常用的数据增强方法">10.5 深度学习中常用的数据增强方法？</h1><ul><li><p>Color Jittering：对颜色的数据增强：图像亮度、饱和度、对比度变化（此处对色彩抖动的理解不知是否得当）；</p></li><li><p>PCA Jittering：首先按照 RGB 三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做 PCA Jittering；</p></li><li><p>Random Scale：尺度变换；</p></li><li><p>Random Crop：采用随机图像差值方式，对图像进行裁剪、缩放；包括 Scale Jittering 方法（VGG 及 ResNet 模型使用）或者尺度和长宽比增强变换；</p></li><li><p>Horizontal/Vertical Flip：水平 / 垂直翻转；</p></li><li><p>Shift：平移变换；</p></li><li><p>Rotation/Reflection：旋转 / 仿射变换；</p></li><li><p>Noise：高斯噪声、模糊处理；</p></li><li><p>Label Shuffle：类别不平衡数据的增广；</p></li></ul></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>小张同学</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://stuxiaozhang.github.io/2020/10/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="深度学习基础">http://stuxiaozhang.github.io/2020/10/24/深度学习基础/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2020/10/12/%E5%B0%8F%E5%BC%A0%E7%9A%84%E4%B8%89%E5%B9%B4%E5%9B%9E%E9%A1%BE/" rel="prev" title="小张的反思日记"><i class="fa fa-chevron-left"></i> 小张的反思日记</a></div><div class="post-nav-item"><a href="/2020/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" rel="next" title="《机器学习》课程笔记">《机器学习》课程笔记 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">1. 基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%84%E6%88%90"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 神经网络组成？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1.1 感知机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.1.2 多层感知机</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%B1%82%E8%A1%A8%E7%A4%BA"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 为什么使用深层表示?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%BE%E4%BB%A5%E8%AE%AD%E7%BB%83"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 为什么深层神经网络难以训练？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">1.3.1.</span> <span class="nav-text">1.3.1 梯度消失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">1.3.2.</span> <span class="nav-text">1.3.2 梯度爆炸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%E7%9F%A9%E9%98%B5%E7%9A%84%E9%80%80%E5%8C%96%E5%AF%BC%E8%87%B4%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9C%89%E6%95%88%E8%87%AA%E7%94%B1%E5%BA%A6%E5%87%8F%E5%B0%91"><span class="nav-number">1.3.3.</span> <span class="nav-text">1.3.3 权重矩阵的退化导致模型的有效自由度减少。</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 深度学习和机器学习有什么不同？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E6%93%8D%E4%BD%9C%E4%B8%8E%E8%AE%A1%E7%AE%97"><span class="nav-number">2.</span> <span class="nav-text">2. 网络操作与计算</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 前向传播与反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%BE%93%E5%87%BA"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 如何计算神经网络的输出？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BE%93%E5%87%BA%E5%80%BC"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 如何计算卷积神经网络输出值？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%20-pooling-%20%E5%B1%82%E8%BE%93%E5%87%BA%E5%80%BC%E8%BE%93%E5%87%BA%E5%80%BC"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 如何计算 Pooling 层输出值输出值？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9B%B4%E6%B7%B1%E6%9C%89%E4%BB%80%E4%B9%88%E6%84%8F%E4%B9%89"><span class="nav-number">2.5.</span> <span class="nav-text">2.5 神经网络更“深”有什么意义？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">3.</span> <span class="nav-text">3. 超参数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 什么是超参数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%AF%BB%E6%89%BE%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E6%9C%80%E4%BC%98%E5%80%BC"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 如何寻找超参数的最优值？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2%E4%B8%80%E8%88%AC%E8%BF%87%E7%A8%8B"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 超参数搜索一般过程？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.</span> <span class="nav-text">4. 激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 为什么需要非线性激活函数？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.1.1.</span> <span class="nav-text">4.1.1 为什么需要激活函数？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E9%9C%80%E8%A6%81%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0"><span class="nav-number">4.1.2.</span> <span class="nav-text">4.1.2 为什么激活函数需要非线性函数？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%9C%89%E5%93%AA%E4%BA%9B%E6%80%A7%E8%B4%A8"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 激活函数有哪些性质？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%8F%8A%E5%9B%BE%E5%83%8F"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 常见的激活函数及图像</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sigmoid-%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.1.</span> <span class="nav-text">4.3.1 sigmoid 激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax-%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.2.</span> <span class="nav-text">4.3.2 softmax 激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sigmoid-%20%E5%87%BD%E6%95%B0%E5%92%8C%20-softmax-%20%E5%87%BD%E6%95%B0%20-%20%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">4.3.2.1.</span> <span class="nav-text">sigmoid 函数和 softmax 函数 的区别？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax-%20%E5%87%BD%E6%95%B0%E5%A6%82%E4%BD%95%E5%BA%94%E7%94%A8%E4%BA%8E%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="nav-number">4.3.2.2.</span> <span class="nav-text">softmax 函数如何应用于多分类？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tanh-%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.3.</span> <span class="nav-text">4.3.3 tanh 激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%20tanh%20%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6%E6%AF%94%20sigmoid%20%E5%BF%AB"><span class="nav-number">4.3.3.1.</span> <span class="nav-text">为什么 tanh 收敛速度比 sigmoid 快？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#relu-%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.4.</span> <span class="nav-text">4.3.4 ReLU 激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%8E%E6%A0%B7%E7%90%86%E8%A7%A3%20-relu-0-%20%E6%97%B6%E6%98%AF%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.4.1.</span> <span class="nav-text">怎样理解 Relu（&lt; 0 时）是非线性激活函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%20-relu-%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%98%E7%82%B9%20-%20%E4%B8%BA%E4%BB%80%E4%B9%88%20relu%20%E8%A6%81%E5%A5%BD%E8%BF%87%20sigmoid%20%E5%92%8C%20tanh"><span class="nav-number">4.3.4.2.</span> <span class="nav-text">使用 ReLu 激活函数的优点？&#x2F; 为什么 ReLu 要好过 sigmoid 和 tanh？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#leaky-relu-%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.5.</span> <span class="nav-text">4.3.5 Leaky Relu 激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softplus-%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.6.</span> <span class="nav-text">4.3.6 SoftPlus 激活函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="nav-number">4.4.</span> <span class="nav-text">4.4 常见激活函数的导数计算？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">4.5.</span> <span class="nav-text">4.5 如何选择激活函数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">4.6.</span> <span class="nav-text">4.6 交叉熵损失函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dropout-%20%E7%B3%BB%E5%88%97%E9%97%AE%E9%A2%98"><span class="nav-number">5.</span> <span class="nav-text">5. Dropout 系列问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 为什么要正则化？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AD%A3%E5%88%99%E5%8C%96%E6%9C%89%E5%88%A9%E4%BA%8E%E9%A2%84%E9%98%B2%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 为什么正则化有利于预防过拟合？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%A7%A3%20dropout%20%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 理解 dropout 正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout%20%E7%8E%87%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">5.4.</span> <span class="nav-text">5.4 dropout 率的选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout%20%E6%9C%89%E4%BB%80%E4%B9%88%E7%BC%BA%E7%82%B9"><span class="nav-number">5.5.</span> <span class="nav-text">5.5 dropout 有什么缺点？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#batch_size%20%E8%BF%99%E5%9D%97%E4%B8%8D%E5%A4%AA%E6%87%82"><span class="nav-number">6.</span> <span class="nav-text">6. Batch_Size(这块不太懂 &#x3D;&#x3D;)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%20-batch_size"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 为什么需要 Batch_Size？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#batch_size-%20%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 Batch_Size 值的选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8%E5%90%88%E7%90%86%E8%8C%83%E5%9B%B4%E5%86%85%E5%A2%9E%E5%A4%A7%20batch_size%20%E6%9C%89%E4%BD%95%E5%A5%BD%E5%A4%84"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 在合理范围内，增大 Batch_Size 有何好处？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B2%E7%9B%AE%E5%A2%9E%E5%A4%A7%20-batch_size-%20%E6%9C%89%E4%BD%95%E5%9D%8F%E5%A4%84"><span class="nav-number">6.4.</span> <span class="nav-text">6.4 盲目增大 Batch_Size 有何坏处？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%83%E8%8A%82%20-batch_size-%20%E5%AF%B9%E8%AE%AD%E7%BB%83%E6%95%88%E6%9E%9C%E5%BD%B1%E5%93%8D%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95"><span class="nav-number">6.5.</span> <span class="nav-text">6.5 调节 Batch_Size 对训练效果影响到底如何？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">7.</span> <span class="nav-text">7. 归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E5%90%AB%E4%B9%89"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 归一化含义？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 为什么要归一化？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BD%92%E4%B8%80%E5%8C%96%E8%83%BD%E6%8F%90%E9%AB%98%E6%B1%82%E8%A7%A3%E6%9C%80%E4%BC%98%E8%A7%A3%E9%80%9F%E5%BA%A6"><span class="nav-number">7.3.</span> <span class="nav-text">7.3 为什么归一化能提高求解最优解速度？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#d-%20%E5%9B%BE%E8%A7%A3%E6%9C%AA%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">7.4.</span> <span class="nav-text">7.4 3D 图解未归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E6%9C%89%E5%93%AA%E4%BA%9B%E7%B1%BB%E5%9E%8B"><span class="nav-number">7.5.</span> <span class="nav-text">7.5 归一化有哪些类型？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">7.5.1.</span> <span class="nav-text">7.5.1 线性归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%B7%AE%E6%A0%87%E5%87%86%E5%8C%96"><span class="nav-number">7.5.2.</span> <span class="nav-text">7.5.2 标准差标准化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">7.5.3.</span> <span class="nav-text">7.5.3 非线性归一化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%20batch-normalization"><span class="nav-number">7.6.</span> <span class="nav-text">7.6 什么是批归一化（Batch Normalization）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%20bn%20%E4%BC%98%E7%82%B9"><span class="nav-number">7.6.1.</span> <span class="nav-text">7.6.1 批归一化（BN）优点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%20bn%20%E6%B5%81%E7%A8%8B"><span class="nav-number">7.6.2.</span> <span class="nav-text">7.6.2 批归一化（BN）流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#batch-normalization%20%E5%9C%A8%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E7%94%A8%E6%AF%94%E8%BE%83%E5%90%88%E9%80%82"><span class="nav-number">7.6.3.</span> <span class="nav-text">7.6.3 Batch Normalization 在什么时候用比较合适？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83%20fine-tuning"><span class="nav-number">8.</span> <span class="nav-text">8. 预训练与微调(fine tuning)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E5%8F%AF%E4%BB%A5%E5%B8%AE%E5%8A%A9%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">8.1.</span> <span class="nav-text">8.1 为什么无监督预训练可以帮助深度学习？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%20-fine-tuning"><span class="nav-number">8.2.</span> <span class="nav-text">8.2 什么是模型微调 fine tuning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83%E6%97%B6%E5%80%99%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E6%98%AF%E5%90%A6%E6%9B%B4%E6%96%B0"><span class="nav-number">8.3.</span> <span class="nav-text">8.3 微调时候网络参数是否更新？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fine-tuning-%20%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%89%E7%A7%8D%E7%8A%B6%E6%80%81"><span class="nav-number">8.4.</span> <span class="nav-text">8.4 fine-tuning 模型的三种状态</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="nav-number">9.</span> <span class="nav-text">9. 学习率</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-number">9.1.</span> <span class="nav-text">9.1 学习率的作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E5%B8%B8%E7%94%A8%E5%8F%82%E6%95%B0%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="nav-number">9.2.</span> <span class="nav-text">9.2 学习率衰减常用参数有哪些</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E6%AE%B5%E5%B8%B8%E6%95%B0%E8%A1%B0%E5%87%8F"><span class="nav-number">9.2.1.</span> <span class="nav-text">9.2.1 分段常数衰减</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E8%A1%B0%E5%87%8F"><span class="nav-number">9.2.2.</span> <span class="nav-text">9.2.2 指数衰减</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E6%8C%87%E6%95%B0%E8%A1%B0%E5%87%8F"><span class="nav-number">9.2.3.</span> <span class="nav-text">9.2.3 自然指数衰减</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E8%A1%B0%E5%87%8F%20-%20%E7%9C%8B%E4%B8%8D%E6%87%82"><span class="nav-number">9.2.4.</span> <span class="nav-text">9.2.4 多项式衰减 (看不懂 &#x3D;&#x3D;)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%99%E5%BC%A6%E8%A1%B0%E5%87%8F"><span class="nav-number">9.2.5.</span> <span class="nav-text">9.2.5 余弦衰减</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95"><span class="nav-number">10.</span> <span class="nav-text">10.5 深度学习中常用的数据增强方法？</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="小张同学" src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/codedog.jpg"><p class="site-author-name" itemprop="name">小张同学</p><div class="site-description" itemprop="description">不要停止奔跑 不要回顾来路</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">25</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=709634432" title="QQ → tencent:&#x2F;&#x2F;AddContact&#x2F;?fromId&#x3D;50&amp;fromSubId&#x3D;1&amp;subcmd&#x3D;all&amp;uin&#x3D;709634432" rel="noopener" target="_blank"><i class="fa fa-fw fa-fab fa-qq"></i>QQ</a> </span><span class="links-of-author-item"><a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2020 – <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">小张同学</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">257k</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="//cdn.jsdelivr.net/npm/animejs@3.1.0/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>"undefined"==typeof MathJax?(window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},tags:"ams"},options:{renderActions:{findScript:[10,n=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const a=new n.options.MathItem(e.textContent,n.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),a.start={node:t,delim:"",n:0},a.end={node:t,delim:"",n:0},n.math.push(a)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!1,notify:!1,appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",avatar:"mm",meta:e,pageSize:"10",visitor:!1,lang:"zh-cn",path:location.pathname,recordIP:!0,serverURLs:""})},window.Valine)})</script></body></html>