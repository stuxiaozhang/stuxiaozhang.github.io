<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.3.0"><script></script><link rel="apple-touch-icon" sizes="180x180" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/avatar-removebg-preview.png"><link rel="icon" type="image/png" sizes="32x32" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/avatar-removebg-preview.png"><link rel="icon" type="image/png" sizes="16x16" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/avatar-removebg-preview.png"><link rel="mask-icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/avatar-removebg-preview.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!1,style:"flat"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!0,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="这一章看的很吃力，等以后懂得多了也许就明白了吧...            改善深层神经网络：超参数调试、正则化以及优化 深度学习的实用层面 数据划分：训练&#x2F;验证&#x2F;测试集 应用深度学习是一个典型的迭代过程。"><meta property="og:type" content="article"><meta property="og:title" content="《深度学习》课程笔记2_改善深层神经网络"><meta property="og:url" content="http://stuxiaozhang.github.io/2020/11/16/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B02/index.html"><meta property="og:site_name" content="小张同学的博客"><meta property="og:description" content="这一章看的很吃力，等以后懂得多了也许就明白了吧...            改善深层神经网络：超参数调试、正则化以及优化 深度学习的实用层面 数据划分：训练&#x2F;验证&#x2F;测试集 应用深度学习是一个典型的迭代过程。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201116195537497.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201117094946442.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201117142928488.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201118140135287.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201118142621742.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201118152903889.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201121105508543.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201122100905548.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201122101038303.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201122103336971.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201123134528833.png"><meta property="og:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201123104538968.png"><meta property="article:published_time" content="2020-11-16T02:04:39.000Z"><meta property="article:modified_time" content="2021-01-02T09:22:45.136Z"><meta property="article:author" content="小张同学"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201116195537497.png"><link rel="canonical" href="http://stuxiaozhang.github.io/2020/11/16/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B02/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>《深度学习》课程笔记2_改善深层神经网络 | 小张同学的博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">小张同学的博客</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">天道酬勤</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a></li><li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>动态</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://stuxiaozhang.github.io/2020/11/16/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B02/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/codedog.jpg"><meta itemprop="name" content="小张同学"><meta itemprop="description" content="不要停止奔跑 不要回顾来路"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="小张同学的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">《深度学习》课程笔记2_改善深层神经网络</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-11-16 10:04:39" itemprop="dateCreated datePublished" datetime="2020-11-16T10:04:39+08:00">2020-11-16</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/2020/11/16/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B02/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2020/11/16/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B02/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>18k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>16 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><div class="note danger"><p>这一章看的很吃力，等以后懂得多了也许就明白了吧...</p></div><h1 id="改善深层神经网络超参数调试正则化以及优化">改善深层神经网络：超参数调试、正则化以及优化</h1><h1 id="深度学习的实用层面">深度学习的实用层面</h1><h2 id="数据划分训练验证测试集">数据划分：训练/验证/测试集</h2><p>应用深度学习是一个典型的迭代过程。</p><p>对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：</p><ul><li>训练集（train set）：用训练集对算法或模型进行<strong>训练</strong>过程；</li><li>验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行<strong>交叉验证</strong>，<strong>选择出最好的模型</strong>；</li><li>测试集（test set）：最后利用测试集对模型进行测试，<strong>获取模型运行的无偏估计</strong>（对学习方法进行评估）。</li></ul><p>在<strong>小数据量</strong>的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：</p><ul><li>无验证集的情况：70% / 30%；</li><li>有验证集的情况：60% / 20% / 20%；</li></ul><p>而在如今的<strong>大数据时代</strong>，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p><p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。</p><p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。</p><ul><li>100 万数据量：98% / 1% / 1%；</li><li>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</li></ul><h3 id="建议">建议</h3><p>建议<strong>验证集要和训练集来自于同一个分布</strong>（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。</p><p>如果不需要用<strong>无偏估计</strong>来评估模型的性能，则可以不需要测试集。</p><h3 id="补充1.-交叉验证cross-validation">补充：1. 交叉验证（cross validation）</h3><p>交叉验证的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。</p><h3 id="无偏估计">2. 无偏估计</h3><p>无偏估计是用样本统计量来估计总体参数时的一种无偏推断。估计量的数学期望等于被估计参数的真实值，则称此估计量为被估计参数的无偏估计，即具有无偏性，是一种用于评价估计量优良性的准则。无偏估计的意义是：在多次重复下，它们的平均数接近所估计的参数真值。无偏估计常被应用于测验分数统计中。</p><h2 id="模型估计偏差方差">模型估计：偏差/方差</h2><p><strong>“偏差-方差分解”（bias-variance decomposition）</strong>是解释学习算法泛化性能的一种重要工具。</p><p>泛化误差可分解为偏差、方差与噪声之和：</p><ul><li><strong>偏差</strong>：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了<strong>学习算法本身的拟合能力</strong>；</li><li><strong>方差</strong>：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong>；</li><li><strong>噪声</strong>：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了<strong>学习问题本身的难度</strong>。</li></ul><p>偏差-方差分解说明，<strong>泛化性能</strong>是由<strong>学习算法的能力</strong>、<strong>数据的充分性</strong>以及<strong>学习任务本身的难度</strong>所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。</p><p>在<strong>欠拟合（underfitting）</strong>的情况下，出现<strong>高偏差（high bias）</strong>的情况，即不能很好地对数据进行分类。</p><p>当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现<strong>过拟合（overfitting）</strong>的情况，在验证集上出现<strong>高方差（high variance）</strong>的现象。</p><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201116195537497.png" alt="image-20201116195537497"><figcaption>image-20201116195537497</figcaption></figure><p>当训练出一个模型以后，如果：</p><ul><li>训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合；</li><li>训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合；</li><li>训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差；</li><li>训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。</li></ul><p>偏差和方差的权衡问题对于模型来说十分重要。</p><p>最优误差通常也称为“贝叶斯误差”。</p><h3 id="应对方法">应对方法</h3><p>存在高偏差：</p><ul><li>扩大网络规模，如添加隐藏层或隐藏单元数目；</li><li>寻找合适的网络架构，使用更大的 NN 结构；</li><li>花费更长时间训练。</li></ul><p>存在高方差：</p><ul><li>获取更多的数据；</li><li>正则化（regularization）；</li><li>寻找更合适的网络结构。</li></ul><p>不断尝试，直到找到低偏差、低方差的框架。</p><p>在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。</p><h2 id="正则化regularization">正则化（regularization）</h2><p><strong>正则化</strong>是在成本函数中加入一个正则化项，惩罚模型的复杂度。<strong>正则化可以用于解决高方差的问题。</strong></p><h3 id="logistic-回归中的正则化">Logistic 回归中的正则化</h3><p>对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数： <span class="math display">\[ J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}{||w||}^2_2 \]</span></p><ul><li>L2 正则化：</li></ul><p><span class="math display">\[ \frac{\lambda}{2m}{||w||}^2_2 = \frac{\lambda}{2m}\sum_{j=1}^{n_x}w^2_j = \frac{\lambda}{2m}w^Tw \]</span></p><ul><li>L1 正则化：</li></ul><p><span class="math display">\[ \frac{\lambda}{2m}{||w||}_1 = \frac{ \lambda }{2m}\sum_{j=1}^{n_x}{|w_j|} \]</span></p><p>其中，λ 为<strong>正则化因子</strong>，是<strong>超参数</strong>。</p><p>由于 L1 正则化最后得到 w 向量中将存在大量的 0，会使模型变得稀疏化。</p><p>相比之下，L2 正则化更加常用。</p><p><strong>注意</strong>，<code>lambda</code>在 Python 中属于保留字，所以在编程的时候，用<code>lambd</code>代替这里的正则化因子。</p><blockquote><p>数学上，范数是一个向量空间或矩阵上所有向量的长度和大小的求和。</p><p>like：<span class="math inline">\(||x||\)</span> ,x可以是一个向量或者矩阵。</p><p>例如一个向量 <span class="math inline">\(a = [3,-2,1]\)</span>，其欧几里得范数为：$ ||a||_2 = $</p></blockquote><h3 id="神经网络中的正则化">神经网络中的正则化</h3><p>对于神经网络，加入正则化的成本函数： <span class="math display">\[ J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L{ {||w^{[l]}||} }^2_F \]</span> 因为 w 的大小为 (<span class="math inline">\(n^{[l−1]}\)</span>, <span class="math inline">\(n^{[l]}\)</span>)，因此 <span class="math display">\[ { {||w^{[l]}||} }^2_F = \sum^{n^{[l-1]} }_{i=1}\sum^{n^{[l]} }_{j=1}(w^{[l]}_{ij})^2 \]</span> 该矩阵范数被称为<strong>弗罗贝尼乌斯范数（Frobenius Norm）</strong>，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。</p><h4 id="权重衰减weight-decay">权重衰减（Weight decay）</h4><p><strong>在加入正则化项后，梯度变为</strong>（反向传播要按这个计算）： <span class="math display">\[ dW^{[l]}= \frac{\partial L}{\partial w^{[l]}} +\frac{\lambda}{m}W^{[l]} \]</span> 代入梯度更新公式： <span class="math display">\[ W^{[l]} := W^{[l]}-\alpha dW^{[l]} \]</span> 可得： <span class="math display">\[ W^{[l]} := W^{[l]} - \alpha [\frac{\partial L}{\partial w^{[l]}} + \frac{\lambda}{m}W^{[l]}]\\ = W^{[l]} - \alpha \frac{\lambda}{m}W^{[l]} - \alpha \frac{\partial L}{\partial w^{[l]}}\\ = (1 - \frac{\alpha\lambda}{m})W^{[l]} - \alpha \frac{\partial L}{\partial w^{[l]}} \]</span> 其中，因为 <span class="math inline">\(1−\frac{αλ}{m}&lt;1\)</span>，会给原来的 <span class="math inline">\(W^{[l]}\)</span>一个衰减的参数，因此 L2 正则化项也被称为<strong>权重衰减（Weight Decay）</strong>。</p><h3 id="正则化可以减小过拟合的原因">正则化可以减小过拟合的原因</h3><h4 id="直观解释">直观解释</h4><p>正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，<strong>直观上</strong>相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。</p><h4 id="数学解释">数学解释</h4><p>假设神经元中使用的激活函数为<code>g(z) = tanh(z)</code>（sigmoid 同理）。</p><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201117094946442.png" alt="image-20201117094946442"><figcaption>image-20201117094946442</figcaption></figure><p>在加入正则化项后，当 λ 增大，导致 <span class="math inline">\(W^{[l]}\)</span>减小，<span class="math inline">\(Z^{[l]}=W^{[l]}a^{[l−1]}+b^{[l]}\)</span> 便会减小。由上图可知，在 z 较小（接近于 0）的区域里，<code>tanh(z)</code>函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。</p><h4 id="其他解释">其他解释</h4><p>在权值 <span class="math inline">\(w^{[L]}\)</span>变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。</p><h2 id="dropout-正则化">dropout 正则化</h2><p><strong>dropout（随机失活）</strong>是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在<strong>计算机视觉（Computer Vision）</strong>领域。</p><h3 id="反向随机失活inverted-dropout">反向随机失活（Inverted dropout）</h3><p>反向随机失活是实现 dropout 的方法。对第 <code>l</code> 层进行 dropout：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = <span class="number">0.8</span>    <span class="comment"># 设置神经元保留概率</span></span><br><span class="line">dl = np.random.rand(al.shape[<span class="number">0</span>], al.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">al = np.multiply(al, dl)</span><br><span class="line">al /= keep_prob</span><br></pre></td></tr></table></figure><p>最后一步 <code>al /= keep_prob</code> 是因为 <span class="math inline">\(a^{[l]}\)</span>中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 <span class="math inline">\(Z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}\)</span> 的期望值，因此除以一个 <code>keep_prob</code> 。</p><p><strong>注意</strong>，在<strong>测试阶段不要使用 dropout</strong>，因为那样会使得预测结果变得随机。</p><h3 id="理解-dropout">理解 dropout</h3><p>对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。</p><p>因此，通过传播过程，dropout 将产生和 L2 正则化相同的<strong>收缩权重</strong>的效果。</p><p>对于不同的层，设置的 <code>keep_prob</code> 也不同。一般来说，神经元较少的层，会设 <code>keep_prob</code> 为 1.0，而神经元多的层则会设置比较小的 <code>keep_prob</code>。</p><p>dropout 的一大<strong>缺点</strong>是<u>成本函数无法被明确定义</u>。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将 <code>keep_prob</code> 全部设置为 1.0 后运行代码，确保 <span class="math inline">\(J(w,b)\)</span> 函数单调递减，再打开 dropout。</p><h2 id="其他正则化方法">其他正则化方法</h2><ul><li>数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。</li><li>早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，当训练集误差降低但验证集误差升高，两者开始发生较大偏差时及时停止迭代，并返回具有最小验证集误差的连接权和阈值，以避免过拟合。这种方法的优点是只运行一次梯度下降，就可以找到 w 的较小值，中间值和较大值，而无需尝试 <span class="math inline">\(L2\)</span> 正则化找 <span class="math inline">\(\lambda\)</span> 的很多值。缺点是无法同时达成偏差和方差的最优。</li></ul><h2 id="标准化输入">标准化输入</h2><p>使用标准化处理输入 X 能够有效加速收敛。</p><h3 id="标准化公式">标准化公式</h3><p><span class="math display">\[ x = \frac{x - \mu}{\sigma} \]</span></p><p>其中， <span class="math display">\[ \mu = \frac{1}{m}\sum^m_{i=1}x^{(i)} \]</span></p><p><span class="math display">\[ \sigma = \sqrt{\frac{1}{m}\sum^m_{i=1}x^{ {(i)}^2} } \]</span></p><h3 id="使用标准化的原因">使用标准化的原因</h3><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201117142928488.png" alt="image-20201117142928488"><figcaption>image-20201117142928488</figcaption></figure><p>有图可知，使用标准化前后，成本函数的形状有较大差别。</p><p>在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。</p><h2 id="梯度消失和梯度爆炸">梯度消失和梯度爆炸</h2><p>在梯度函数上出现的以指数级递增或者递减的情况分别称为<strong>梯度爆炸</strong>或者<strong>梯度消失</strong>。</p><p>假定 <span class="math inline">\(g(z)=z,b^{[l]}=0\)</span>，对于目标输出有： <span class="math display">\[ \hat{y} = W^{[L]}W^{[L-1]}...W^{[2]}W^{[1]}X \]</span></p><ul><li>对于 <span class="math inline">\(W^{[l]}\)</span> 的值大于 1 的情况，激活函数的值将以指数级递增；</li><li>对于 <span class="math inline">\(W^{[l]}\)</span> 的值小于 1 的情况，激活函数的值将以指数级递减。</li></ul><p>对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。</p><h3 id="利用初始化缓解梯度消失和梯度爆炸">利用初始化缓解梯度消失和梯度爆炸</h3><p>根据 <span class="math display">\[ z={w}_1{x}_1+{w}_2{x}_2 + ... + {w}_n{x}_n + b \]</span> 为了预防 z 值过大或过小，当输入的数量 n 较大时，我们希望每个 <span class="math inline">\(w_i\)</span> 的值都小一些，这样它们的和得到的 z 也较小。</p><p>为了得到较小的 <span class="math inline">\(w_i\)</span> ，设置 <code>Var(wi)=1/n</code>，这里称为 <strong>Xavier initialization</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WL = np.random.randn(WL.shape[<span class="number">0</span>], WL.shape[<span class="number">1</span>]) * np.sqrt(<span class="number">1</span>/n)</span><br></pre></td></tr></table></figure><p>其中 n 是输入的神经元个数，即 <code>WL.shape[1]</code>。</p><p>这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。</p><p>同理，也有 <strong>He Initialization</strong>。它和 Xavier initialization 唯一的区别是<code>Var(wi)=2/n</code>，适用于 <strong>ReLU</strong> 作为激活函数时。</p><p>当激活函数使用 ReLU 时，<code>Var(wi)=2/n</code> ；当激活函数使用 tanh 时，<code>Var(wi)=1/n</code>。</p><h2 id="梯度检验gradient-checking">梯度检验（Gradient checking）</h2><h3 id="梯度的数值逼近">梯度的数值逼近</h3><p>使用双边误差的方法去逼近导数，精度要高于单边误差。</p><ul><li>单边误差：</li></ul><p><span class="math display">\[ f&#39;(\theta) = {\lim_{\varepsilon\to 0}} = \frac{f(\theta + \varepsilon) - (\theta)}{\varepsilon} \]</span></p><ul><li>双边误差求导：</li></ul><p><span class="math display">\[ f&#39;(\theta) = {\lim_{\varepsilon\to 0}} = \frac{f(\theta + \varepsilon) - (\theta - \varepsilon)}{2\varepsilon} \]</span></p><p>当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。</p><h3 id="梯度检验的实施">梯度检验的实施</h3><h4 id="连接参数">连接参数</h4><p>将 <span class="math inline">\(W^{[1]}\)</span>，<span class="math inline">\(b^{[1]}\)</span>，...，<span class="math inline">\(W^[L]\)</span>，<span class="math inline">\(b^{[l]}\)</span> 全部连接出来，成为一个巨型向量 θ。这样， <span class="math display">\[ J(W^{[1]}, b^{[1]}, ..., W^{[L]}，b^{[L]}) = J(\theta) \]</span> 同时，对 <span class="math inline">\(dW^{[1]}\)</span>，<span class="math inline">\(db^{[1]}\)</span>，...，<span class="math inline">\(dW^[L]\)</span>，<span class="math inline">\(db^{[l]}\)</span> 执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。</p><p>现在，我们需要找到 dθ 和代价函数 J 的梯度的关系。</p><h4 id="进行梯度检验">进行梯度检验</h4><p>现在的问题的是： <span class="math inline">\(d\theta\)</span> 和代价函数 <span class="math inline">\(J\)</span> 的梯度有什么关系？</p><p>答：<strong>grad check</strong> ，梯度检验。首先我们要清楚 <span class="math inline">\(J\)</span> 函数是超参数 <span class="math inline">\(\theta\)</span> 的一个函数，也可以将 <span class="math inline">\(J\)</span> 函数展开为 <span class="math inline">\(J(\theta_1, \theta_2, \theta_3, ...)\)</span> ，不论超参数 <span class="math inline">\(\theta\)</span> 的维度是多少，为了实施梯度检验，要做的就是循环执行，从而对每个 <span class="math inline">\(i\)</span> 也就是对每个 <span class="math inline">\(\theta\)</span> 组成元素计算 的值，在这里使用双边误差。即： <span class="math display">\[ d\theta_{approx}[i] ＝ \frac{J(\theta_1, \theta_2, ..., \theta_i+\varepsilon, ...) - J(\theta_1, \theta_2, ..., \theta_i-\varepsilon, ...)}{2\varepsilon} \]</span> 正常情况下应该： <span class="math display">\[ d\theta_{approx}[i] \approx{d\theta[i]} = \frac{\partial J}{\partial \theta_i} \]</span> 那，如何验证这两个向量相互逼近呢？</p><p>答：欧几里得距离。然后用向量长度归一化，即使用向量长度的欧几里得范数来预防向量过大或过小。即用梯度检验值： <span class="math display">\[ \frac{ {||d\theta_{approx} - d\theta||}_{2} }{ {||d\theta_{approx}||}_2+{||d\theta||}_{2} } \]</span> 来检验反向传播的实施是否正确。</p><ol type="1"><li>这个值如果 <span class="math inline">\(\approx 10^{-7}\)</span> ，说明神经网络很好，神经网络的实施是正确的。倒数逼近很有可能是正确的。</li><li>这个值如果 <span class="math inline">\(\approx 10^{-5}\)</span> ，要小心，也许没问题，但是要再次检查，可能有bug。</li><li>这个值如果 <span class="math inline">\(\approx 10^{-3}\)</span> ，应该是有错，应该仔细检查所有 <span class="math inline">\(\theta\)</span> 项，看是否有一个具体的 <span class="math inline">\(i\)</span> 值，使得 <span class="math inline">\(d\theta_{approx}\)</span> 和 <span class="math inline">\(d\theta[i]\)</span> 大不相同，并用它来追踪一些求导计算是否正确，经过一些调试，最终结果会是这种非常小的值 <span class="math inline">\((10^{-7})\)</span> 。</li></ol><p>其中， <span class="math display">\[ {||x||}_2 = \sum^N_{i=1}{|x_i|}^2 \]</span> 表示向量 x 的 2-范数（也称“欧几里德范数”）。</p><p>​</p><h3 id="在神经网络实施梯度检验的使用技巧和注意事项">在神经网络实施梯度检验的使用技巧和注意事项</h3><ol type="1"><li>不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；</li><li>如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；</li><li>当成本函数包含正则项时，也需要带上正则项进行检验；</li><li>梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；</li></ol><h1 id="优化算法">优化算法</h1><p>深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。</p><h2 id="batch-梯度下降法">batch 梯度下降法</h2><p><strong>batch 梯度下降法</strong>（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。</p><p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。</p><p>但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 <strong>mini-batch</strong>。</p><h2 id="mini-batch-梯度下降法">Mini-Batch 梯度下降法</h2><p><strong>Mini-Batch 梯度下降法</strong>（小批量梯度下降法）每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。</p><p>使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。</p><p>batch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201118140135287.png" alt="image-20201118140135287" style="zoom:50%"></p><h3 id="batch-的不同大小size带来的影响">batch 的不同大小（size）带来的影响</h3><ul><li>mini-batch 的大小为 1，即是<strong>随机梯度下降法（stochastic gradient descent）</strong>，每个样本都是独立的 mini-batch；</li><li>mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；</li></ul><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201118142621742.png" alt="image-20201118142621742"><figcaption>image-20201118142621742</figcaption></figure><ul><li>batch 梯度下降法：<ul><li>对所有 m 个训练样本执行一次梯度下降，<strong>每一次迭代时间较长，训练过程慢</strong>；</li><li>相对噪声低一些，幅度也大一些；</li><li>成本函数总是向减小的方向下降。</li></ul></li><li>随机梯度下降法：<ul><li>对每一个训练样本执行一次梯度下降，训练速度快，但<strong>丢失了向量化带来的计算加速</strong>；</li><li>有很多噪声，减小学习率可以适当；</li><li>成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。</li></ul></li></ul><p>因此，选择一个 <code>1 &lt; size &lt; m</code> 的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。</p><h3 id="mini-batch-大小的选择">mini-batch 大小的选择</h3><ul><li>如果训练样本的大小比较小，如 m ⩽ 2000 时，选择 batch 梯度下降法；</li><li>如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 <span class="math inline">\(2^6、2^7、...、2^9\)</span>；</li><li>mini-batch 的大小要符合 CPU/GPU 内存。</li></ul><p>mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。</p><h3 id="获得-mini-batch-的步骤">获得 mini-batch 的步骤</h3><ol type="1"><li>将数据集打乱；</li><li>按照既定的大小分割数据集；</li></ol><p>其中打乱数据集的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m = X.shape[<span class="number">1</span>] </span><br><span class="line">permutation = <span class="built_in">list</span>(np.random.permutation(m))</span><br><span class="line">shuffled_X = X[:, permutation]</span><br><span class="line">shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))</span><br></pre></td></tr></table></figure><p><code>np.random.permutation</code>与<code>np.random.shuffle</code>有两处不同：</p><ol type="1"><li>如果传给<code>permutation</code>一个矩阵，它会返回一个洗牌后的矩阵副本；而<code>shuffle</code>只是对一个矩阵进行洗牌，没有返回值。</li><li>如果传入一个整数，它会返回一个洗牌后的<code>arange</code>。</li></ol><h3 id="符号表示">符号表示</h3><ul><li>使用上角小括号 i 表示训练集里的值，<span class="math inline">\(x^{(i)}\)</span> 是第 i 个训练样本；</li><li>使用上角中括号 l 表示神经网络的层数，<span class="math inline">\(z^{[l]}\)</span> 表示神经网络中第 l 层的 z 值；</li><li>现在引入大括号 t 来代表不同的 mini-batch，因此有 <span class="math inline">\(X_t\)</span>、<span class="math inline">\(Y_t\)</span>。</li></ul><div class="note danger"><p>这一块开始看的很吃力，基本上没怎么看懂...以后有机回会来再看一次555</p></div><h2 id="指数平均加权">指数平均加权</h2><p><strong>指数加权平均（Exponentially Weight Average）</strong>是一种常用的序列数据处理方式，计算公式为： <span class="math display">\[ S_t = \begin{cases} Y_1, &amp;t = 1 \\ \beta S_{t-1} + (1-\beta)Y_t, &amp;t &gt; 1 \end{cases} \]</span> 其中 <span class="math inline">\(Y_t\)</span> 为 t 下的实际值，<span class="math inline">\(S_t\)</span> 为 t 下加权平均后的值，β 为权重值。</p><p>指数加权平均数在统计学中被称为“指数加权移动平均值”。</p><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201118152903889.png" alt="image-20201118152903889"><figcaption>image-20201118152903889</figcaption></figure><p>给定一个时间序列，例如伦敦一年每天的气温值，图中蓝色的点代表真实数据。对于一个即时的气温值，取权重值 β 为 0.9，根据求得的值可以得到图中的红色曲线，它反映了气温变化的大致趋势。</p><p>当取权重值 β=0.98 时，可以得到图中更为平滑的绿色曲线。而当取权重值 β=0.5 时，得到图中噪点更多的黄色曲线。<strong>β 越大相当于求取平均利用的天数越多</strong>，曲线自然就会越平滑而且越滞后。</p><h3 id="理解指数平均加权">理解指数平均加权</h3><p>当 β 为 0.9 时， <span class="math display">\[ v_{100} = 0.9v_{99} + 0.1 \theta_{100} \]</span></p><p><span class="math display">\[ v_{99} = 0.9v_{98} + 0.1 \theta_{99} \]</span></p><p><span class="math display">\[ v_{98} = 0.9v_{97} + 0.1 \theta_{98} \]</span></p><p><span class="math display">\[ ... \]</span></p><p>展开： <span class="math display">\[ v_{100} = 0.1 \theta_{100} + 0.1 * 0.9 \theta_{99} + 0.1 * {(0.9)}^2 \theta_{98} + ... \]</span> 其中 θi 指第 i 天的实际数据。所有 θ 前面的系数（不包括 0.1）相加起来为 1 或者接近于 1，这些系数被称作<strong>偏差修正（Bias Correction）</strong>。</p><p>根据函数极限的一条定理： <span class="math display">\[ {\lim_{\beta\to 0}}(1 - \beta)^{\frac{1}{\beta}} = \frac{1}{e} \approx 0.368 \]</span> 当 β 为 0.9 时，可以当作把过去 10 天的气温指数加权平均作为当日的气温，因为 10 天后权重已经下降到了当天的 1/3 左右。同理，当 β 为 0.98 时，可以把过去 50 天的气温指数加权平均作为当日的气温。</p><p>因此，在计算当前时刻的平均值时，只需要前一天的平均值和当前时刻的值。 <span class="math display">\[ v_t = \beta v_{t-1} + (1 - \beta)\theta_t \]</span> 考虑到代码，只需要不断更新 v 即可： <span class="math display">\[ v := \beta v + (1 - \beta)\theta_t \]</span> 指数平均加权并<strong>不是最精准</strong>的计算平均数的方法，你可以直接计算过去 10 天或 50 天的平均值来得到更好的估计，但缺点是保存数据需要占用更多内存，执行更加复杂，计算成本更加高昂。</p><p>指数加权平均数公式的好处之一在于它只需要一行代码，且占用极少内存，因此<strong>效率极高，且节省成本</strong>。</p><h3 id="指数平均加权的偏差修正">指数平均加权的偏差修正</h3><p>我们通常有 <span class="math display">\[ v_0 = 0 \]</span></p><p><span class="math display">\[ v_1 = 0.98v_0 + 0.02\theta_1 \]</span></p><p>因此，<span class="math inline">\(v_1\)</span> 仅为第一个数据的 0.02（或者说 1- β），显然不准确。往后递推同理。</p><p>因此，我们修改公式为 <span class="math display">\[ v_t = \frac{\beta v_{t-1} + (1 - \beta)\theta_t}NaN \]</span> 随着 t 的增大，β 的 t 次方趋近于 0。因此当 t 很大的时候，偏差修正几乎没有作用，但是在前期学习可以帮助更好的预测数据。在实际过程中，一般会忽略前期偏差的影响。</p><h2 id="动量梯度下降法">动量梯度下降法</h2><p><strong>动量梯度下降（Gradient Descent with Momentum）</strong>是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为：</p><p>for l = 1, .. , L： <span class="math display">\[ v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \]</span></p><p><span class="math display">\[ v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \]</span></p><p><span class="math display">\[ W^{[l]} := W^{[l]} - \alpha v_{dW^{[l]}} \]</span></p><p><span class="math display">\[ b^{[l]} := b^{[l]} - \alpha v_{db^{[l]}} \]</span></p><p>其中，将动量衰减参数 β 设置为 0.9 是超参数的一个常见且效果不错的选择。当 β 被设置为 0 时，显然就成了 batch 梯度下降法。</p><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201121105508543.png" alt="image-20201121105508543"><figcaption>image-20201121105508543</figcaption></figure><p>进行一般的梯度下降将会得到图中的蓝色曲线，由于存在上下波动，减缓了梯度下降的速度，因此只能使用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。</p><p>而使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线。</p><p>当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。</p><p>另外，在 10 次迭代之后，移动平均已经不再是一个具有偏差的预测。因此实际在使用梯度下降法或者动量梯度下降法时，不会同时进行偏差修正。</p><h3 id="动量梯度下降法的形象解释">动量梯度下降法的形象解释</h3><p>将成本函数想象为一个碗状，从顶部开始运动的小球向下滚，其中 <span class="math inline">\(dw\)</span>，<span class="math inline">\(db\)</span> 想象成球的加速度；而 <span class="math inline">\(v_{dw}\)</span>、<span class="math inline">\(v_{db}\)</span> 相当于速度。</p><p>小球在向下滚动的过程中，因为加速度的存在速度会变快，但是由于 β 的存在，其值小于 1，可以认为是摩擦力，所以球不会无限加速下去。</p><h2 id="rmsprop-算法">RMSProp 算法</h2><p><strong>RMSProp（Root Mean Square Propagation，均方根传播）</strong>算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为（省略了 l）： <span class="math display">\[ s_{dw} = \beta s_{dw} + (1 - \beta)(dw)^2 \]</span></p><p><span class="math display">\[ s_{db} = \beta s_{db} + (1 - \beta)(db)^2 \]</span></p><p><span class="math display">\[ w := w - \alpha \frac{dw}{\sqrt{s_{dw} + \epsilon}} \]</span></p><p><span class="math display">\[ b := b - \alpha \frac{db}{\sqrt{s_{db} + \epsilon}} \]</span></p><p>其中，ϵ 是一个实际操作时加上的较小数（例如 <span class="math inline">\(10^{-8}\)</span>），为了防止分母太小而导致的数值不稳定。</p><p>当 dw 或 db 较大时，<span class="math inline">\((dw)^2\)</span>、<span class="math inline">\((db)^2\)</span> 会较大，进而 <span class="math inline">\(s_{dw}\)</span>、<span class="math inline">\(s_{db}\)</span> 也会较大，最终使得 <span class="math display">\[ \frac{dw}{\sqrt{s_{dw} + \epsilon}} \]</span> 和 <span class="math display">\[ \frac{db}{\sqrt{s_{db} + \epsilon}} \]</span> 较小，从而减小某些维度梯度更新波动较大的情况，使下降速度变得更快。</p><p>RMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。并且，它和 Adam 优化算法已被证明适用于不同的深度学习网络结构。</p><p>注意，β 也是一个超参数</p><h2 id="adam-优化算法">Adam 优化算法</h2><p><strong>Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）</strong>基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果。具体过程如下（省略了 l）：</p><p>首先进行初始化： <span class="math display">\[ v_{dW} = 0, s_{dW} = 0, v_{db} = 0, s_{db} = 0 \]</span> 用每一个 mini-batch 计算 dW、db，第 t 次迭代时： <span class="math display">\[ v_{dW} = \beta_1 v_{dW} + (1 - \beta_1) dW \]</span></p><p><span class="math display">\[ v_{db} = \beta_1 v_{db} + (1 - \beta_1) db \]</span></p><p><span class="math display">\[ s_{dW} = \beta_2 s_{dW} + (1 - \beta_2) {(dW)}^2 \]</span></p><p><span class="math display">\[ s_{db} = \beta_2 s_{db} + (1 - \beta_2) {(db)}^2 \]</span></p><p>一般使用 Adam 算法时需要计算偏差修正： <span class="math display">\[ v^{corrected}_{dW} = \frac{v_{dW}}{1-{\beta_1}^t} \]</span></p><p><span class="math display">\[ v^{corrected}_{db} = \frac{v_{db}}{1-{\beta_1}^t} \]</span></p><p><span class="math display">\[ s^{corrected}_{dW} = \frac{s_{dW}}{1-{\beta_2}^t} \]</span></p><p><span class="math display">\[ s^{corrected}_{db} = \frac{s_{db}}{1-{\beta_2}^t} \]</span></p><p>所以，更新 W、b 时有： <span class="math display">\[ W := W - \alpha \frac{v^{corrected}_{dW}}{ {\sqrt{s^{corrected}_{dW}} + \epsilon} } \]</span></p><p><span class="math display">\[ b := b - \alpha \frac{v^{corrected}_{db} }{ {\sqrt{s^{corrected}_{db} } + \epsilon} } \]</span></p><p>（可以看到 Andrew 在这里 ϵ 没有写到平方根里去，和他在 RMSProp 中写的不太一样。考虑到 ϵ 所起的作用，影响不大）</p><h3 id="超参数的选择">超参数的选择</h3><p>Adam 优化算法有很多的超参数，其中</p><ul><li>学习率 α：需要尝试一系列的值，来寻找比较合适的；</li><li>β1：常用的缺省值为 0.9；</li><li>β2：Adam 算法的作者建议为 0.999；</li><li>ϵ：不重要，不会影响算法表现，Adam 算法的作者建议为 <span class="math inline">\(10^{−8}\)</span>；</li></ul><p>β1、β2、ϵ 通常不需要调试。</p><h2 id="学习率衰减">学习率衰减</h2><p>如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201122100905548.png" alt="image-20201122100905548" style="zoom:25%"></p><p>而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。</p><p><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201122101038303.png" alt="image-20201122101038303" style="zoom:25%"></p><p>最常用的学习率衰减方法： <span class="math display">\[ \alpha = \frac{1}{1 + decay\_rate * epoch\_num} * \alpha_0 \]</span> 其中，<code>decay_rate</code> 为衰减率（超参数），<code>epoch_num</code> 为将所有的训练样本完整过一遍的次数。</p><ul><li>指数衰减：</li></ul><p><span class="math display">\[ \alpha = 0.95^{epoch\_num} * \alpha_0 \]</span></p><ul><li>其他：</li></ul><p><span class="math display">\[ \alpha = \frac{k}{\sqrt{epoch\_num}} * \alpha_0 \]</span></p><ul><li><p>离散下降</p><p>对于较小的模型，也有人会在训练时根据进度手动调小学习率。</p></li></ul><h2 id="局部最优问题">局部最优问题</h2><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201122103336971.png" alt="image-20201122103336971"><figcaption>image-20201122103336971</figcaption></figure><p><strong>鞍点（saddle）</strong>是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率是极小的，因此在低维度的局部最优点的情况并不适用于高维度。</p><p>结论：</p><ul><li>在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；</li><li>鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。</li></ul><h1 id="超参数调试batch-正则化和程序框架">超参数调试、Batch 正则化和程序框架</h1><h2 id="超参数调试处理">超参数调试处理</h2><h3 id="重要程度排序">重要程度排序</h3><p>目前已经讲到过的超参数中，重要程度依次是（仅供参考）：</p><ul><li><strong>最重要</strong>：<ul><li>学习率 α；</li></ul></li><li><strong>其次重要</strong>：<ul><li>β：动量衰减参数，常设置为 0.9；</li><li>#hidden units：各隐藏层神经元个数；</li><li>mini-batch 的大小；</li></ul></li><li><strong>再次重要</strong>：<ul><li>β1，β2，ϵ：Adam 优化算法的超参数，常设为 0.9、0.999、<span class="math inline">\(10^{−8}\)</span>；</li><li>#layers：神经网络层数;</li><li>decay_rate：学习衰减率；</li></ul></li></ul><h3 id="调参技巧">调参技巧</h3><p>系统地组织超参调试过程的技巧：</p><ul><li><strong>随机选择</strong>点（而非均匀选取），用这些点实验超参数的效果。这样做的原因是我们提前很难知道超参数的重要程度，可以通过选择更多值来进行更多实验；</li><li>由粗糙到精细：聚焦效果不错的点组成的小区域，在其中更密集地取值，以此类推；</li></ul><h3 id="选择合适的范围不懂">选择合适的范围(不懂==)</h3><ul><li>对于学习率 α，用<strong>对数标尺</strong>而非线性轴更加合理：0.0001、0.001、0.01、0.1 等，然后在这些刻度之间再随机均匀取值；</li><li>对于 β，取 0.9 就相当于在 10 个值中计算平均值，而取 0.999 就相当于在 1000 个值中计算平均值。可以考虑给 1-β 取值，这样就和取学习率类似了。</li></ul><p>上述操作的原因是当 β 接近 1 时，即使 β 只有微小的改变，所得结果的灵敏度会有较大的变化。例如，β 从 0.9 增加到 0.9005 对结果（1/(1-β)）几乎没有影响，而 β 从 0.999 到 0.9995 对结果的影响巨大（从 1000 个值中计算平均值变为 2000 个值中计算平均值）。</p><h3 id="一些建议">一些建议</h3><ul><li>深度学习如今已经应用到许多不同的领域。不同的应用出现相互交融的现象，某个应用领域的超参数设定有可能通用于另一领域。不同应用领域的人也应该更多地阅读其他研究领域的 paper，跨领域地寻找灵感；</li><li>考虑到数据的变化或者服务器的变更等因素，建议每隔几个月至少一次，重新测试或评估超参数，来获得实时的最佳模型；</li><li>根据你所拥有的计算资源来决定你训练模型的方式：<ul><li>Panda（熊猫方式）：在在线广告设置或者在计算机视觉应用领域有大量的数据，但受计算能力所限，同时试验大量模型比较困难。可以采用这种方式：试验一个或一小批模型，初始化，试着让其工作运转，观察它的表现，不断调整参数；</li><li>Caviar（鱼子酱方式）：拥有足够的计算机去平行试验很多模型，尝试很多不同的超参数，选取效果最好的模型；</li></ul></li></ul><h2 id="batch-normalization">Batch Normalization</h2><p><strong>批标准化（Batch Normalization，经常简称为 BN）</strong>会使参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更庞大，工作效果也很好，也会使训练更容易。</p><p>之前，我们对输入特征 X 使用了标准化处理。我们也可以用同样的思路处理<strong>隐藏层</strong>的激活值 <span class="math inline">\(a^{[l]}\)</span>，以加速 <span class="math inline">\(W^{[l+1]}\)</span>和 <span class="math inline">\(b^{[l+1]}\)</span>的训练。在<strong>实践</strong>中，经常选择标准化 <span class="math inline">\(Z^{[l]}\)</span>： <span class="math display">\[ \mu = \frac{1}{m} \sum_i z^{(i)} \]</span></p><p><span class="math display">\[ \sigma^2 = \frac{1}{m} \sum_i {(z_i - \mu)}^2 \]</span></p><p><span class="math display">\[ z_{norm}^{(i)} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}} \]</span></p><p>其中，m 是单个 mini-batch 所包含的样本个数，ϵ 是为了防止分母为零，通常取 <span class="math inline">\(10^{−8}\)</span>。</p><p>这样，我们使得所有的输入 <span class="math inline">\(z^{(i)}\)</span>值为 0，方差为 1。但我们不想让隐藏层单元总是含有平均值 0 和方差 1，也许隐藏层单元有了不同的分布会更有意义。因此，我们计算 <span class="math display">\[ \tilde z^{(i)} = \gamma z^{(i)}_{norm} + \beta \]</span> 其中，γ 和 β 都是模型的学习参数，所以可以用各种梯度下降算法来更新 γ 和 β 的值，如同更新神经网络的权重一样。</p><p>通过对 γ 和 β 的合理设置，可以让 z<sub>(i)z</sub>(i)的均值和方差为任意值。这样，我们对隐藏层的 z(i)z(i)进行标准化处理，用得到的 z<sub>(i)z</sub>(i)替代 z(i)z(i)。</p><p><strong>设置 γ 和 β 的原因</strong>是，如果各隐藏层的输入均值在靠近 0 的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。因此，需要用 γ 和 β 对标准化后的结果做进一步处理。</p><h3 id="将-bn-应用于神经网络">将 BN 应用于神经网络</h3><p>对于 L 层神经网络，经过 Batch Normalization 的作用，整体流程如下：</p><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201123134528833.png" alt="image-20201123134528833"><figcaption>image-20201123134528833</figcaption></figure><p>实际上，Batch Normalization 经常使用在 mini-batch 上，这也是其名称的由来。</p><p>使用 Batch Normalization 时，因为标准化处理中包含减去均值的一步，因此 b 实际上没有起到作用，其数值效果交由 β 来实现。因此，在 Batch Normalization 中，可以省略 b 或者暂时设置为 0。</p><p>在使用梯度下降算法时，分别对 <span class="math inline">\(W^{[l]}\)</span>，<span class="math inline">\(β^{[l]}\)</span>和 <span class="math inline">\(γ^{[l]}\)</span>进行迭代更新。除了传统的梯度下降算法之外，还可以使用之前学过的动量梯度下降、RMSProp 或者 Adam 等优化算法。</p><h3 id="bn-有效的原因">BN 有效的原因</h3><p>Batch Normalization 效果很好的原因有以下两点：</p><ol type="1"><li>通过对隐藏层各神经元的输入做类似的标准化处理，提高神经网络训练速度；</li><li>可以使前面层的权重变化对后面层造成的影响减小，整体网络更加健壮。</li></ol><p>关于第二点，如果实际应用样本和训练样本的数据分布不同（例如，橘猫图片和黑猫图片），我们称发生了“<strong>Covariate Shift</strong>”。这种情况下，一般要对模型进行重新训练。Batch Normalization 的作用就是减小 Covariate Shift 所带来的影响，让模型变得更加健壮，鲁棒性（Robustness）更强。</p><p>即使输入的值改变了，由于 Batch Normalization 的作用，使得均值和方差保持不变（由 γ 和 β 决定），限制了在前层的参数更新对数值分布的影响程度，因此后层的学习变得更容易一些。Batch Normalization 减少了各层 W 和 b 之间的耦合性，让各层更加独立，实现自我训练学习的效果。</p><p>另外，Batch Normalization 也<strong>起到微弱的正则化</strong>（regularization）效果。因为在每个 mini-batch 而非整个数据集上计算均值和方差，只由这一小部分数据估计得出的均值和方差会有一些噪声，因此最终计算出的 <span class="math inline">\(\tilde z^{(i)}\)</span> 也有一定噪声。类似于 dropout，这种噪声会使得神经元不会再特别依赖于任何一个输入特征。</p><p>因为 Batch Normalization 只有微弱的正则化效果，因此可以和 dropout 一起使用，以获得更强大的正则化效果。通过应用更大的 mini-batch 大小，可以减少噪声，从而减少这种正则化效果。</p><p>最后，不要将 Batch Normalization 作为正则化的手段，而是当作加速学习的方式。正则化只是一种非期望的副作用，Batch Normalization 解决的还是反向传播过程中的梯度问题（梯度消失和爆炸）。</p><h3 id="测试时的-batch-normalization">测试时的 Batch Normalization</h3><p>Batch Normalization 将数据以 mini-batch 的形式逐一处理，但在测试时，可能需要对每一个样本逐一处理，这样无法得到 μ 和 <span class="math inline">\(σ^2\)</span> 。</p><p>理论上，我们可以将所有训练集放入最终的神经网络模型中，然后将每个隐藏层计算得到的 <span class="math inline">\(μ^{[l]}\)</span>和 <span class="math inline">\(σ^{2[l]}\)</span>直接作为测试过程的 μ 和 σ 来使用。但是，实际应用中一般不使用这种方法，而是使用之前学习过的指数加权平均的方法来预测测试过程单个样本的 μ 和 <span class="math inline">\(σ^2\)</span> 。</p><p>对于第 l 层隐藏层，考虑所有 mini-batch 在该隐藏层下的 <span class="math inline">\(μ^{[l]}\)</span> 和 <span class="math inline">\(σ^{2[l]}\)</span>，然后用指数加权平均的方式来预测得到当前单个样本的 <span class="math inline">\(μ^{[l]}\)</span> 和 <span class="math inline">\(σ^{2[l]}\)</span>。这样就实现了对测试过程单个样本的均值和方差估计。</p><h2 id="softmax-回归">Softmax 回归</h2><p>目前为止，介绍的分类例子都是二分类问题：神经网络输出层只有一个神经元，表示预测输出 <span class="math inline">\(\hat y\)</span> 是正类的概率 P(y = 1|x)， <span class="math inline">\(\hat y\)</span> &gt; 0.5 则判断为正类，反之判断为负类。</p><p>对于<strong>多分类问题</strong>，用 C 表示种类个数，则神经网络输出层，也就是第 L 层的单元数量 <span class="math inline">\(n^{[L]}=C\)</span> 。每个神经元的输出依次对应属于该类的概率，即 <span class="math inline">\(P(y=c|x),c=0,1,..,C−1\)</span> 。有一种 Logistic 回归的一般形式，叫做 <strong>Softmax 回归</strong>，可以处理多分类问题。</p><p>对于 Softmax 回归模型的输出层，即第 L 层，有： <span class="math display">\[ Z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]} \]</span> for i in range(L)，有： <span class="math display">\[ a^{[L]}_i = \frac{e^{Z^{[L]}_i}}{\sum^C_{i=1}e^{Z^{[L]}_i}} \]</span> 为输出层每个神经元的输出，对应属于该类的概率，满足： <span class="math display">\[ \sum^C_{i=1}a^{[L]}_i = 1 \]</span> 一个直观的计算例子如下：</p><figure><img data-src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201123104538968.png" alt="image-20201123104538968"><figcaption>image-20201123104538968</figcaption></figure><h3 id="损失函数和成本函数">损失函数和成本函数</h3><p>定义<strong>损失函数</strong>为： <span class="math display">\[ L(\hat y, y) = -\sum^C_{j=1}y_jlog\hat y_j \]</span> 当 i 为样本真实类别，则有： <span class="math display">\[ y_j = 0, j \ne i \]</span> 因此，损失函数可以简化为： <span class="math display">\[ L(\hat y, y) = -y_ilog\hat y_i = log \hat y_i \]</span> 所有 m 个样本的<strong>成本函数</strong>为： <span class="math display">\[ J = \frac{1}{m}\sum^m_{i=1}L(\hat y, y) \]</span></p><h3 id="梯度下降法">梯度下降法</h3><p>多分类的 Softmax 回归模型与二分类的 Logistic 回归模型只有输出层上有一点区别。经过不太一样的推导过程，仍有 <span class="math display">\[ dZ^{[L]} = A^{[L]} - Y \]</span> 反向传播过程的其他步骤也和 Logistic 回归的一致。</p><h2 id="深度学习框架">深度学习框架</h2><h3 id="比较著名的框架">比较著名的框架</h3><ul><li>Caffe / Caffe 2</li><li>CNTK</li><li>DL4J</li><li>Keras</li><li>Lasagne</li><li>mxnet</li><li>PaddlePaddle</li><li>TensorFlow</li><li>Theano</li><li>Torch</li></ul><h3 id="选择框架的标准">选择框架的标准</h3><ul><li>便于编程：包括神经网络的开发和迭代、配置产品；</li><li>运行速度：特别是训练大型数据集时；</li><li>是否真正开放：不仅需要开源，而且需要良好的管理，能够持续开放所有功能。</li></ul></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>小张同学</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://stuxiaozhang.github.io/2020/11/16/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B02/" title="《深度学习》课程笔记2_改善深层神经网络">http://stuxiaozhang.github.io/2020/11/16/《深度学习》课程笔记2/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2020/11/14/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="prev" title="Python学习笔记"><i class="fa fa-chevron-left"></i> Python学习笔记</a></div><div class="post-nav-item"><a href="/2020/11/23/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B03/" rel="next" title="《深度学习》课程笔记3_搭建机器学习项目">《深度学习》课程笔记3_搭建机器学习项目 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96"><span class="nav-number">1.</span> <span class="nav-text">改善深层神经网络：超参数调试、正则化以及优化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2"><span class="nav-number">2.</span> <span class="nav-text">深度学习的实用层面</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86%E8%AE%AD%E7%BB%83%E9%AA%8C%E8%AF%81%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="nav-number">2.1.</span> <span class="nav-text">数据划分：训练&#x2F;验证&#x2F;测试集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%BA%E8%AE%AE"><span class="nav-number">2.1.1.</span> <span class="nav-text">建议</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A5%E5%85%851.-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81cross-validation"><span class="nav-number">2.1.2.</span> <span class="nav-text">补充：1. 交叉验证（cross validation）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1"><span class="nav-number">2.1.3.</span> <span class="nav-text">2. 无偏估计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E4%BC%B0%E8%AE%A1%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE"><span class="nav-number">2.2.</span> <span class="nav-text">模型估计：偏差&#x2F;方差</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%94%E5%AF%B9%E6%96%B9%E6%B3%95"><span class="nav-number">2.2.1.</span> <span class="nav-text">应对方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96regularization"><span class="nav-number">2.3.</span> <span class="nav-text">正则化（regularization）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#logistic-%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">2.3.1.</span> <span class="nav-text">Logistic 回归中的正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">2.3.2.</span> <span class="nav-text">神经网络中的正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8Fweight-decay"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">权重衰减（Weight decay）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%AF%E4%BB%A5%E5%87%8F%E5%B0%8F%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="nav-number">2.3.3.</span> <span class="nav-text">正则化可以减小过拟合的原因</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B4%E8%A7%82%E8%A7%A3%E9%87%8A"><span class="nav-number">2.3.3.1.</span> <span class="nav-text">直观解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%87%8A"><span class="nav-number">2.3.3.2.</span> <span class="nav-text">数学解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E8%A7%A3%E9%87%8A"><span class="nav-number">2.3.3.3.</span> <span class="nav-text">其他解释</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">2.4.</span> <span class="nav-text">dropout 正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E9%9A%8F%E6%9C%BA%E5%A4%B1%E6%B4%BBinverted-dropout"><span class="nav-number">2.4.1.</span> <span class="nav-text">反向随机失活（Inverted dropout）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%90%86%E8%A7%A3-dropout"><span class="nav-number">2.4.2.</span> <span class="nav-text">理解 dropout</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">2.5.</span> <span class="nav-text">其他正则化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%E8%BE%93%E5%85%A5"><span class="nav-number">2.6.</span> <span class="nav-text">标准化输入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%E5%85%AC%E5%BC%8F"><span class="nav-number">2.6.1.</span> <span class="nav-text">标准化公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%A0%87%E5%87%86%E5%8C%96%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="nav-number">2.6.2.</span> <span class="nav-text">使用标准化的原因</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">2.7.</span> <span class="nav-text">梯度消失和梯度爆炸</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%A9%E7%94%A8%E5%88%9D%E5%A7%8B%E5%8C%96%E7%BC%93%E8%A7%A3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">2.7.1.</span> <span class="nav-text">利用初始化缓解梯度消失和梯度爆炸</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8Cgradient-checking"><span class="nav-number">2.8.</span> <span class="nav-text">梯度检验（Gradient checking）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%95%B0%E5%80%BC%E9%80%BC%E8%BF%91"><span class="nav-number">2.8.1.</span> <span class="nav-text">梯度的数值逼近</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E7%9A%84%E5%AE%9E%E6%96%BD"><span class="nav-number">2.8.2.</span> <span class="nav-text">梯度检验的实施</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E5%8F%82%E6%95%B0"><span class="nav-number">2.8.2.1.</span> <span class="nav-text">连接参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9B%E8%A1%8C%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C"><span class="nav-number">2.8.2.2.</span> <span class="nav-text">进行梯度检验</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%96%BD%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E7%9A%84%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E5%92%8C%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">2.8.3.</span> <span class="nav-text">在神经网络实施梯度检验的使用技巧和注意事项</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">3.1.</span> <span class="nav-text">batch 梯度下降法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mini-batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">Mini-Batch 梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#batch-%E7%9A%84%E4%B8%8D%E5%90%8C%E5%A4%A7%E5%B0%8Fsize%E5%B8%A6%E6%9D%A5%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">3.2.1.</span> <span class="nav-text">batch 的不同大小（size）带来的影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mini-batch-%E5%A4%A7%E5%B0%8F%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">3.2.2.</span> <span class="nav-text">mini-batch 大小的选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%8E%B7%E5%BE%97-mini-batch-%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="nav-number">3.2.3.</span> <span class="nav-text">获得 mini-batch 的步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%A1%A8%E7%A4%BA"><span class="nav-number">3.2.4.</span> <span class="nav-text">符号表示</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E5%B9%B3%E5%9D%87%E5%8A%A0%E6%9D%83"><span class="nav-number">3.3.</span> <span class="nav-text">指数平均加权</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%90%86%E8%A7%A3%E6%8C%87%E6%95%B0%E5%B9%B3%E5%9D%87%E5%8A%A0%E6%9D%83"><span class="nav-number">3.3.1.</span> <span class="nav-text">理解指数平均加权</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E5%B9%B3%E5%9D%87%E5%8A%A0%E6%9D%83%E7%9A%84%E5%81%8F%E5%B7%AE%E4%BF%AE%E6%AD%A3"><span class="nav-number">3.3.2.</span> <span class="nav-text">指数平均加权的偏差修正</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">3.4.</span> <span class="nav-text">动量梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E5%BD%A2%E8%B1%A1%E8%A7%A3%E9%87%8A"><span class="nav-number">3.4.1.</span> <span class="nav-text">动量梯度下降法的形象解释</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rmsprop-%E7%AE%97%E6%B3%95"><span class="nav-number">3.5.</span> <span class="nav-text">RMSProp 算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adam-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">3.6.</span> <span class="nav-text">Adam 优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">3.6.1.</span> <span class="nav-text">超参数的选择</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="nav-number">3.7.</span> <span class="nav-text">学习率衰减</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%97%AE%E9%A2%98"><span class="nav-number">3.8.</span> <span class="nav-text">局部最优问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95batch-%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6"><span class="nav-number">4.</span> <span class="nav-text">超参数调试、Batch 正则化和程序框架</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E5%A4%84%E7%90%86"><span class="nav-number">4.1.</span> <span class="nav-text">超参数调试处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%E7%A8%8B%E5%BA%A6%E6%8E%92%E5%BA%8F"><span class="nav-number">4.1.1.</span> <span class="nav-text">重要程度排序</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7"><span class="nav-number">4.1.2.</span> <span class="nav-text">调参技巧</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E8%8C%83%E5%9B%B4%E4%B8%8D%E6%87%82"><span class="nav-number">4.1.3.</span> <span class="nav-text">选择合适的范围(不懂&#x3D;&#x3D;)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E5%BB%BA%E8%AE%AE"><span class="nav-number">4.1.4.</span> <span class="nav-text">一些建议</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#batch-normalization"><span class="nav-number">4.2.</span> <span class="nav-text">Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%86-bn-%E5%BA%94%E7%94%A8%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.2.1.</span> <span class="nav-text">将 BN 应用于神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bn-%E6%9C%89%E6%95%88%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="nav-number">4.2.2.</span> <span class="nav-text">BN 有效的原因</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E6%97%B6%E7%9A%84-batch-normalization"><span class="nav-number">4.2.3.</span> <span class="nav-text">测试时的 Batch Normalization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax-%E5%9B%9E%E5%BD%92"><span class="nav-number">4.3.</span> <span class="nav-text">Softmax 回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.1.</span> <span class="nav-text">损失函数和成本函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">4.3.2.</span> <span class="nav-text">梯度下降法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="nav-number">4.4.</span> <span class="nav-text">深度学习框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AF%94%E8%BE%83%E8%91%97%E5%90%8D%E7%9A%84%E6%A1%86%E6%9E%B6"><span class="nav-number">4.4.1.</span> <span class="nav-text">比较著名的框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E6%A1%86%E6%9E%B6%E7%9A%84%E6%A0%87%E5%87%86"><span class="nav-number">4.4.2.</span> <span class="nav-text">选择框架的标准</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="小张同学" src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/codedog.jpg"><p class="site-author-name" itemprop="name">小张同学</p><div class="site-description" itemprop="description">不要停止奔跑 不要回顾来路</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">25</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=709634432" title="QQ → tencent:&#x2F;&#x2F;AddContact&#x2F;?fromId&#x3D;50&amp;fromSubId&#x3D;1&amp;subcmd&#x3D;all&amp;uin&#x3D;709634432" rel="noopener" target="_blank"><i class="fa fa-fw fa-fab fa-qq"></i>QQ</a> </span><span class="links-of-author-item"><a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2020 – <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">小张同学</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">264k</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="//cdn.jsdelivr.net/npm/animejs@3.1.0/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>"undefined"==typeof MathJax?(window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},tags:"ams"},options:{renderActions:{findScript:[10,n=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const a=new n.options.MathItem(e.textContent,n.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),a.start={node:t,delim:"",n:0},a.end={node:t,delim:"",n:0},n.math.push(a)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!1,notify:!1,appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",avatar:"mm",meta:e,pageSize:"10",visitor:!1,lang:"zh-cn",path:location.pathname,recordIP:!0,serverURLs:""})},window.Valine)})</script></body></html>