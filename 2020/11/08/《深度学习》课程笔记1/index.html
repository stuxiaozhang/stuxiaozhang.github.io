<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><link rel="icon" href="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="
此篇文章是吴恩达《深度学习》课程的笔记，主要源自于黄思腾大佬的博客

神经网络和深度学习
神经网络基础

实现一个神经网络时，如果需要遍历整个训练集，并不需要直接使用 for 循环。
神经网络的计算过程中，通常有一个正向过程（forward pass）或者叫正向传播步骤（forward propagation step），接着会有一个反向过程（backward pass）或者叫反向"><meta name="author" content="小张同学"><meta name="keywords" content=""><title>《深度学习》课程笔记1_神经网络和深度学习 - 小张同学的博客</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"stuxiaozhang.github.io",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:4},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",app_key:"CgnvRL262D07ied40NiXm2VL",server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.3.0"></head><body><header style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>xiaozhang's space</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/schedule/"><i class="iconfont icon-cliplist"></i> 动态</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://gitee.com/stuxiaozhang/blogimage/raw/master/img/mytheme/post.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="《深度学习》课程笔记1_神经网络和深度学习"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 小张同学 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-11-08 12:57" pubdate>2020年11月8日</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 5.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 87 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">《深度学习》课程笔记1_神经网络和深度学习</h1><p class="note note-info">本文最后更新于：2021年1月2日</p><div class="markdown-body"><blockquote><p>此篇文章是吴恩达《深度学习》课程的笔记，主要源自于黄思腾大佬的博客</p></blockquote><h1 id="神经网络和深度学习">神经网络和深度学习</h1><h2 id="神经网络基础">神经网络基础</h2><ul><li>实现一个神经网络时，如果需要遍历整个训练集，并不需要直接使用 for 循环。</li><li>神经网络的计算过程中，通常有一个正向过程（forward pass）或者叫<strong>正向传播步骤（forward propagation step）</strong>，接着会有一个反向过程（backward pass）或者叫<strong>反向传播步骤（backward propagation step）</strong>。</li></ul><h3 id="logistic-回归">Logistic 回归</h3><p><strong>逻辑回归( Logistic Regression)：</strong>是一种用于解决<strong>监督学习（Supervised Learning）</strong>问题的学习算法。</p><p>进行逻辑回归的目的是使训练数据的标签值与预测出来的值之间的误差最小化。</p><p>Logistic 回归是一个用于<strong>二分类</strong>的算法，给定一些输入，输出结果是离散值。</p><p>Logistic 回归中使用的参数如下：</p><ul><li>输入的特征向量：<span class="math inline">\(x∈R^nx\)</span>，其中 <span class="math inline">\(n^x\)</span> 是特征数量；</li><li>用于训练的标签：<span class="math inline">\(y∈0,1\)</span></li><li>权重：<span class="math inline">\(w∈R^nx\)</span></li><li>偏置：$ b∈R$</li><li>输出：<span class="math inline">\(\hat{y} = \sigma(w^Tx+b)\)</span></li><li>Sigmoid 函数：</li></ul><p><span class="math display">\[ s = \sigma(w^Tx+b) = \sigma(z) = \frac{1}{1+e^{-z}} \]</span></p><p>为将 <span class="math inline">\(wTx+b\)</span> 约束在 [0, 1] 间，引入 Sigmoid 函数。从下图可看出，Sigmoid 函数的值域为 [0, 1]。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201109154334967.png" srcset="/img/loading.gif" lazyload alt="image-20201109154334967" style="zoom:67%"></p><h3 id="损失函数">损失函数</h3><p><strong>损失函数（loss function）</strong>用于衡量<u>单个样本</u>预测结果 <span class="math inline">\(\hat{y}^{(i)}\)</span> 与真实值 <span class="math inline">\(y^{(i)}\)</span> 之间的误差。</p><p>最简单的损失函数定义方式为<u>平方差损失函数</u>： <span class="math display">\[ L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^2 \]</span> 但 Logistic 回归中我们并不倾向于使用这样的损失函数，因为之后讨论的优化问题会变成非凸的，最后会得到很多个局部最优解，梯度下降法可能找不到全局最优值。</p><p>一般使用<u>交叉熵损失函数</u>： <span class="math display">\[ L(\hat{y},y) = -(y\log\hat{y})-(1-y)\log(1-\hat{y}) \]</span> 损失函数是在单个训练样本中定义的，它衡量了在<strong>单个</strong>训练样本上的表现。</p><p>而<strong>代价函数（cost function，或者称作成本函数）</strong>衡量的是在<strong>全体</strong>训练样本上的表现，即衡量参数 w 和 b 的效果。 <span class="math display">\[ J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)}) \]</span></p><div class="note note-warning"><p>损失函数：针对单个训练样本。</p><p>代价函数：针对全体训练样本，是参数的总代价。</p></div><h3 id="梯度下降法gradient-descent">梯度下降法（Gradient Descent）</h3><p>函数的<strong>梯度（gradient）</strong>指出了函数的最陡增长方向。即是说，按梯度的方向走，函数增长得就越快。那么按梯度的负方向走，函数值自然就降低得最快了。</p><p>模型的训练目标即是寻找合适的 w 与 b 以最小化代价函数值。简单起见我们先假设 w 与 b 都是一维实数，那么可以得到如下的 J 关于 w 与 b 的图：</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201113092437430.png" srcset="/img/loading.gif" lazyload alt="image-20201113092437430" style="zoom:67%"></p><p>可以看到，成本函数 J 是一个<strong>凸函数</strong>，与非凸函数的区别在于其不含有多个局部最低点；选择这样的代价函数就保证了无论我们初始化模型参数如何，都能够寻找到合适的最优解。</p><p>参数 w 的更新公式为： <span class="math display">\[ w := w - \alpha\frac{dJ(w, b)}{dw} \]</span> 其中 α 表示学习速率，即每次更新的 w 的步伐长度。<span class="math inline">\(\frac{dJ(w, b)}{dw}\)</span> 是函数 <span class="math inline">\(dJ(w,b)\)</span> 对 w 求偏导。</p><p>当 w 大于最优解 w′ 时，导数大于 0，那么 w 就会向更小的方向更新。反之当 w 小于最优解 w′ 时，导数小于 0，那么 w 就会向更大的方向更新。迭代直到收敛。</p><p>在成本函数 J(w, b) 中还存在参数 b，因此也有：</p><p><span class="math display">\[ b := b - \alpha\frac{dJ(w, b)}{db} \]</span></p><h3 id="计算图computation-graph">计算图（Computation Graph）</h3><p>神经网络中的计算即是由多个计算网络输出的前向传播与计算梯度的后向传播构成。所谓的<strong>反向传播（Back Propagation）</strong>即是当我们需要计算最终值相对于某个特征变量的导数时，我们需要利用计算图中上一步的结点定义。计算图解释了为什么我们用这种方式组织这些计算过程。</p><h3 id="logistic-回归中的梯度下降法">Logistic 回归中的梯度下降法</h3><p>假设输入的特征向量维度为 2，即输入参数共有 x1, w1, x2, w2, b 这五个。可以推导出如下的计算图：</p><figure><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201113080326996.png" srcset="/img/loading.gif" lazyload alt="image-20201113080326996"><figcaption>image-20201113080326996</figcaption></figure><p>考虑单个样例，已知： <span class="math display">\[ L(a,y)=-(y\log a + (1-y)\log (1-a)) \]</span></p><p><span class="math display">\[ a=\sigma(z) = \frac{1}{1+e^{-z}} \]</span></p><p><span class="math display">\[ z=w_1x_1+w_2x_2 + b \]</span></p><blockquote><p>注：对sigmoid求导： <span class="math display">\[ dz=\frac{a}{1-a} \]</span></p></blockquote><hr><p>首先反向求出 L 对于 a 的导数： <span class="math display">\[ da=\frac{dL(a,y)}{da}=-\frac{y}{a}+\frac{1-y}{1-a} \]</span> 然后继续反向求出 L 对于 z 的导数： <span class="math display">\[ dz=\frac{dL}{dz}=\frac{dL(a,y)}{dz}=\frac{dL}{da}\frac{da}{dz}=a−y \]</span> 依此类推求出最终的损失函数相较于原始参数的导数之后，根据如下公式进行参数更新： <span class="math display">\[ w_1:=w_1-\alpha dw_1 \]</span></p><p><span class="math display">\[ w_2:=w_2-\alpha dw_2 \]</span></p><p><span class="math display">\[ b:=b-\alpha db \]</span></p><hr><p>接下来我们需要将对于单个用例的损失函数扩展到整个训练集的代价函数： <span class="math display">\[ J(w,b)=\frac{1}{m}\sum^m_{i=1}L(a^{(i)},y^{(i)}) \]</span></p><p><span class="math display">\[ a^{(i)}=\hat{y}^{(i)}=\sigma(z^{(i)})=\sigma(w^Tx^{(i)}+b) \]</span></p><p>我们可以对于某个权重参数 w1，其导数计算为： <span class="math display">\[ dw_1=\frac{\partial J(w,b)}{\partial{w_1}}=\frac{1}{m}\sum^m_{i=1}\frac{\partial L(a^{(i)},y^{(i)})}{\partial{w_1}} \]</span> 完整的 Logistic 回归中某次训练的伪代码流程如下，这里仅假设特征向量的维度为 2：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">J=<span class="hljs-number">0</span>; dw1=<span class="hljs-number">0</span>; dw2=<span class="hljs-number">0</span>; db=<span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">for</span> i = <span class="hljs-number">1</span> to m<br>    z(i) = wx(i)+b;<br>    a(i) = sigmoid(z(i));<br>    J += -[y(i)log(a(i))+(<span class="hljs-number">1</span>-y(i)）log(<span class="hljs-number">1</span>-a(i));<br>    dz(i) = a(i)-y(i);<br>    dw1 += x1(i)dz(i);<br>    dw2 += x2(i)dz(i);<br>    db += dz(i);<br><br>J/= m;<br>dw1/= m;<br>dw2/= m;<br>db/= m;<br>w=w-alpha*dw<br>b=b-alpha*db<br></code></pre></td></tr></table></figure><p>上述过程在计算时有一个缺点：你需要编写两个 for 循环。第一个 for 循环遍历 m 个样本，而第二个 for 循环遍历所有特征。如果有大量特征，在代码中显式使用 for 循环会使算法很低效。<strong>向量化</strong>可以用于解决显式使用 for 循环的问题。</p><h3 id="向量化">向量化</h3><p>在 Logistic 回归中，需要计算 <span class="math display">\[ z=w^Tx+b \]</span> 如果是非向量化的循环方式操作，代码可能如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">z = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_x):<br>	z[i] += w[i] * x[i];<br>z += b<br></code></pre></td></tr></table></figure><p>而如果是向量化的操作，代码则会简洁很多，并带来近百倍的性能提升（并行指令）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">z = np.dot(w, x) + b<br></code></pre></td></tr></table></figure><p>不用显式 for 循环，实现 Logistic 回归的梯度下降一次迭代（对应之前伪代码的 for 循环部分。这里公式和 NumPy 的代码混杂，注意分辨）： <span class="math display">\[ Z=w^TX+b=np.dot(w.T, x) + b \]</span></p><p><span class="math display">\[ A=\sigma(Z) \]</span></p><p><span class="math display">\[ dZ=A-Y \]</span></p><p><span class="math display">\[ dw=\frac{1}{m}XdZ^T \]</span></p><p><span class="math display">\[ db=\frac{1}{m}np.sum(dZ) \]</span></p><p><span class="math display">\[ w:=w-\sigma dw \]</span></p><p><span class="math display">\[ b:=b-\sigma db \]</span></p><p>正向和反向传播尽管如此，多次迭代的梯度下降依然需要 for 循环。</p><h3 id="广播broadcasting">广播（broadcasting）</h3><p>Numpy 的 Universal functions 中要求输入的数组 shape 是一致的。当数组的 shape 不相等的时候，则会使用广播机制，调整数组使得 shape 一样，满足规则，则可以运算，否则就出错。</p><p>四条规则：</p><ol type="1"><li>让所有输入数组都向其中 shape 最长的数组看齐，shape 中不足的部分都通过在前面加 1 补齐；</li><li>输出数组的 shape 是输入数组 shape 的各个轴上的最大值；</li><li>如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为 1 时，这个数组能够用来计算，否则出错；</li><li>当输入数组的某个轴的长度为 1 时，沿着此轴运算时都用此轴上的第一组值。</li></ol><h3 id="numpy-使用技巧">NumPy 使用技巧</h3><p>转置对秩为 1 的数组无效。因此，应该避免使用秩为 1 的数组，用 n * 1 的矩阵代替。例如，用<code>np.random.randn(5,1)</code> 代替 <code>np.random.randn(5)</code> 。</p><p>如果得到了一个秩为 1 的数组，可以使用 <code>reshape</code> 进行转换。</p><h2 id="浅层神经网络">浅层神经网络</h2><h3 id="神经网络表示">神经网络表示</h3><p>竖向堆叠起来的输入特征被称作神经网络的<strong>输入层（the input layer）</strong>。</p><p>神经网络的<strong>隐藏层（a hidden layer）</strong>。“隐藏”的含义是<strong>在训练集中</strong>，这些中间节点的真正数值是无法看到的。</p><p><strong>输出层（the output layer）</strong>负责输出预测值。</p><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201114113032833.png" srcset="/img/loading.gif" lazyload alt="image-20201114113032833" style="zoom:67%"></p><p>如图是一个<strong>双层神经网络</strong>，也称作<strong>单隐层神经网络（a single hidden layer neural network）</strong>。当我们计算网络的层数时，通常不考虑输入层，因此图中隐藏层是第一层，输出层是第二层，而输入层为第零层。</p><p>约定俗成的符号表示是：</p><ul><li>上标“[ ]”括号中的数字表示<u>神经网络中的第几层</u>，a代表着<strong>激活（Activation）</strong>，指的是不同层次的神经网络传递给后续层次的值。</li><li>输入层的激活值为 <span class="math inline">\(a[0]\)</span> ；</li><li>同样，隐藏层也会产生一些激活值，记作 <span class="math inline">\(a^{[1]}\)</span> 隐藏层的第一个单元（或者说节点）就记作 <span class="math inline">\(a_1^{[1]}\)</span> ,输出层同理。</li><li>另外，隐藏层和输出层都是带有参数 W 和 b 的。它们都使用上标 [1] 来表示是和第一个隐藏层有关，或者上标 [2] 来表示是和输出层有关。</li></ul><h3 id="计算神经网络的输出">计算神经网络的输出</h3><p><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201115084405346.png" srcset="/img/loading.gif" lazyload alt="image-20201115084405346" style="zoom:50%"></p><p>实际上，神经网络只不过将 Logistic 回归的计算步骤重复很多次。对于隐藏层的第一个节点， <span class="math display">\[ z _1^{[1]} = (W _1^{[1]})^TX+b _1^{[1]} \]</span></p><p><span class="math display">\[ a _1^{[1]} = \sigma(z _1^{[1]}) \]</span></p><p>我们可以类推得到，对于第一个隐藏层有下列公式： <span class="math display">\[ z^{[1]} = (W^{[1]})^Ta^{[0]}+b^{[1]} \]</span></p><p><span class="math display">\[ a^{[1]} = \sigma(z^{[1]}) \]</span></p><p>其中，<span class="math inline">\(a[0]\)</span> 可以是一个列向量，也可以将多个列向量堆叠起来得到矩阵。如果是后者的话，得到的 <span class="math inline">\(z[1]\)</span> 和 <span class="math inline">\(a[1]\)</span> 也是一个矩阵。</p><p>同理，对于输出层有： <span class="math display">\[ z^{[2]} = (W^{[2]})^Ta^{[1]}+b^{[2]} \]</span></p><p><span class="math display">\[ \hat{y} = a^{[2]} = \sigma(z^{[2]}) \]</span></p><p>值得注意的是<strong>层与层之间参数矩阵的规格大小</strong>。</p><ul><li>输入层和隐藏层之间：<span class="math inline">\((W^{[1]})^T\)</span> 的 shape 为 <code>(4,3)</code> ，前面的 4 是隐藏层神经元的个数，后面的 3 是输入层神经元的个数；<span class="math inline">\(b^{[1]}\)</span> 的 shape 为 <code>(4,1)</code> ，和隐藏层的神经元个数相同。</li><li>隐藏层和输出层之间：<span class="math inline">\((W^{[2]})^T\)</span> 的 shape 为 <code>(1,4)</code> ，前面的 1 是输出层神经元的个数，后面的 4 是隐藏层神经元的个数；<span class="math inline">\(b^{[2]}\)</span> 的 shape 为 <code>(1,1)</code> ，和输出层的神经元个数相同。</li></ul><h3 id="激活函数">激活函数</h3><p>有一个问题是神经网络的隐藏层和输出单元用什么激活函数。之前我们都是选用 sigmoid 函数，但有时其他函数的效果会好得多。</p><p>可供选用的激活函数有：</p><figure><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201115102234900.png" srcset="/img/loading.gif" lazyload alt="image-20201115102234900"><figcaption>image-20201115102234900</figcaption></figure><h4 id="tanh-函数the-hyperbolic-tangent-function双曲正切函数">tanh 函数（the hyperbolic tangent function，双曲正切函数）</h4><p><span class="math display">\[ a = \frac{e^z - e^{-z}}{e^z + e^{-z}} \]</span></p><p>效果几乎总比 sigmoid 函数好（除开<strong>二元分类的输出层</strong>，因为我们希望输出的结果介于 0 到 1 之间），因为函数输出介于 -1 和 1 之间，激活函数的平均值就更接近 0，有类似数据中心化的效果。</p><p>然而，tanh 函数存在和 sigmoid 函数一样的缺点：当 z 趋紧无穷大（或无穷小），导数的梯度（即函数的斜率）就趋紧于 0，这使得梯度算法的速度大大减缓。</p><h4 id="relu-函数the-rectified-linear-unit修正线性单元">ReLU 函数（the rectified linear unit，修正线性单元）</h4><p><span class="math display">\[ a=max(0,z) \]</span></p><p>当 z &gt; 0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度，收敛速度远大于 sigmoid 和 tanh。然而当 z &lt; 0 时，梯度一直为 0，但是实际的运用中，该缺陷的影响不是很大。</p><h4 id="leaky-relu带泄漏的-relu">Leaky ReLU（带泄漏的 ReLU）：</h4><p><span class="math display">\[ a=max(0.01z,z) \]</span></p><p>当 z &gt; 0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度，收敛速度远大于 sigmoid 和 tanh。然而当 z &lt; 0 时，梯度一直为 0，但是实际的运用中，该缺陷的影响不是很大。</p><blockquote><p>经验：在选择激活函数的时候，如果是二分类问题，输出层选 sigmoid 函数，其他层都选 ReLU 函数。如果在不知道该选什么的时候就选择 ReLU，当然也没有固定答案，要依据实际问题在交叉验证集合中进行验证分析。当然，我们可以在不同层选用不同的激活函数。</p></blockquote><h4 id="使用-relu-激活函数的优点-为什么relu要好过sigmoid和tanh">使用 ReLU 激活函数的优点？/ 为什么ReLU要好过sigmoid和tanh？</h4><ol type="1"><li>采用 sigmoid 等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法和指数运算，计算量相对大，而采用 ReLU 激活函数，整个过程的计算量节省很多。</li><li>对于深层网络，sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在 sigmoid 接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），这种现象称为饱和，从而无法完成深层网络的训练，为什么会无法完成训练呢，因为神经网络更新w的时候就是依靠导数来更新的，如果导数接近0，那 w 更新之后还是原来的 w了。而 ReLU 就不会有饱和倾向，不会有特别小的梯度出现。</li></ol><blockquote><p>需注意， ReLU 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性。主要问题有两个：</p><ol type="1"><li>0点附近不可微。所以通常在0点只求右导数</li><li>它会“谋杀”一些神经元。就是说在 BP 的过程中很快会让一些神经元的导数永远是0，于是这些神经元等于被抛弃了，也就是被谋杀了。这提高了速度和精度，但是也有些武断，于是为了解决这个问题，引入了Leaky ReLU。Leaky ReLu 不会产生这个问题。</li></ol></blockquote><h3 id="使用非线性激活函数的原因">使用非线性激活函数的原因</h3><ol type="1"><li><p>使用线性激活函数和不使用激活函数、直接使用 Logistic 回归没有区别，那么无论神经网络有多少层，输出都是输入的线性组合，与<strong>没有隐藏层</strong>效果相当，就成了最原始的感知器了。</p></li><li><p>使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。</p></li></ol><h3 id="激活函数的导数">激活函数的导数</h3><ul><li>sigmoid 函数：</li></ul><p><span class="math display">\[ g(z) = \frac{1}{1+e^{-z}} \]</span></p><p><span class="math display">\[ g\prime(z)=\frac{dg(z)}{dz} = \frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})=g(z)(1-g(z)) \]</span></p><ul><li>tanh 函数：</li></ul><p><span class="math display">\[ g(z) = tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \]</span></p><p><span class="math display">\[ g\prime(z)=\frac{dg(z)}{dz} = 1-(tanh(z))^2=1-(g(z))^2 \]</span></p><ul><li>ReLU 函数：</li></ul><p><span class="math display">\[ g(z)=max(0,z) \]</span></p><p><span class="math display">\[ \frac{d}{d z} g(z)=\left\{\begin{array}{ll} 0 &amp; \text { if } z&lt;0 \\ 1 &amp; \text { if } z&gt;0 \\ \text { undefined } &amp; \text { if } z=0 \end{array}\right. \]</span></p><h2 id="神经网络的梯度下降法">神经网络的梯度下降法</h2><h3 id="前向传播">前向传播</h3><figure><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201115110144415.png" srcset="/img/loading.gif" lazyload alt="image-20201115110144415"><figcaption>image-20201115110144415</figcaption></figure><p><span class="math display">\[ z^{[1]}={(w^{[1]})}^Tx+b^{[1]} \]</span></p><p><span class="math display">\[ a^{[1]}=g^{[1]}(z^{[1]}) \]</span></p><p><span class="math display">\[ z^{[2]}={(w^{[2]})}^Ta^{[1]}+b^{[2]},(x=a^{[1]}) \]</span></p><p><span class="math display">\[ a^{[2]}=g^{[2]}(z^{[2]})=\sigma(z^{[2]}) \]</span></p><p><span class="math display">\[ \mathcal{L}\left(a^{[2]}, y\right)=-\left(y \log a^{[2]}+(1-y) \log \left(1-a^{[2]}\right)\right) \]</span></p><p>在训练过程中，经过前向传播后得到的最终结果跟训练样本的真实值总是存在一定误差，这个误差便是损失函数。想要减小这个误差，当前应用最广的一个算法便是梯度下降，于是用损失函数，从后往前，依次求各个参数的偏导，这就是所谓的<strong>反向传播（Back Propagation）</strong>，一般简称这种算法为BP算法。</p><h3 id="反向传播">反向传播</h3><figure><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201115112715694.png" srcset="/img/loading.gif" lazyload alt="image-20201115112715694"><figcaption>image-20201115112715694</figcaption></figure><p>已知 sigmoid 函数的导数为： <span class="math display">\[ a^{[2]^{\prime}}=\operatorname{sigmoid}\left(z^{[2]}\right)^{\prime}=\frac{\partial a^{[2]}}{\partial z^{[2]}}=a^{[2]}\left(1-a^{[2]}\right) \]</span></p><p>由复合函数求导中的链式法则，反向传播过程中： <span class="math display">\[ d a^{[2]}=\frac{\partial \mathcal{L}\left(a^{[2]}, y\right)}{\partial a^{[2]}} =-\frac{y}{a^{[2]}} + \frac{1-y}{1-a^{[2]}},(对交叉熵损失函数求导) \]</span></p><p><span class="math display">\[ d z^{[2]}=\frac{\partial \mathcal{L}\left(a^{[2]}, y\right)}{\partial a^{[2]}} \cdot \frac{\partial \alpha^{[2]}}{\partial z^{[2]}}=a^{[2]}-y \]</span></p><p><span class="math display">\[ d w^{[2]}=\frac{\partial \mathcal{L}\left(a^{[2]}, y\right)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial w^{[2]}}=d z^{[2]}=a^{[2]}-y \]</span></p><p><span class="math display">\[ d b^{[2]}=\frac{\partial \mathcal{L}\left(a^{[2]}, y\right)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial b^{[2]}}=d z^{[2]}=a^{[2]}-y \]</span></p><p><span class="math display">\[ d a^{[1]}=\frac{\partial \mathcal{L}\left(a^{[2]}, y\right)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}=d z^{[2]} \cdot w^{[2]} \]</span></p><p><span class="math display">\[ d z^{[1]}=\frac{\partial \mathcal{L}\left(a^{[2]}, y\right)}{\partial a^{(2]}} \cdot \frac{\partial \alpha^{[2]}}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=d z^{[2]} \cdot w^{[2]} \times g^{[1]^{\prime}}\left(z^{[1]}\right) \]</span></p><p><span class="math display">\[ d w^{[1]}=\frac{\partial \mathcal{L}\left(\alpha^{[2]}, y\right)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{(2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} \cdot \frac{\partial z^{[1]}}{\partial w^{[1]}}=d z^{[1]} \cdot{X^{T}} \]</span></p><p><span class="math display">\[ d b^{[1]}=\frac{\partial \mathcal{L}\left(a^{[2]}, y\right)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} \cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=d z^{[1]} \]</span></p><p>这便是反向传播的整个推导过程，在具体的算法实现过程中，使用梯度下降的方法，将各个参数进行向量化、取平均值，不断进行更新。</p><h3 id="随机初始化">随机初始化</h3><p>如果在初始时将两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。</p><p>在初始化的时候，<strong>W 参数要进行随机初始化，不可以设置为 0</strong>。而 b 因为不存在对称性的问题，可以设置为 0。</p><p>以 2 个输入，2 个隐藏神经元为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">W = np.random.rand(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)* <span class="hljs-number">0.01</span><br>b = np.zeros((<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p>这里将 W 的值乘以 0.01（或者其他的常数值）的原因是为了使得权重 W 初始化为较小的值，<u>这是因为使用 sigmoid 函数或者 tanh 函数作为激活函数时，W 比较小，则 Z=WX+b 所得的值趋近于 0，梯度较大，能够提高算法的更新速度。</u>而如果 W 设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。</p><p>ReLU 和 Leaky ReLU 作为激活函数时不存在这种问题，因为在大于 0 的时候，梯度均为 1。</p><h2 id="深层神经网络">深层神经网络</h2><h3 id="深层网络中的前向和反向传播">深层网络中的前向和反向传播</h3><h4 id="前向传播-1">前向传播</h4><p>推导整个前向传播的过程可得：</p><p><strong>输入：</strong> <span class="math inline">\(a^{[l-1]}\)</span></p><p><strong>输出：</strong> <span class="math inline">\(a^{[l]}\)</span>， <span class="math inline">\(cache(z^{[l]})\)</span></p><p><strong>公式：</strong> <span class="math display">\[ Z^{[l]}=W^{[l]}\cdot a^{[l-1]}+b^{[l]} \]</span></p><p><span class="math display">\[ a^{[l]}=g^{[l]}(Z^{[l]}) \]</span></p><h4 id="反向传播-1">反向传播</h4><p><strong>输入</strong>：<span class="math inline">\(da^{[l]}\)</span></p><p><strong>输出</strong>：<span class="math inline">\(da^{[l−1]}\)</span>，<span class="math inline">\(dW[l]\)</span>，<span class="math inline">\(db^{[l]}\)</span></p><p><strong>公式：</strong> <span class="math display">\[ dZ^{[l]}=da^{[l]}*g^{[l]}{&#39;}(Z^{[l]}) \]</span></p><p><span class="math display">\[ dW^{[l]}=dZ^{[l]}\cdot a^{[l-1]} \]</span></p><p><span class="math display">\[ db^{[l]}=dZ^{[l]} \]</span></p><p><span class="math display">\[ da^{[l-1]}=W^{[l]T}\cdot dZ^{[l]} \]</span></p><ul><li>总结：</li></ul><figure><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/A3AB54BACCBBD3A9346DCD5378E9A876.png" srcset="/img/loading.gif" lazyload alt="A3AB54BACCBBD3A9346DCD5378E9A876"><figcaption>A3AB54BACCBBD3A9346DCD5378E9A876</figcaption></figure><h3 id="搭建深层神经网络块">搭建深层神经网络块</h3><figure><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201115205800365.png" srcset="/img/loading.gif" lazyload alt="image-20201115205800365"><figcaption>image-20201115205800365</figcaption></figure><p>神经网络的一步训练（一个梯度下降循环），包含了从 <span class="math inline">\(a[0]\)</span>（即 x）经过一系列正向传播计算得到 <span class="math inline">\(\hat y\)</span>（即 <span class="math inline">\(a[l]\)</span>）。然后再计算 <span class="math inline">\(da[l]\)</span>，开始实现反向传播，用<strong>链式法则</strong>得到所有的导数项，W 和 b 也会在每一层被更新。</p><p>在代码实现时，可以将正向传播过程中计算出来的 z 值缓存下来，待到反向传播计算时使用。</p><h3 id="矩阵的维度">矩阵的维度</h3><p><span class="math display">\[ W^{[l]}: (n^{[l]}, n^{[l-1]}) \]</span></p><p><span class="math display">\[ b^{[l]}: (n^{[l]}, 1) \]</span></p><p><span class="math display">\[ dW^{[l]}: (n^{[l]}, n^{[l-1]}) \]</span></p><p><span class="math display">\[ db^{[l]}: (n^{[l]}, 1) \]</span></p><p>对于 Z、a，向量化之前有： <span class="math display">\[ Z^{[l]}, a^{[l]}: (n^{[l]}, 1) \]</span> 而在向量化之后，则有： <span class="math display">\[ Z^{[l]}, A^{[l]}: (n^{[l]}, m) \]</span> 在计算反向传播时，dZ、dA 的维度和 Z、A 是一样的。</p><figure><img src="https://gitee.com/stuxiaozhang/blogimage/raw/master/img/机器学习/image-20201115203736131.png" srcset="/img/loading.gif" lazyload alt="image-20201115203736131"><figcaption>image-20201115203736131</figcaption></figure><h3 id="使用深层表示的原因">使用深层表示的原因</h3><p>对于人脸识别，神经网络的第一层从原始图片中提取人脸的轮廓和边缘，每个神经元学习到不同边缘的信息；网络的第二层将第一层学得的边缘信息组合起来，形成人脸的一些局部的特征，例如眼睛、嘴巴等；后面的几层逐步将上一层的特征组合起来，形成人脸的模样。随着神经网络层数的增加，特征也从原来的边缘逐步扩展为人脸的整体，由整体到局部，由简单到复杂。层数越多，那么模型学习的效果也就越精确。</p><p>同样的，对于语音识别，第一层神经网络可以学习到语言发音的一些音调，后面更深层次的网络可以检测到基本的音素，再到单词信息，逐渐加深可以学到短语、句子。</p><p>通过例子可以看到，随着神经网络的深度加深，模型能学习到更加复杂的问题，功能也更加强大。</p><h3 id="参数和超参数">参数和超参数</h3><p><strong>参数</strong>即是我们在过程中想要模型学习到的信息（<strong>模型自己能计算出来的</strong>），例如 W[l]W[l]，b[l]b[l]。而<strong>超参数（hyper parameters）</strong>即为控制参数的输出值的一些网络信息（<strong>需要人经验判断</strong>）。超参数的改变会导致最终得到的参数 <span class="math inline">\(W^{[l]}\)</span>，<span class="math inline">\(b^{[l]}\)</span> 的改变。</p><p>典型的超参数有：</p><ul><li>学习速率：α</li><li>迭代次数：N</li><li>隐藏层的层数：L</li><li>每一层的神经元个数：<span class="math inline">\(n^{[1]}\)</span>，<span class="math inline">\(n^{[2]}\)</span>，...</li><li>激活函数 g(z) 的选择</li></ul><p>当开发新应用时，预先很难准确知道超参数的最优值应该是什么。因此，通常需要尝试很多不同的值。应用深度学习领域是一个很大程度基于经验的过程。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处来源：<a href="https://stuxiaozhang.github.io/">小张的宇宙空间站</a></p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2020/11/10/Django%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Django学习笔记</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2020/11/01/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/"><span class="hidden-mobile">爬虫基础</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"81O1jSOqY3veRxOg71BDYfri-gzGzoHsz",appKey:"CgnvRL262D07ied40NiXm2VL",placeholder:"快来评论鸭~",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:"https://cdn.bootcdn.net/ajax/libs/emojione/4.5.0/lib/js/emojione.min.js",emojiMaps:null,enableQQ:!0,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://stuxiaozhang.github.io" target="_blank" rel="nofollow noopener"><span>小张同学的宇宙空间站</span></a> 已经运转了<span id="timeDate">载入天数...</span><script src="/js/duration.js"></script></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{renderActions:{findScript:[10,a=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const n=new a.options.MathItem(e.textContent,a.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),n.start={node:t,delim:"",n:0},n.end={node:t,delim:"",n:0},a.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>